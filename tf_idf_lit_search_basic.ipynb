{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e1c9c8a",
   "metadata": {},
   "source": [
    "### TF-IDF From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d926468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import math \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce2363d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12b3b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import os\n",
    "\n",
    "# configs = [\"query\", \"corpus_clean\", \"corpus_s2orc\"]\n",
    "\n",
    "\n",
    "# # Loop through and download each if not already saved\n",
    "# for config in configs:\n",
    "#     save_path = f\"LitSearch_{config}\"\n",
    "    \n",
    "#     if os.path.exists(save_path):\n",
    "#         print(f\" {config} already downloaded at {save_path}, skipping.\\n\")\n",
    "#         continue  # Skip download if directory already exists\n",
    "    \n",
    "#     print(f\" Downloading configuration: {config}\")\n",
    "#     dataset = load_dataset(\"princeton-nlp/LitSearch\", config)\n",
    "#     dataset.save_to_disk(save_path)\n",
    "#     print(f\" Saved {config} to {save_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f954ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3c786e9",
   "metadata": {},
   "source": [
    "### Load the dataset and make corpus\n",
    "* corpus: dict of document_id -> list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36d14d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64183, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_from_disk(\"LitSearch_corpus_clean\")\n",
    "data['full'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00b1ee04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['corpusid', 'title', 'abstract', 'citations', 'full_paper'],\n",
       "    num_rows: 64183\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['full'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec351f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing documents: 100%|██████████| 64183/64183 [00:03<00:00, 18111.31it/s]\n"
     ]
    }
   ],
   "source": [
    "documents={}    \n",
    "for item in tqdm(data[\"full\"], desc=\"Processing documents\"):\n",
    "    doc_id = str(item[\"corpusid\"])\n",
    "    contents = (item[\"title\"] or \"\") + \" \" + (item[\"abstract\"] or \"\")\n",
    "    documents[doc_id] = contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a340a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PHENAKI: VARIABLE LENGTH VIDEO GENERATION FROM OPEN DOMAIN TEXTUAL DESCRIPTIONS We present Phenaki, a model capable of realistic video synthesis, given a sequence of textual prompts. Generating videos from text is particularly challenging due to the computational cost, limited quantities of high quality text-video data and variable length of videos. To address these issues, we introduce a new model for learning video representation which compresses the video to a small representation of discrete tokens. This tokenizer uses causal attention in time, which allows it to work with variable-length videos. To generate video tokens from text we are using a bidirectional masked transformer conditioned on pre-computed text tokens. The generated video tokens are subsequently de-tokenized to create the actual video. To address data issues, we demonstrate how joint training on a large corpus of image-text pairs as well as a smaller number of video-text examples can result in generalization beyond what is available in the video datasets. Compared to the previous video generation methods, Phenaki can generate arbitrary long videos conditioned on a sequence of prompts (i.e. time variable text or a story) in open domain. To the best of our knowledge, this is the first time a paper studies generating videos from time variable prompts. In addition, compared to the perframe baselines, the proposed video encoder-decoder computes fewer tokens per video but results in better spatio-temporal consistency. ‡ Equal contribution.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['252715594']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25181b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#documents: 64183\n"
     ]
    }
   ],
   "source": [
    "corpus = dict() # docid -> list of words for the document\n",
    "for doc_id, text in documents.items():\n",
    "    corpus[doc_id] = text.lower().split()\n",
    "print(f\"#documents: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "262e75f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#average document length: 131.95 words.\n",
      "#unique words: 296439\n"
     ]
    }
   ],
   "source": [
    "lengths= [len(corpus[x]) for x in corpus]\n",
    "print(f\"#average document length: {sum(lengths)/len(lengths):.2f} words.\")\n",
    "all_words=set()\n",
    "for x in corpus:\n",
    "    all_words.update(corpus[x])\n",
    "\n",
    "unique_words= list(set(all_words))\n",
    "print(f\"#unique words: {len(unique_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a5f8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths= [len(corpus[x]) for x in corpus]\n",
    "all_words=set()\n",
    "for x in corpus:\n",
    "    all_words.update(corpus[x])\n",
    "\n",
    "stats={}\n",
    "stats['doc_size']=len(corpus)  # number of documents\n",
    "stats['vocab_size']=len(all_words) # vocabulary size\n",
    "stats['mean_dl']=sum(lengths)/len(lengths) # average document length\n",
    "stats['max_dl']=max(lengths) # maximum document length\n",
    "stats['min_dl']=min(lengths) # minimum document length\n",
    "\n",
    "# stats['doc_lengths']={docid: len(corpus[docid]) for docid in corpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c30cfe30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_size': 64183,\n",
       " 'vocab_size': 296439,\n",
       " 'mean_dl': 131.94582677656078,\n",
       " 'max_dl': 3347,\n",
       " 'min_dl': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de6465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = {w: i for i, w in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3d8f81",
   "metadata": {},
   "source": [
    "### Make Inverted Index\n",
    "* inverted_index: dict of word -> list of document_ids with frequency containing the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f722705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inverted_index(corpus):\n",
    "    \"\"\" \n",
    "    a word can be found in which documents and how many times.\n",
    "    corpus: {docid -> list of words for the document}\n",
    "    inverted_index: {word -> {docid: frequency of the word in docid}}\n",
    "    \"\"\"\n",
    "    inverted_index = dict() # word -> {docid: frequency in docid}\n",
    "    for docid in tqdm(corpus, desc=\"Building inverted index\"):\n",
    "        words = corpus[docid]\n",
    "        for word in words:\n",
    "            if word not in inverted_index:\n",
    "                inverted_index[word] = dict()\n",
    "            if docid not in inverted_index[word]:\n",
    "                inverted_index[word][docid] = 0\n",
    "            inverted_index[word][docid] += 1\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b140911f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building inverted index:   0%|          | 0/64183 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building inverted index: 100%|██████████| 64183/64183 [00:02<00:00, 25088.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(296439,\n",
       " ['phenaki:',\n",
       "  'variable',\n",
       "  'length',\n",
       "  'video',\n",
       "  'generation',\n",
       "  'from',\n",
       "  'open',\n",
       "  'domain',\n",
       "  'textual',\n",
       "  'descriptions'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index = make_inverted_index(corpus)\n",
    "len(inverted_index), list(inverted_index.keys())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93f8fd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docids=list( inverted_index['the'].keys() )\n",
    "# len(docids), docids[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a93e09",
   "metadata": {},
   "source": [
    "### TF-IDF Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ad390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_term_frequency(document_tokens, vocab_size):\n",
    "    \"\"\" \n",
    "    for each word, how important it is in this document. (#time it appears in this doc / total #words in this doc)\n",
    "    \"\"\"\n",
    "    tf=np.zeros(vocab_size, dtype=float)\n",
    "\n",
    "    doc_len = len(document_tokens)\n",
    "    if doc_len == 0:\n",
    "        return tf \n",
    "\n",
    "    counts = Counter(document_tokens) \n",
    "    for tok, c in counts.items():\n",
    "        idx = index_map.get(tok)\n",
    "        if idx is not None:\n",
    "            tf[idx] = c / doc_len\n",
    "    return tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e65a485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_inverse_document_frequency(document_tokens, inverted_index, document_size, vocab_size):\n",
    "    \"\"\" \n",
    "    for each word, how important it is in the corpus. (#document contains it / total #documents)\n",
    "    \"\"\" \n",
    "    idf_vector = np.zeros( vocab_size, dtype=float )\n",
    "    for token in document_tokens:\n",
    "        how_many_docs_contain_this_token = len( inverted_index[token] ) \n",
    "        token_index = index_map.get(token)\n",
    "        idf_value = np.log( (document_size+1) / (how_many_docs_contain_this_token + 1.0) )\n",
    "        idf_vector[token_index] = idf_value\n",
    "    return idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe451b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19e81607",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= stats['vocab_size']\n",
    "document_size= stats['doc_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22f07e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0199847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# document_id = '252715594' \n",
    "# tokens = corpus[document_id] \n",
    "# tf_vector = calc_term_frequency(tokens, vocab_size)\n",
    "# idf_vector = calc_inverse_document_frequency(tokens, inverted_index, document_size, vocab_size)\n",
    "# tf_idf = tf_vector * idf_vector\n",
    "# tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608de6e5",
   "metadata": {},
   "source": [
    "### calculate tf-idf vector for all the documents in the corpus\n",
    "* require 155GB RAM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa2a6cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating TF-IDF for documents: 100%|██████████| 64183/64183 [01:19<00:00, 810.33it/s]\n"
     ]
    }
   ],
   "source": [
    "tf_idfs={}\n",
    "for docid in tqdm(corpus, desc=\"Calculating TF-IDF for documents\"):\n",
    "    document_tokens = corpus[docid]\n",
    "    tf_vector = calc_term_frequency(document_tokens, vocab_size)\n",
    "    idf_vector = calc_inverse_document_frequency(document_tokens, inverted_index, document_size, vocab_size)\n",
    "    tf_idfs[docid] = tf_vector * idf_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d97a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eef5440e",
   "metadata": {},
   "source": [
    "### Load query documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c3e5655",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_query = load_from_disk(\"LitSearch_query\")\n",
    "\n",
    "query_0 = dataset_query[\"full\"][0][\"query\"]\n",
    "query_1 = dataset_query[\"full\"][1][\"query\"]\n",
    "\n",
    "requests = {0: query_0, 1: query_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3bc3dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?',\n",
       " 1: 'Are there any resources available for translating Tunisian Arabic dialect that contain both manually translated comments by native speakers and additional data augmented through methods like segmentation at stop words level?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850efdcf",
   "metadata": {},
   "source": [
    "### calculate tf-idf for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "559b0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate tf-idf for the query\n",
    "\n",
    "query_tf_idfs={}\n",
    "top_k = 3\n",
    "\n",
    "for qid in requests:\n",
    "    query_tokens = requests[qid].lower().split()\n",
    "\n",
    "    tf_vector = calc_term_frequency(query_tokens, vocab_size)\n",
    "    idf_vector = calc_inverse_document_frequency(query_tokens, inverted_index, document_size, vocab_size)\n",
    "    query_tf_idf = tf_vector * idf_vector\n",
    "    query_tf_idfs[qid] = query_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1565f26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c230fad7",
   "metadata": {},
   "source": [
    "### Calculate cosine distance for each query and rank documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8efb0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64183/64183 [00:02<00:00, 22481.60it/s]\n",
      "100%|██████████| 64183/64183 [00:02<00:00, 22934.05it/s]\n"
     ]
    }
   ],
   "source": [
    "all_query_results = {}\n",
    "\n",
    "top_k = 50\n",
    "\n",
    "for qid in query_tf_idfs:\n",
    "    query_tf_idf = query_tf_idfs[qid]\n",
    "\n",
    "    query_result = dict() # docid -> score\n",
    "    #now calculate cosine similarity between the query and each document\n",
    "    for docid in tqdm(tf_idfs):\n",
    "        doc_tf_idf = tf_idfs[docid]\n",
    "        dot_product = np.dot(query_tf_idf.T, doc_tf_idf) \n",
    "        query_norm = np.linalg.norm(query_tf_idf)\n",
    "        doc_norm = np.linalg.norm(doc_tf_idf)\n",
    "        if query_norm == 0 or doc_norm == 0:\n",
    "            cosine_similarity = 0.0\n",
    "        else:\n",
    "            cosine_similarity = dot_product / (query_norm * doc_norm)\n",
    "        query_result[docid] = cosine_similarity\n",
    "\n",
    "    query_result_sorted = sorted(query_result.items(), key=lambda x: x[1], reverse=True)\n",
    "    all_query_results[qid] = query_result_sorted[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3775740a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [('257038997', 0.27946243631698464),\n",
       "  ('259137871', 0.25003909563714743),\n",
       "  ('11743245', 0.20766365431527253),\n",
       "  ('235258277', 0.1938886061467414),\n",
       "  ('201670719', 0.19212080978400298),\n",
       "  ('215238664', 0.1891933070023115),\n",
       "  ('218502458', 0.18135526472169056),\n",
       "  ('221995575', 0.17857260379122605),\n",
       "  ('258865530', 0.16888846091192222),\n",
       "  ('256461337', 0.16026205980247404),\n",
       "  ('8451212', 0.15753114231005344),\n",
       "  ('227231666', 0.15437580370964962),\n",
       "  ('222290473', 0.15291581010282185),\n",
       "  ('241583522', 0.15280084750910733),\n",
       "  ('237563200', 0.15014159265579255),\n",
       "  ('235652233', 0.14962857723906986),\n",
       "  ('253107373', 0.14834680466046865),\n",
       "  ('3960960', 0.1468440997957143),\n",
       "  ('222132977', 0.14419822337178312),\n",
       "  ('226284011', 0.14379230484233985),\n",
       "  ('259370760', 0.14377171107605005),\n",
       "  ('256461264', 0.143290963771035),\n",
       "  ('248780060', 0.1428707623746974),\n",
       "  ('250390701', 0.1420915168716719),\n",
       "  ('256461166', 0.1420206915048201),\n",
       "  ('233307448', 0.1417215047032746),\n",
       "  ('259370551', 0.14002528581885973),\n",
       "  ('235436167', 0.1391863296736758),\n",
       "  ('248779934', 0.13619775773039508),\n",
       "  ('259370795', 0.13467050185318127),\n",
       "  ('204867723', 0.13313113555024148),\n",
       "  ('22421874', 0.13207646094312953),\n",
       "  ('252693220', 0.13166310774901743),\n",
       "  ('256390129', 0.1315184579786112),\n",
       "  ('244909258', 0.13081561203345263),\n",
       "  ('247593909', 0.1305390823859536),\n",
       "  ('258967881', 0.13038985224424948),\n",
       "  ('2331610', 0.12973854464124246),\n",
       "  ('235294276', 0.12845524308410824),\n",
       "  ('234482764', 0.12574361649863317),\n",
       "  ('3643430', 0.12397949555098745),\n",
       "  ('264490431', 0.12372849069760948),\n",
       "  ('239015981', 0.12299455170891575),\n",
       "  ('225076095', 0.12257346150093304),\n",
       "  ('247922354', 0.12239966484544294),\n",
       "  ('199453123', 0.12177283373197763),\n",
       "  ('211026455', 0.12079036152755915),\n",
       "  ('233189631', 0.11966842211197747),\n",
       "  ('235390766', 0.11928471944129958),\n",
       "  ('248779987', 0.11876138419365559)],\n",
       " 1: [('252432736', 0.34411843532369946),\n",
       "  ('6825507', 0.3220394450344605),\n",
       "  ('9517956', 0.3057372609260658),\n",
       "  ('227231792', 0.30006906916973053),\n",
       "  ('1644594', 0.2923273054468831),\n",
       "  ('16825250', 0.2881559300305549),\n",
       "  ('256739275', 0.2273915398410954),\n",
       "  ('5797224', 0.21508728570251434),\n",
       "  ('52012926', 0.21255145202105663),\n",
       "  ('233365022', 0.208054757339757),\n",
       "  ('15225095', 0.20507810360444484),\n",
       "  ('16213870', 0.19234194677732347),\n",
       "  ('256461096', 0.1882297980768195),\n",
       "  ('214612326', 0.1831453734418789),\n",
       "  ('258765255', 0.18139531367723008),\n",
       "  ('29437576', 0.17978573479023516),\n",
       "  ('11851475', 0.17561297212447452),\n",
       "  ('1612194', 0.17450632857165366),\n",
       "  ('3513881', 0.174262206005259),\n",
       "  ('253762069', 0.1727574206695844),\n",
       "  ('909680', 0.17001538351164985),\n",
       "  ('256461111', 0.16996296760258658),\n",
       "  ('259376822', 0.16786516517316274),\n",
       "  ('238583454', 0.1653741916639796),\n",
       "  ('9653643', 0.16480153914433093),\n",
       "  ('12678205', 0.16019535982753477),\n",
       "  ('12160022', 0.15762481713453722),\n",
       "  ('199379793', 0.15215971555917102),\n",
       "  ('17326709', 0.15129982359294392),\n",
       "  ('249980208', 0.15069759622530152),\n",
       "  ('14582094', 0.14908425917025742),\n",
       "  ('8333055', 0.14633734220362588),\n",
       "  ('10961392', 0.14601319210465227),\n",
       "  ('259834596', 0.14376523458626245),\n",
       "  ('259833888', 0.14376523458626245),\n",
       "  ('227231625', 0.14371766082996837),\n",
       "  ('7546993', 0.14058595061855392),\n",
       "  ('9726397', 0.14054531309757934),\n",
       "  ('221970959', 0.14008656335703093),\n",
       "  ('32734599', 0.13866132452829574),\n",
       "  ('11707528', 0.1354120860749638),\n",
       "  ('231979434', 0.13493779543384057),\n",
       "  ('227231049', 0.13492593809127146),\n",
       "  ('18196992', 0.1347730155200714),\n",
       "  ('1787135', 0.13472256226654145),\n",
       "  ('13160358', 0.13462881015537137),\n",
       "  ('15439564', 0.1342478373961608),\n",
       "  ('21701426', 0.13394250389514892),\n",
       "  ('7790831', 0.13367484747073202),\n",
       "  ('250426644', 0.1330181277573658)]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_query_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3151da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46c975ad",
   "metadata": {},
   "source": [
    "### save in .run file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6954809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = \"tfidf_basic\"\n",
    "request_id = 0\n",
    "\n",
    "\n",
    "output_file = \"litsearch_topk_results_tfidf_basic.run\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for qid in all_query_results:\n",
    "        results = all_query_results[qid]\n",
    "        request_id += 1\n",
    "        rank = 0\n",
    "        for doc_id, score in results:\n",
    "            rank += 1\n",
    "            line = f\"{request_id} Q0 {doc_id} {rank} {score:.6f} {method_name}\\n\"\n",
    "            # print(line.strip())\n",
    "            out_f.write(line)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3ec3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d16e6129",
   "metadata": {},
   "source": [
    "### save in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4f0a1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_name = \"tfidf_basic\"\n",
    "request_id = 0\n",
    "\n",
    "\n",
    "output_file = \"litsearch_top3_results_tfidf_basic.jsonl\"\n",
    "\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for qid in all_query_results:\n",
    "        results = all_query_results[qid]\n",
    "        request_id += 1\n",
    "        query_text = requests[qid]\n",
    "        top3=[]\n",
    "        for rank, (doc_id, score) in enumerate(results[:3]):\n",
    "            # print(f\"Rank {rank+1}: DocID={doc_id}, Score={score:.6f}\")\n",
    "            rd = {\"doc_id\": doc_id, \"score\": score, \"rank\": rank + 1}\n",
    "            top3.append(rd)\n",
    "        \n",
    "\n",
    "        out_f.write(json.dumps({\n",
    "            \"qid\": qid,\n",
    "            \"query\": query_text,\n",
    "            \"top3\": top3\n",
    "        }) + \"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4161346f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08cf38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
