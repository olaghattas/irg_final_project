{"original_query": "Are there any research papers on methods to compress large-scale language models using task-agnostic knowledge distillation techniques?", "expanded_query": "Search for research papers, surveys, and conference articles on compressing large-scale language models such as BERT, GPT, T5, and other transformer-based architectures using task-agnostic knowledge distillation techniques, including teacher\u2011student frameworks, parameter pruning, quantization, low\u2011rank factorization, structured and unstructured pruning, mixed\u2011precision training, and other model compression methods, with a focus on accuracy trade\u2011offs, compression ratios, inference speedups, memory footprint reductions, and deployment on edge devices or cloud environments, covering venues like ACL, EMNLP, NeurIPS, ICLR, ICML, COLING, arXiv, and relevant open\u2011source implementations."}
{"original_query": "Are there any resources available for translating Tunisian Arabic dialect that contain both manually translated comments by native speakers and additional data augmented through methods like segmentation at stop words level?", "expanded_query": "Are there any publicly available resources, corpora, or datasets for translating Tunisian Arabic dialect that include manually translated comments by native speakers, annotated with stop\u2011word segmentation, augmented with data\u2011augmentation techniques such as tokenization, synthetic data generation, or segmentation at stop\u2011word level, and suitable for NLP, machine translation, linguistic research, parallel corpora, translation memory, crowdsourced annotations, and Arabic dialectology?"}
{"original_query": "Are there any studies that explore post-hoc techniques for hallucination detection at both the token- and sentence-level in neural sequence generation tasks?", "expanded_query": "Are there any studies that explore post-hoc techniques for hallucination detection at both the token- and sentence-level in neural sequence generation tasks such as summarization, translation, dialogue, question answering, and story generation using transformer-based models like GPT-3, GPT-4, T5, BART, LLaMA, PaLM, and BERT, employing methods such as confidence scoring, attention weight analysis, saliency maps, LIME, SHAP, gradient-based attribution, and evaluation metrics including precision, recall, F1, BLEU, ROUGE, METEOR, human evaluation, and automatic evaluation for token-level and sentence-level hallucination detection?"}
{"original_query": "Are there any tools or studies that have focused on building a morphological analyzer specifically for handling multiple Arabic dialects?", "expanded_query": "Are there any tools, studies, or research projects that focus on building a morphological analyzer specifically for handling multiple Arabic dialects such as Egyptian, Levantine, Gulf, and Maghrebi, using rule\u2011based, hybrid, or machine\u2011learning approaches, that provide Arabic morphological segmentation, tagging, and analysis, and that offer open\u2011source software, Arabic dialect corpora, NLP resources, and computational linguistics methods for Arabic dialectal processing?"}
{"original_query": "Are there papers that propose contextualized calibration for the probability of answers in language models?", "expanded_query": "Are there research papers or surveys that propose contextualized calibration methods for the probability of answers in language models, such as transformers (BERT, GPT, RoBERTa, XLNet), including techniques like temperature scaling, Bayesian calibration, post\u2011hoc calibration, self\u2011consistency, and contextualized calibration for answer confidence, evaluated with metrics such as expected calibration error, and published in venues like ACL, EMNLP, NAACL, ICLR, NeurIPS, ICML, or on arXiv?"}
{"original_query": "Are there studies that combine convolutional and recurrent neural network approaches to extract multiple types of features for relation extraction? If so, could you point me to one of them?", "expanded_query": "Are there studies that combine convolutional neural networks and recurrent neural networks, such as CNN+BiLSTM or CNN+RNN hybrid models, to extract multiple types of features\u2014including syntactic, semantic, contextualized embeddings, dependency parsing, named entity recognition, semantic role labeling, attention mechanisms, and transformer-based embeddings\u2014for relation extraction tasks on datasets like ACE2005, SemEval, OpenIE, CoNLL, and if so, could you point me to one of them?"}
{"original_query": "Can you direct me to research that explores methods for transforming multi-hop questions into single-hop sub-questions to leverage existing single-hop answer models?", "expanded_query": "Can you direct me to research that explores methods for transforming multi\u2011hop questions into single\u2011hop sub\u2011questions to leverage existing single\u2011hop answer models, including approaches such as question decomposition, question rewriting, neural module networks, graph neural networks, semantic parsing, dependency parsing, knowledge graph traversal, and evaluation on datasets like HotpotQA, WikiHop, and MultiRC, as well as relevant conference proceedings from ACL, EMNLP, NeurIPS, ICLR, and arXiv preprints?"}
{"original_query": "Can you direct me to studies that explore techniques like question answering and passage retrieval for mitigating the effects of clickbait headlines?", "expanded_query": "Find peer\u2011reviewed studies, conference papers, journal articles, systematic reviews, and meta\u2011analyses on clickbait mitigation using question answering and passage retrieval techniques, including transformer\u2011based QA, dense passage retrieval, semantic search, retrieval\u2011augmented generation, BERT, RoBERTa, XLNet, sentence transformers, FAISS, BM25, clickbait detection, classification, filtering, reduction, impact on click\u2011through rates, misinformation spread, user engagement, information\u2011seeking behavior, media literacy, ethical implications, bias, fairness, adversarial robustness, content moderation, recommendation, search engine relevance, and user behavior in news articles, social media platforms like Twitter, Facebook, Reddit, and search engines like Google, Bing, and Yahoo."}
{"original_query": "Can you list some publications that discuss the evaluation metrics used in semantic role labeling tasks, specifically focusing on precision and evaluation scripts used in shared tasks like SemEval?", "expanded_query": "semantic role labeling evaluation metrics precision recall F1-score macro-average micro-average evaluation scripts evaluation tools evaluation frameworks evaluation methodology evaluation protocols SemEval-2010 Task 7 SemEval-2012 Task 7 SemEval-2013 Task 7 SemEval-2014 Task 7 SemEval-2015 Task 7 SemEval-2016 Task 7 SemEval-2017 Task 7 SemEval-2018 Task 7 SemEval-2019 Task 7 SemEval-2020 Task 7 SemEval-2021 Task 7 SemEval-2022 Task 7 SemEval-2023 Task 7 SemEval-2024 Task 7 CoNLL-2005 shared task CoNLL-2012 shared task evaluation scripts used in CoNLL-2005 evaluation scripts used in CoNLL-2012"}
{"original_query": "Can you point me to a paper that discussed transformer-based sentence embeddings?", "expanded_query": "Looking for academic papers on transformer-based sentence embeddings, including BERT, SBERT, Sentence Transformers, contextualized sentence representations, semantic similarity, natural language processing, deep learning, pretrained language models, and related research studies"}
{"original_query": "Can you point me to a work that uses diagnostic tools to detect depression from online posts, and investigates strategies that address common temporal and topical artifacts that plague these models?", "expanded_query": "Looking for a research paper or study that applies machine learning diagnostic tools such as logistic regression, SVM, random forest, or transformer-based models (BERT, RoBERTa, GPT) to detect depression from online posts (Twitter, Reddit, Facebook, Instagram, blog, forum) using natural language processing and sentiment analysis, while investigating strategies to mitigate common temporal artifacts (seasonality, time\u2011of\u2011day, weekday/weekend, posting frequency, concept drift) and topical artifacts (topic drift, topic bias, topic shift, dynamic topic modeling, LDA, NMF) that plague these models, including techniques like temporal smoothing, rolling windows, topic alignment, domain adaptation, transfer learning, ensemble methods, data augmentation, and bias mitigation, and evaluating model robustness with cross\u2011validation, time\u2011series split, precision, recall, F1, ROC, AUC, and interpretability methods (SHAP, LIME, attention visualization), with clinical validation against PHQ\u20119 or other psychometric scales, and addressing ethical concerns such as privacy, de\u2011identification, and data governance."}
{"original_query": "Can you point me to studies discussing methods for evaluating text generation models on various dimensions? I'm particularly interested in models like T5 and FLAN-T5, and how to assess their performance on summary-level and turn-level tasks.", "expanded_query": "Can you point me to studies discussing methods for evaluating text generation models on various dimensions, including automatic metrics such as BLEU, ROUGE, METEOR, BERTScore, BLEURT, and human evaluation frameworks, focusing on pretrained language models like T5, FLAN\u2011T5, and their variants (T5\u2011base, T5\u2011large, T5\u20113B, T5\u201111B, FLAN\u2011T5\u2011base, FLAN\u2011T5\u2011large, FLAN\u2011T5\u2011xxl), and how to assess their performance on summary\u2011level tasks (e.g., CNN/DailyMail, XSum, Gigaword) and turn\u2011level tasks in dialogue systems (e.g., MultiWOZ, DSTC, dialogue state tracking, turn\u2011level coherence, relevance, fluency, diversity), including task\u2011specific benchmarks, evaluation methodologies, error analysis, and interpretability studies."}
{"original_query": "Can you point me to studies that explore the impact of different data augmentation strategies, such as feature/token/span cutoff or dropout, in the context of contrastive learning for sentence representations?", "expanded_query": "Studies on the impact of data augmentation strategies such as feature dropout, token dropout, span cutoff, random deletion, random substitution, masking, noise injection, syntactic and semantic augmentation in contrastive learning for sentence representations, including SimCSE, SimCLR, BERT, RoBERTa, sentence transformers, evaluation on STS benchmark, GLUE, SentEval, cosine similarity, semantic textual similarity, paraphrase detection, NLI, downstream classification, retrieval, clustering, robustness, generalization, transfer learning, domain adaptation, pretraining, fine-tuning, ablation studies, hyperparameter tuning, loss functions such as NT\u2011Xent, InfoNCE, contrastive loss, evaluation metrics, and empirical results reported in ACL, EMNLP, NAACL, COLING, ICLR, NeurIPS, ICML, arXiv preprints."}
{"original_query": "Can you point me towards research on contrastive learning methods used for fine-tuning sentence representations, where in-batch negatives may sometimes unintentionally be similar to the positive examples?", "expanded_query": "Find research on contrastive learning for fine\u2011tuning sentence representations, especially studies addressing in\u2011batch negative sampling pitfalls where negatives can be similar to positives, covering hard negative mining, negative sampling bias, temperature scaling, SimCSE, SimCLR, sentence transformers, BERT, RoBERTa, sentence\u2011BERT, semantic similarity, embedding space, semantic search, text similarity, NLP, benchmark datasets such as STS\u2011B and GLUE, and key papers from Google, Microsoft, OpenAI, and top conferences."}
{"original_query": "Can you point to studies or tasks focused on detecting patronizing and condescending language, particularly in contexts involving vulnerable communities?", "expanded_query": "Can you point to studies or tasks focused on detecting patronizing and condescending language, particularly in contexts involving vulnerable communities such as people of color, LGBTQ+ individuals, immigrants, people with disabilities, low\u2011income groups, women, elderly, and other marginalized populations, including research on linguistic markers, tone detection, sentiment analysis, bias detection, hate speech, microaggressions, NLP datasets, benchmarks, annotation guidelines, and evaluation metrics for condescension detection in social media, news, online forums, and other digital communication?"}
{"original_query": "Can you recommend a conversational QA dataset where the human questioner does not have access to the evidence passage to simulate a more real-world information-seeking environment?", "expanded_query": "Can you recommend a conversational QA dataset such as ConvAI2, QuAC, CoQA, or HotpotQA where the human questioner does not have access to the evidence passage, simulating a real\u2011world information\u2011seeking environment with retrieval\u2011based or knowledge\u2011base constraints, multi\u2011turn dialogue, open\u2011domain search, question generation, and answer verification tasks?"}
{"original_query": "Can you recommend a foundational paper that provides a scalable framework for generating English sentences with controllable semantic and syntactic attributes for the purpose of augmenting datasets in NLP tasks?", "expanded_query": "Recommend a foundational paper that presents a scalable framework for generating English sentences with controllable semantic and syntactic attributes using conditional language models or attribute\u2011conditioned generation, aimed at augmenting datasets for NLP tasks such as text classification, sentiment analysis, and machine translation, covering semantic control, syntactic control, attribute conditioning, data augmentation, and scalability."}
{"original_query": "Can you recommend a paper that uses an NLI model for sentence-level relation extraction using hypothesis generation and verification with entity-type constraints?", "expanded_query": "Can you recommend a recent academic paper that applies a transformer-based natural language inference model for sentence-level relation extraction, using hypothesis generation and verification with entity-type constraints, incorporating entity typing, entity linking, semantic role labeling, knowledge graph integration, fine-tuned BERT or RoBERTa architectures, evaluated on datasets such as TACRED, FewRel, or OpenIE, and published in top NLP conferences like ACL, EMNLP, or NeurIPS, while addressing semantic entailment, contradiction detection, attention mechanisms, contextual embeddings, entity type hierarchy, ontology, schema, entity type embeddings, relation classification, semantic constraints, and reporting F1, precision, and recall metrics?"}
{"original_query": "Can you recommend some literature that focuses on dependency-based models for relation extraction, especially those that utilize dependency parsing to capture non-local syntactic relations?", "expanded_query": "Can you recommend literature on dependency-based models for relation extraction that emphasize dependency parsing, non-local syntactic relations, long-range dependencies, syntactic path features, dependency tree kernels, graph neural networks, transformer-based approaches, BERT, GCN, GAT, syntactic distance, non-projective dependencies, semantic role labeling, and datasets such as ACE, SemEval, NYT, CoNLL?"}
{"original_query": "Can you refer me to research that adapts the concept of Word Mover's Distance to sentences, addressing the limitations of bag-of-words approaches and considering the order of words for text similarity?", "expanded_query": "Can you refer me to research papers or academic studies that adapt the Word Mover's Distance concept to sentence-level similarity, addressing bag-of-words limitations, incorporating word order, using contextualized embeddings such as BERT, RoBERTa, or GPT, exploring sequence-aware distance metrics, sentence embeddings, semantic similarity, semantic alignment, semantic vector space, and evaluating text similarity with order-aware WMD variants in NLP?"}
{"original_query": "Can you suggest a corpus that contains French encyclopedia documents with semantic annotations and includes a test set of manually written question/answer triplets that align with the constraints of FrameNet semantic analysis?", "expanded_query": "Recommend a French encyclopedia corpus that includes semantic annotations, a test set of manually written question/answer triplets, aligns with FrameNet semantic analysis constraints, covers French encyclopedic documents such as French Wikipedia and other encyclopedic texts, uses semantic role labeling, frame semantics, French FrameNet guidelines, manual QA annotation, evaluation set, semantic annotation schema, ontology, knowledge base, French NLP resources, and provides a French FrameNet\u2011like dataset for research."}
{"original_query": "Can you suggest any literature that explores the idea of training neural networks to translate text passages into related questions?", "expanded_query": "Literature on neural question generation, text-to-question generation, question generation from text passages, neural network architectures for question generation, transformer-based models such as BERT, GPT, T5, BART, encoder-decoder attention mechanisms, sequence-to-sequence models, deep learning for question generation, datasets like SQuAD, NarrativeQA, MS MARCO, question generation datasets, academic papers, conference proceedings from ACL, EMNLP, NeurIPS, ICLR, arXiv, survey articles, state-of-the-art methods, question generation for education, automated assessment, knowledge extraction, question answering, natural language processing, machine learning, literature review, research articles, academic sources."}
{"original_query": "Can you suggest literature on a dataset that categorizes various emotions like anger, anticipation, fear, joy, and sadness in Facebook posts across multiple languages?", "expanded_query": "Suggest literature on publicly available multilingual Facebook posts datasets that categorize emotions such as anger, anticipation, fear, joy, and sadness for social media sentiment analysis, covering emotion taxonomy, annotation guidelines, cross-lingual emotion detection, affective computing, deep learning and transformer-based models like multilingual BERT and XLM\u2011R, including references to Facebook API usage, data privacy, ethical considerations, and related emotion lexicons and corpora."}
{"original_query": "Can you suggest literature on enhanced semantic parsing methods that focus on generating high-quality meaning representations and utilize knowledge-constrained decoding under specific grammar rules?", "expanded_query": "Literature on enhanced semantic parsing methods focusing on generating high\u2011quality meaning representations, employing knowledge\u2011constrained decoding, and applying specific grammar rules, including neural semantic parsing, transformer\u2011based approaches, knowledge graph integration, formal semantics, lambda calculus, SPARQL, SQL, probabilistic context\u2011free grammar, dependency grammar, domain\u2011specific rule\u2011based systems, state\u2011of\u2011the\u2011art benchmarks, datasets, evaluation metrics, and recent survey papers."}
{"original_query": "Can you suggest recent studies that have integrated prompt fine-tuning into semi-supervised learning workflows for natural language understanding tasks?", "expanded_query": "Can you suggest recent studies that have integrated prompt fine-tuning into semi-supervised learning workflows for natural language understanding tasks, including prompt-based fine-tuning, prompt adapters, LoRA, self-training, pseudo-labeling, transformer models such as BERT, GPT, T5, LLaMA, prompt engineering, few-shot and zero-shot learning, cross-lingual NLU, domain adaptation, low-resource languages, text classification, named entity recognition, sentiment analysis, question answering, prompt-based self-training, prompt-based pseudo-labeling, prompt-based data augmentation, prompt-based active learning, prompt-based curriculum learning, prompt-based knowledge distillation, prompt-based multi-task learning, prompt-based continual learning, prompt-based domain adaptation, prompt-based zero-shot learning, prompt-based few-shot learning, prompt-based in-context learning, prompt-based parameter-efficient fine-tuning, prompt-based LoRA adapters, and related semi-supervised learning pipelines."}
{"original_query": "Can you suggest some literature that evaluates the ability of context-aware machine translation systems to handle discourse phenomena such as deixis and lexical cohesion?", "expanded_query": "Can you suggest some literature that evaluates the ability of context-aware machine translation systems, such as neural transformer-based document-level MT and contextualized BERT/GPT models, to handle discourse phenomena including deixis, lexical cohesion, anaphora resolution, co-reference, pragmatic inference, and other discourse-level challenges, using automatic metrics like BLEU, METEOR, TER, as well as human evaluation, and referencing benchmark datasets, conference proceedings (ACL, EMNLP, NAACL, COLING), journal articles, and recent studies on context-aware MT evaluation frameworks and literature reviews?"}
{"original_query": "Can you suggest some recent datasets that have been used for studying stance detection in tweets, particularly those targeting specific individuals and events since 2020?", "expanded_query": "Can you suggest some recent datasets that have been used for studying stance detection in tweets, particularly those targeting specific individuals and events since 2020, including Twitter API collections on political leaders, election and protest events, COVID-19 discussions, multilingual stance corpora, datasets released on Kaggle, GitHub, Zenodo, ACL Anthology, arXiv, and Harvard Dataverse, with annotations for target individuals, hashtags, event-based stance labels, and evaluation metrics such as precision, recall, F1, accuracy, cross\u2011validation, transfer learning, and domain adaptation, referencing benchmark datasets like SemEval, FNC\u20111, ICWSM event stance datasets, and including details on crowdsourced annotation, ground truth, gold standard, annotation guidelines, transformer models such as BERT, RoBERTa, XLNet, and covering languages like English, Spanish, Arabic, Chinese, and years 2021, 2022, 2023, 2024?"}
{"original_query": "Could you direct me towards a study that explores the potential to predict a reader's native language based on their eye movement patterns while reading English texts?", "expanded_query": "Could you direct me towards a study that explores the potential to predict a reader's native language based on their eye movement patterns while reading English texts, using eye-tracking metrics such as fixation duration, saccade length, and visual attention, and applying machine learning, computational modeling, and linguistic typology to analyze cross-linguistic influence, L2 reading proficiency, language transfer effects, bilingual reading behavior, reading fluency, and language identification in eye movement data?"}
{"original_query": "Could you point me to research on binary classification systems that predict whether sentences within a context require clarification, particularly using the wikiHowToImprove dataset?", "expanded_query": "Could you point me to research on binary classification systems for sentence-level clarification detection within contextual how-to guides, especially leveraging the wikiHowToImprove dataset, covering methods such as transformer-based models, feature engineering, discourse analysis, ambiguity detection, and evaluation metrics like precision, recall, F1, and cross-validation, as well as relevant literature on context-aware NLP, sentence segmentation, and clarifying question generation?"}
{"original_query": "Could you point me to studies that discuss the development of open information extraction systems with lexical and syntactic constraints to ensure the extraction is coherent and informative?", "expanded_query": "Could you point me to studies, surveys, systematic reviews, and conference proceedings that discuss the development and evaluation of open information extraction systems incorporating lexical constraints, syntactic constraints, dependency parsing, POS tagging, semantic role labeling, and contextual embeddings such as BERT or transformer-based models to ensure coherent, informative, high-precision, high-recall extractions, including benchmark datasets like CoNLL, SemEval, TREC, and evaluation metrics, and referencing key works such as Stanford OpenIE, ReVerb, OLLIE, ClausIE, OpenIE 4.0, and recent neural open IE approaches, as well as resources like WordNet, FrameNet, and knowledge graph integration, published in venues like ACL, EMNLP, NAACL, COLING, LREC, Computational Linguistics, JAR, IEEE TKDE, ACM TOIS, and available on arXiv or Google Scholar."}
{"original_query": "Could you point me to studies that have investigated dialectal variations and normalized text in non-Latin scripts, such as Arabic or Japanese?", "expanded_query": "Could you point me to studies that have investigated dialectal variations and normalized text in non-Latin scripts, such as Arabic or Japanese, including Arabic dialect corpora, Japanese dialect corpora, dialect identification, dialect classification, text normalization techniques, orthographic variation, phonological variation, syntactic variation, semantic variation, computational linguistics, natural language processing, language resources, corpora, speech recognition, machine translation, script conversion, Romanization, transliteration, Unicode normalization, NFKC, NFKD, canonical decomposition, tokenization, word segmentation, character segmentation, language modeling, dialectal variation research, dialectology, regional dialects, colloquial speech, formal register, dialect identification methods, dialect identification techniques, dialect identification in Arabic, dialect identification in Japanese, non-Latin script NLP, Arabic NLP, Japanese NLP, language technology, linguistic typology, dialectal variation in Arabic and Japanese, normalized text processing, text preprocessing, language identification, dialectal variation studies, dialectal variation in Arabic script, dialectal variation in Japanese script, non-Latin script text normalization, Arabic dialect normalization, Japanese dialect normalization, dialectal variation studies in Arabic and Japanese, dialectal variation research in Arabic and Japanese, dialectal variation studies non-Latin scripts, dialectal variation and normalized text in non-Latin scripts, Arabic dialect studies, Japanese dialect studies, dialectal variation research, dialectology, dialect identification, dialect classification, dialect identification in Arabic, dialect identification in Japanese, dialect identification in non-Latin scripts, dialect identification techniques, dialect identification methods, dialect identification approaches."}
{"original_query": "Could you point me toward some large-scale multilingual Amazon customer review data?", "expanded_query": "Could you point me toward some large-scale multilingual Amazon customer review data, such as an open-source Amazon product reviews dataset in English, Spanish, French, German, Chinese, and other languages, suitable for NLP tasks like sentiment analysis, aspect-based sentiment analysis, cross-lingual transfer learning, product recommendation, text classification, entity extraction, machine translation, language modeling, deep learning, and data mining, available from public sources like Kaggle, AWS Open Data, GitHub, or other open datasets?"}
{"original_query": "Could you provide me with a reference that discusses the development of classifiers for suicide risk detection in a low-resource language, with a specific focus on using explicit suicide-related terminology?", "expanded_query": "Could you provide me with a reference that discusses the development of classifiers for suicide risk detection in a low-resource language, with a specific focus on using explicit suicide-related terminology, including machine learning, natural language processing, text classification, low-resource NLP techniques, transfer learning, cross-lingual embeddings, lexicon-based methods, data scarcity, annotation guidelines, data augmentation, semi-supervised learning, evaluation metrics such as precision, recall, F1, AUC, and considerations of ethics, privacy, bias mitigation, and model interpretability in mental health and public health contexts?"}
{"original_query": "Could you recommend datasets that include SQL annotations over WikiTQ?", "expanded_query": "Could you recommend datasets that include SQL annotations over WikiTQ, such as WikiSQL, WikiTQ-SQL, or other natural language to SQL datasets for question answering over Wikipedia, semantic parsing, structured query language, and knowledge base queries?"}
{"original_query": "Could you recommend studies that provide a baseline for experiments using supervised constituency parsers with a focus on few-shot learning settings and have also reported on the use of pre-training and data augmentation techniques for parser performance improvement?", "expanded_query": "Could you recommend studies that provide a baseline for experiments using supervised constituency parsers with a focus on few-shot learning settings and have also reported on the use of pre-training and data augmentation techniques for parser performance improvement, including transformer-based models such as BERT, RoBERTa, GPT, XLNet, datasets like Penn Treebank, Universal Dependencies, CoNLL, evaluation metrics such as F1, LAS, UAS, and techniques such as back-translation, paraphrasing, synthetic data generation, transfer learning, cross-lingual adaptation, and citing key works like Liu et al. 2020, Zhang et al. 2021, and others?"}
{"original_query": "Could you suggest a paper that introduces an approach to relation extraction that involves learning syntax dependency structures using a tree LSTM model?", "expanded_query": "Could you recommend a research paper that introduces a neural network-based relation extraction approach using a tree LSTM model to learn syntactic dependency structures, incorporating dependency parsing, syntactic trees, deep learning, neural architecture, attention mechanisms, and benchmark datasets such as CoNLL, NYT, SemEval, or OpenIE, and comparing to transformer-based models like BERT, RoBERTa, or graph neural networks, while discussing joint learning, multi-task learning, and state-of-the-art performance?"}
{"original_query": "Could you suggest studies that employ novel methods for capturing data, specifically in the context of sarcasm detection on social media platforms like Twitter?", "expanded_query": "Could you suggest recent academic studies and conference papers that employ novel data capturing methods such as multimodal annotation, crowdsourcing, automated labeling, or advanced NLP techniques, specifically for sarcasm detection on social media platforms like Twitter, including datasets, transformer-based models, contextual embeddings, linguistic feature analyses, and sentiment or irony detection research?"}
{"original_query": "Has there been any recent work or competitions focused on the development of methods to counteract clickbait through spoiling, such as revealing key information upfront?", "expanded_query": "Recent research, datasets, and competitions on clickbait detection and mitigation via spoiling, revealing key information upfront, spoiler detection, spoiler removal, spoiler prevention, spoiler tags, spoiler warnings, content summarization, NLP, machine learning, AI, click-through rate, user engagement, clickbait mitigation, content moderation, spoiler detection datasets, spoiler detection challenges, Kaggle competitions, ACL, EMNLP, NeurIPS, ICLR, NLP benchmarks, evaluation metrics, spoiler detection algorithms, spoiler detection methods, spoiler detection evaluation, spoiler detection tools, spoiler detection systems, spoiler detection solutions"}
{"original_query": "I am exploring state-of-the-art techniques in language representation models that are trained to understand context from both the preceding and succeeding text. Where can I find foundational research on this topic, including information about the Transformer architecture, and the specific tasks such models are pre-trained on?", "expanded_query": "I am exploring state-of-the-art techniques in language representation models that are trained to understand context from both the preceding and succeeding text, such as bidirectional transformer-based models like BERT, RoBERTa, XLNet, and ELECTRA, and I want to find foundational research on this topic, including key papers such as \"Attention Is All You Need\", the BERT paper, XLNet, RoBERTa, and ELECTRA, and information about the Transformer architecture, self-attention, multi-head attention, positional encoding, layer normalization, feed-forward networks, as well as the specific pre-training tasks such as masked language modeling, next sentence prediction, sentence order prediction, and other unsupervised objectives, and the datasets and benchmarks used for pre-training and fine-tuning such as Wikipedia, BookCorpus, OpenWebText, GLUE, SuperGLUE, SQuAD, MNLI, and the venues like ACL, EMNLP, NAACL, ICLR, NeurIPS, and arXiv."}
{"original_query": "I am looking for research that has explored topic and frame conditioning in transformer language models to enhance the quality of generated argument claims. Is there a paper discussing this approach?", "expanded_query": "Search for research papers on topic and frame conditioning in transformer language models to improve generated argument claim quality, covering topic-aware and frame-aware generation, conditional language modeling, argument mining, claim quality metrics, transformer architectures (BERT, GPT, RoBERTa, XLNet), prompt engineering, and state-of-the-art NLP studies."}
{"original_query": "I am looking to understand more about sequence-to-sequence pre-training and its applications in natural language tasks. Can you suggest a significant paper that describes the denoising process for such models?", "expanded_query": "sequence-to-sequence pretraining denoising process natural language tasks summarization translation question answering BART T5 masked language modeling self-supervised learning transformer architecture denoising autoencoder text-to-text transfer transformer pretraining objectives corruption functions reconstruction objective"}
{"original_query": "I would like to understand the theoretical basis for using the nuclear norm of a weight matrix as a measure of complexity in linear models for probing tasks. Which paper should I refer to?", "expanded_query": "I would like to understand the theoretical basis for using the nuclear norm (trace norm) of a weight matrix as a measure of complexity in linear probing models for representation learning tasks, including connections to matrix norms, singular value decomposition, low\u2011rank regularization, Rademacher complexity, generalization bounds, and sample complexity in linear models, and which seminal papers or recent works (e.g., Srebro et al., Bach, Shalev\u2011Shwartz, Arora, Hsu, Kakade) should I refer to?"}
{"original_query": "I'm conducting research on computational humor and looking at various approaches to detect it within texts. What are some articles that explore features like repetition or use language models like GPT-2 for humor recognition?", "expanded_query": "I'm conducting research on computational humor and looking at various approaches to detect it within texts, including feature extraction methods such as repetition, syntactic patterns, semantic anomalies, sentiment shifts, sarcasm, irony, and the use of language models like GPT\u20112 and other transformer\u2011based architectures for humor recognition; what are some recent academic articles, conference papers, or journal studies that explore these features and methods in the context of humor detection, evaluation metrics, datasets, and state\u2011of\u2011the\u2011art performance?"}
{"original_query": "I'm exploring efficient transformer architectures for language embeddings and came across some work that utilizes advanced pre-trained models. Which paper should I reference to learn more about the use of XLM-R for multilingual representation learning in a transformer-based setting?", "expanded_query": "Which paper should I reference to learn more about the use of XLM\u2011R (XLM\u2011RoBERTa) for multilingual representation learning in a transformer\u2011based setting, including efficient transformer architectures, cross\u2011lingual embeddings, pre\u2011trained language models, parameter efficiency, knowledge distillation, model compression, and the original XLM\u2011R: Cross\u2011Lingual Language Model RoBERTa paper?"}
{"original_query": "I'm exploring research that utilizes large datasets for the task of sentence simplification. Are there any prominent datasets sourced from Wikipedia that I could look into?", "expanded_query": "Large-scale sentence simplification datasets sourced from Wikipedia, including Simple English Wikipedia, WikiLarge, Simple Wikipedia corpora, Wikipedia-based simplification corpora, NLP text simplification datasets from Wikipedia, prominent Wikipedia-derived sentence simplification datasets, Simple English Wikipedia dataset, WikiLarge dataset, other Wikipedia-based simplification corpora, lexical simplification, syntactic simplification, text simplification, sentence simplification, research, NLP, machine learning, deep learning, transformer models, evaluation metrics, BLEU, SARI, FKGL, readability, Flesch\u2011Kincaid, simplification tasks, corpora, datasets, and related resources."}
{"original_query": "I'm exploring ways to enhance question answering systems through domain adaptation. Could you point me towards research that specifically focuses on synthetic data generation for this purpose?", "expanded_query": "I'm exploring ways to enhance question answering systems through domain adaptation, synthetic data generation, data augmentation, transfer learning, cross\u2011domain QA, domain\u2011specific corpora, language model fine\u2011tuning, and I would like to find research papers, conference proceedings, journal articles, arXiv preprints, ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, and other NLP and machine learning venues that specifically focus on synthetic data generation for domain adaptation in QA systems."}
{"original_query": "I'm interested in understanding how perplexity is utilized in identifying misinformation or fact-checking. Are there studies discussing this application of perplexity?", "expanded_query": "How is perplexity employed in transformer\u2011based language models such as GPT\u20114, BERT, RoBERTa, XLNet, and T5 for detecting misinformation, fact\u2011checking, and source credibility, and what peer\u2011reviewed studies, conference papers (ACL, EMNLP, NAACL, ICLR, NeurIPS), arXiv preprints, and datasets (FakeNewsNet, LIAR, PolitiFact, Snopes) discuss using perplexity as a metric for model uncertainty, novelty detection, or textual entailment in misinformation detection, including evaluation metrics like precision, recall, F1, accuracy, human evaluation, crowdsourcing, bias mitigation, ethical AI, multilingual misinformation detection, and cross\u2011lingual transfer?"}
{"original_query": "I'm looking for a comprehensive dataset that has been influential in fact verification research", "expanded_query": "Looking for a comprehensive, influential fact verification dataset used in NLP and machine learning research, such as FEVER, ClaimBuster, PolitiFact, Snopes, Hoaxy, FakeNewsNet, or the Fact Extraction and VERification corpus, that includes annotated claims, source credibility, knowledge base links, multi\u2011lingual coverage, and is publicly available for benchmarking, training, and evaluation of transformer models like BERT, RoBERTa, XLNet, GPT, and other NLP tasks in the fact\u2011checking research community."}
{"original_query": "I'm looking for a paper that discusses improvements in constituency parsing performance by applying a partition strategy for content embedding and positional embedding within self-attention and label attention layers.", "expanded_query": "Find a research paper on transformer-based neural constituency parsing that reports performance improvements by applying a partition strategy for content embedding and positional embedding within self-attention and label attention layers, detailing embedding partitioning techniques, attention head partitioning, evaluation on Penn Treebank and other treebanks, F1 score gains, comparisons to state-of-the-art models such as BERT, RoBERTa, XLNet, and other neural parsers, with citations from ACL, EMNLP, NAACL, NeurIPS, ICLR, arXiv, and journal publications."}
{"original_query": "I'm looking for innovative approaches to data annotation on platforms like Amazon Mechanical Turk that focus on maximizing document coverage. Is there any research discussing strategies that balance the trade-off between full annotation and broader document coverage?", "expanded_query": "Looking for innovative crowdsourced data annotation strategies on platforms such as Amazon Mechanical Turk, CrowdFlower, and Prolific that maximize document coverage while balancing the trade-off between full annotation and broader coverage, including research on annotation quality control, task design, incentive mechanisms, budget allocation, partial labeling, active learning, semi\u2011supervised learning, annotation guidelines, coverage metrics, and cost\u2011efficiency studies."}
{"original_query": "I'm looking into morphological embedding algorithms that build upon the word2vec model by utilizing character n-grams. Which papers should I read to learn more about this approach?", "expanded_query": "Which research papers by Bengio et al. 2003, Mikolov et al. 2013, Bojanowski et al. 2017, and others on morphological embedding algorithms that extend word2vec with character n\u2011grams, subword or subword\u2011level embeddings, fastText, subword\u2011based word embeddings, subword representation learning for morphologically rich languages, rare and OOV words, low\u2011resource languages, subword embeddings for rare words, subword embeddings for OOV words, and subword embeddings for morphologically rich languages should I read?"}
{"original_query": "I'm looking into the distillation process of language models and would like to examine studies that specifically discuss the attention mechanism alignment in the teacher-student model architecture. Are there any papers you can suggest?", "expanded_query": "Looking for research papers on knowledge distillation of transformer-based language models that focus on attention mechanism alignment between teacher and student architectures, including attention transfer, attention supervision, attention weight alignment, cross-attention alignment, attention distillation loss, attention map comparison, BERT, GPT, RoBERTa, transformer compression, fine-grained attention alignment metrics, and studies on attention alignment evaluation in teacher\u2011student models."}
{"original_query": "I'm researching insertion-based decoding methods for semantic parsing and language modeling, and I'm looking for works that discuss alternatives to traditional loss functions such as cross-entropy, particularly those using Kullback\u2013Leibler divergence in this context. Could you point me to some studies on this?", "expanded_query": "Looking for research papers on insertion\u2011based decoding methods for semantic parsing and language modeling that explore alternative loss functions to cross\u2011entropy, especially those employing Kullback\u2013Leibler divergence, including works on sequence\u2011to\u2011sequence and transformer architectures, non\u2011autoregressive models, structured prediction, probabilistic inference, variational inference, entropy regularization, and related benchmarks such as SQL generation, SPARQL, logical form generation, semantic role labeling, natural language inference, BLEU, ROUGE, and other semantic parsing datasets."}
{"original_query": "I'm researching on the efficacy of recurrent networks in language modeling and CCG supertagging. Could you point me to studies that explore LSTM architectures and model comparisons in these tasks?", "expanded_query": "Find research papers and conference proceedings on recurrent neural networks, especially LSTM, GRU, bidirectional LSTM, stacked LSTM, and deep LSTM architectures, comparing their performance in language modeling tasks such as next-word prediction on Penn Treebank, WikiText-2, WikiText-103, and LM1B, using metrics like perplexity, accuracy, and F1, and in CCG supertagging tasks, focusing on token-level classification, sequence labeling, syntactic parsing, supertagging accuracy, with benchmark datasets, evaluation protocols, cross-validation, statistical significance, error analysis, ablation studies, attention mechanisms, and transfer learning, and also compare with Transformer-based models (BERT, RoBERTa, XLNet), including references to ACL, EMNLP, NAACL, COLING, NeurIPS, ICLR, ICML, arXiv, Google Scholar, and terms such as contextualized embeddings (ELMo, BERT embeddings), word embeddings (fastText, GloVe), subword tokenization (Byte-Pair Encoding, WordPiece, SentencePiece), and evaluation metrics like macro/micro F1, perplexity, and accuracy."}
{"original_query": "I'm searching for studies that explore advancements in dependency parsing, particularly using graph-to-graph transformers with iterative refinement processes. Which publications should I look into?", "expanded_query": "Looking for studies on advanced dependency parsing using graph\u2011to\u2011graph transformers with iterative refinement, covering graph neural networks, transformer\u2011based models, graph attention and convolutional networks, structured attention, multi\u2011step refinement, neural dependency parsing, state\u2011of\u2011the\u2011art benchmarks such as Universal Dependencies and CoNLL, and key publications from ACL, EMNLP, NAACL, COLING, and recent works by Zhou, Liu, Zhang, and others on graph transformer architectures for syntactic parsing."}
{"original_query": "In discourse parsing literature, which works have explored parser performance by adopting the original Parseval procedure and reporting micro-averaged F1 scores?", "expanded_query": "In discourse parsing literature, which research papers and studies have examined parser performance by employing the original Parseval procedure, reporting micro\u2011averaged F1 scores, and how do these works compare across RST parsing, discourse treebank evaluation, discourse segmentation, discourse relation classification, Rhetorical Structure Theory, Penn Discourse Treebank, micro\u2011averaged versus macro\u2011averaged F1 metrics, parser evaluation benchmarks, and other discourse parsing datasets and models?"}
{"original_query": "In researching metrics for human-interaction with computer-assisted translation tools, which studies have analyzed parameters such as the keystroke ratio and mouse action ratio to quantify user effort or engagement?", "expanded_query": "In researching metrics for human-interaction with computer-assisted translation tools, which studies have analyzed parameters such as the keystroke ratio and mouse action ratio to quantify user effort or engagement, including user effort metrics, interaction logs, keystroke dynamics, mouse movement analysis, translation productivity, post-editing effort, translation memory usage, SDL Trados, MemoQ, Wordfast, OmegaT, Memsource, HCI metrics, ergonomics, user fatigue, interface usability, translation workflow, user experience, interaction cost, translation speed, user behavior, user engagement, translation quality, and related research?"}
{"original_query": "In the area of argument mining, could you point to literature that uses of dependency parsers to determine the argumentativeness of texts in dialogue systems?", "expanded_query": "Argument mining, dependency parsing, syntactic dependencies, argumentativeness detection, argumentative discourse analysis, dialogue systems, conversational AI, natural language processing, machine learning, deep learning, transformer models, BERT, RoBERTa, graph neural networks, semantic role labeling, argument structure, argumentative stance detection, dialogue act classification, argument mining literature, dependency-based argument detection, argument mining datasets, dialogue corpora, argumentative dialogue, argument mining research, literature review, citation, papers, ACL, EMNLP, NAACL, COLING, LREC, AAAI, IJCNLP, Journal of Artificial Intelligence Research, Computational Linguistics, dependency parser models, spaCy, Stanford CoreNLP, UDPipe, argumentativeness classification, argumentative text classification, argument mining in chatbots, argumentative dialogue generation, argument mining in dialogue systems."}
{"original_query": "In the context of Named Entity Recognition tasks across multiple languages, which work highlights the necessity of retrieving related knowledge to aid in the annotation of ambiguous named entities?", "expanded_query": "In the context of cross\u2011lingual and multilingual Named Entity Recognition tasks, which scholarly work emphasizes the necessity of retrieving related knowledge from knowledge bases such as Wikipedia, DBpedia, or Wikidata to aid in the annotation and disambiguation of ambiguous named entities, leveraging contextual embeddings, entity linking, and knowledge graph embeddings for improved annotation guidelines and annotation workflows?"}
{"original_query": "In the context of machine translation, can you point me towards literature discussing the specifications for setting up encoder/decoder layers, attention heads, and other hyperparameters for a neural network model?", "expanded_query": "In the context of machine translation, can you point me towards literature discussing the specifications for setting up encoder/decoder layers, attention heads, and other hyperparameters for a neural network model, including Transformer architecture, BERT, GPT, Seq2Seq, self\u2011attention, multi\u2011head attention, positional encoding, layer normalization, dropout, learning rate schedules, Adam optimizer, beam search, BLEU score, NMT, OpenNMT, Tensor2Tensor, fairseq, MarianNMT, Transformer\u2011XL, XLNet, T5, mBART, hyperparameter tuning, grid search, Bayesian optimization, neural architecture search, PyTorch, TensorFlow, Keras, tokenization, BPE, SentencePiece, vocabulary size, batch size, gradient clipping, weight decay, regularization, and related research papers from ACL, EMNLP, NAACL, ICLR, NeurIPS, ICML, arXiv."}
{"original_query": "In the context of natural language processing, I am looking for research that explores the relationship between a model's prediction entropy and its tendencies to copy existing text versus generating novel content. Can you recommend a paper?", "expanded_query": "Natural language processing research exploring the relationship between a language model\u2019s prediction entropy and its tendency to copy existing text versus generate novel content, including studies on entropy\u2011based sampling, temperature, top\u2011k/top\u2011p, beam search, entropy regularization, copy bias, memorization, novelty detection, hallucination, and generation diversity in transformer\u2011based models such as GPT\u20113, GPT\u20114, BERT, XLNet, and their impact on output quality and content originality."}
{"original_query": "In the context of simultaneous machine translation, which tool or technique could I use to generate ground-truth alignments for training models to interpret and generate translations incrementally?", "expanded_query": "In the context of simultaneous machine translation, which tool or technique could I use to generate ground\u2011truth alignments for training models to interpret and generate translations incrementally, such as fast_align, GIZA++, IBM Model 4, alignment extraction from parallel corpora like Europarl or OpenSubtitles, phrase or word alignment, sentence alignment, transformer attention alignment, incremental alignment extraction for low\u2011latency or streaming translation, alignment generation for incremental decoding, alignment annotation for partial input and output, alignment quality evaluation metrics, and alignment datasets for incremental translation research?"}
{"original_query": "In the field of reinforcement learning models for multi-hop reasoning, what issue involves an agent erroneously correlating a successful outcome with irrelevant or coincidental actions, and are there any papers discussing this phenomenon?", "expanded_query": "In the field of reinforcement learning models for multi\u2011hop reasoning, what issue involves an agent erroneously correlating a successful outcome with irrelevant or coincidental actions, often called spurious correlation or reward hacking, and are there any papers\u2014such as those presented at NeurIPS, ICLR, ACL, EMNLP, AAAI, IJCAI, or posted on arXiv\u2014by researchers from OpenAI, DeepMind, Google Research, Microsoft Research, Stanford, MIT, University of Toronto, Oxford, Cambridge, or other institutions that discuss this phenomenon in the context of policy gradient, actor\u2011critic, Q\u2011learning, transformer\u2011based RL, graph neural networks, chain\u2011of\u2011thought prompting, multi\u2011hop QA, knowledge graph completion, semantic parsing, counterfactual reasoning, causal inference, explainability, bias mitigation, dataset curation, reward shaping, spurious reward signals, robustness, and generalization to unseen queries?"}
{"original_query": "What approaches have been used to address the limitations of the beam search method in neural machine translation systems in terms of considering the full target context?", "expanded_query": "What approaches have been used to address the limitations of the beam search method in neural machine translation systems in terms of considering the full target context, including dynamic beam width, coverage penalty, length normalization, diversity-promoting beam search, nucleus sampling, top\u2011k sampling, transformer models, attention mechanisms, contextual embeddings, decoder state, global context modeling, reinforcement learning, policy gradient, coverage models, semantic coherence, syntactic consistency, post\u2011editing, human\u2011in\u2011the\u2011loop, parallel decoding, ensemble decoding, language model integration, BERT, GPT, context\u2011aware decoding, target\u2011side context, source\u2011target alignment, attention coverage, beam search variants, search space pruning, hypothesis ranking, beam search with diversity, beam search with length penalty, beam search with coverage penalty, beam search with context gating, beam search with target\u2011side context modeling, neural machine translation, sequence\u2011to\u2011sequence, transformer architecture, attention\u2011based models, RNN\u2011based models, encoder\u2011decoder, contextualized decoding, target context modeling, full target context, beam search limitations, search algorithm, decoder state, contextual embeddings, contextualized language models."}
{"original_query": "What are some approaches to generating sports news reports from event data and what models have been used for this task in Finnish language NLP?", "expanded_query": "What natural language generation approaches, including rule-based, template-based, neural, transformer-based, and data-to-text pipelines, have been applied to generate sports news reports from structured event data, and which Finnish language models (such as FinBERT, Finnish GPT-2, Finnish BART, FinGPT, and other transformer-based models) have been used for this task in Finnish NLP, including any available Finnish sports news corpora, datasets, and evaluation benchmarks?"}
{"original_query": "What are some good datasets for conversational question answering?", "expanded_query": "What are the best publicly available large-scale datasets for conversational question answering, including multi-turn dialogue QA, knowledge-grounded QA, retrieval-based QA, generative QA, and benchmark datasets such as ConvAI2, DSTC, QuAC, CoQA, MultiWOZ, SQuAD, and other resources for training and evaluating conversational AI and chatbots?"}
{"original_query": "What are some of the key papers to look at for understanding how attention mechanisms have been used to enhance bidirectional recurrent neural networks in relation classification tasks?", "expanded_query": "Key papers on attention mechanisms enhancing bidirectional recurrent neural networks for relation classification tasks, including seminal works on attention\u2011based BiLSTM and BiGRU for relation extraction, transformer\u2011based self\u2011attention in NLP, hierarchical attention networks, attention visualization, contextual embeddings such as BERT, ELMo, and RoBERTa, their impact on relation classification benchmarks, recent advances in attention\u2011based BiRNN architectures, and studies on attention weight interpretability in relation extraction."}
{"original_query": "What are some recent advancements in training systems to parse complex multi-hop questions into a sequence of simpler query steps for improved question answering?", "expanded_query": "Recent advancements in training systems for parsing complex multi\u2011hop questions into simpler query steps include chain\u2011of\u2011thought prompting, neural module networks, graph neural networks, semantic parsing, knowledge graph traversal, retrieval\u2011augmented generation, transformer models like GPT\u20114, PaLM, Gopher, Turing, BERT, RoBERTa, and their applications in improved question answering across domains such as biomedical, legal, and general knowledge, leveraging research from OpenAI, Google DeepMind, Microsoft Research, Allen Institute for AI, and datasets like HotpotQA, WikiHop, ComplexWebQuestions, and OpenBookQA, with a focus on explainable AI, step\u2011by\u2011step reasoning, question decomposition frameworks, semantic role labeling, dependency parsing, neural\u2011symbolic integration, and knowledge graph embeddings."}
{"original_query": "What are some soft-constrained methods proposed in the literature for terminology translation in neural machine translation systems, and how do they differ from hard-constrained decoding methods that might degrade translation quality or increase complexity?", "expanded_query": "Which soft\u2011constrained terminology translation methods have been proposed in neural machine translation literature, such as coverage penalty, lexical constraint integration, constrained beam search with penalty, reinforcement learning reward shaping, attention\u2011based constraint gating, and how do they compare to hard\u2011constrained decoding approaches like forced decoding, hard constraint beam search, dictionary\u2011based forced substitution, or hard constraint alignment in terms of translation quality metrics (BLEU, TER, COMET), computational complexity, decoding latency, and risk of degrading translation fluency or increasing search space, across frameworks like OpenNMT, Marian, Transformer, BERT, GPT, T5, mBART, on datasets such as WMT, IWSLT, TED, in domains like medical, legal, technical, and using terminology tables or bilingual lexicons?"}
{"original_query": "What are some studies that leverage statistical machine translation methodologies, like GIZA++, to improve the alignment process between words and grammar rules in the context of semantic parsing and generation of meaning representations?", "expanded_query": "Which studies use statistical machine translation methods such as GIZA++, fast_align, IBM Model 4, and phrase-based alignment to improve word\u2011to\u2011grammar rule alignment for semantic parsing and meaning representation generation, including approaches that leverage parallel corpora, alignment error rate metrics, and evaluation on datasets like GeoQuery, ATIS, or AMR, and how do these studies integrate neural models such as BERT or Transformer\u2011based MT frameworks to enhance semantic parsing accuracy?"}
{"original_query": "What are some techniques or tools used in machine learning for matching and grounding annotated data to an existing knowledge base, particularly using sentence embedding-based cosine similarity or clustering algorithms?", "expanded_query": "What are some techniques or tools used in machine learning for matching and grounding annotated data to an existing knowledge base, particularly using sentence embedding\u2011based cosine similarity or clustering algorithms, such as entity linking, knowledge graph embeddings (TransE, RotatE, DistMult), sentence transformers (SBERT, RoBERTa, GPT\u2011based embeddings), approximate nearest neighbor search (FAISS, Annoy, HNSW), vector databases (Milvus, Pinecone, Weaviate, Qdrant, Elasticsearch vector search), clustering methods (K\u2011means, DBSCAN, Agglomerative, spectral clustering, hierarchical clustering), dimensionality reduction (t\u2011SNE, UMAP), evaluation metrics (silhouette score, Davies\u2011Bouldin, ARI), graph neural networks for knowledge base completion, ontology alignment, schema matching, entity resolution, and contrastive learning or triplet loss for fine\u2011tuning domain\u2011specific embeddings."}
{"original_query": "What are the recent developments in evaluating the flow or 'streaming degree' of the translation processes in simultaneous machine translation (SiMT), and which metric has proven useful for this purpose?", "expanded_query": "What are the recent developments in evaluating the flow or streaming degree of simultaneous machine translation (SiMT) processes, including latency, wait time, real\u2011time translation, quality\u2011latency trade\u2011off, BLEU, METEOR, WER, streaming degree metric, streaming degree of translation, real\u2011time evaluation frameworks, dynamic programming, online evaluation, Transformer\u2011based SiMT, encoder\u2011decoder architectures, beam search, and which metric has proven useful for this purpose, such as the streaming degree metric, as applied in systems from Google, Microsoft, DeepL, OpenNMT, and academic research from Stanford, MIT, University of Washington, and University of Edinburgh?"}
{"original_query": "What paper should I look at if I am interested in the challenges of compositional generalization in the context of semantic parsing, especially regarding the impact of unseen local structures in program outputs?", "expanded_query": "Which research papers on compositional generalization in semantic parsing address the impact of unseen local structures in program outputs, neural semantic parsing, sequence\u2011to\u2011tree models, syntactic constraints, data sparsity, evaluation metrics, benchmark datasets such as SCAN, ATIS, GeoQuery, Lambda calculus, SPARQL, transformer\u2011based approaches, attention mechanisms, structured prediction, and state\u2011of\u2011the\u2011art methods?"}
{"original_query": "What papers should I refer to if I want to explore datasets and shared tasks designed for the purpose of fact verification using evidence from Wikipedia?", "expanded_query": "What papers should I refer to if I want to explore datasets and shared tasks designed for the purpose of fact verification using evidence from Wikipedia, such as the FEVER dataset and its shared tasks from 2018 to 2024, other Wikipedia-based fact-checking corpora, evidence retrieval benchmarks, and recent NLP conferences like ACL, EMNLP, NAACL, and the associated papers on claim verification, evidence extraction, and textual entailment?"}
{"original_query": "What prior works suggested that exposure bias could lead to hallucinations in neural machine translation models?", "expanded_query": "Which prior works, such as Sennrich et al. (2016) on back\u2011translation, Vaswani et al. (2017) on the Transformer, and others, suggested that exposure bias in neural machine translation models could lead to hallucinations, semantic drift, faithfulness errors, and how do these findings relate to teacher forcing, scheduled sampling, beam search, coverage penalty, and evaluation metrics like BLEU, METEOR, chrF, TER, human evaluation, on datasets such as WMT, Europarl, TED talks, and OpenSubtitles, and how do they inform improvements in decoder design, attention mechanisms, and language model integration?"}
{"original_query": "What research could I reference to understand the methodology for binarizing non-binary subtrees in the context of discourse parsing?", "expanded_query": "What research could I reference to understand the methodology for binarizing non-binary subtrees in the context of discourse parsing, including RST parsing, Penn Discourse Treebank, treebank-based evaluation, binarization heuristics, bottom-up and top-down strategies, neural network models, transformer-based discourse parsing, graph neural networks, structured prediction, CRFs, sequence labeling, and evaluation metrics such as F1, precision, recall, across ACL, EMNLP, NAACL, COLING, LREC, SIGDAT conferences?"}
{"original_query": "What research has been done on annotating user comments with claim verifiability", "expanded_query": "What research has been done on annotating user comments with claim verifiability, including datasets, annotation guidelines, inter-annotator agreement, crowdsourcing methods, NLP pipelines, machine learning models, fact-checking frameworks, social media platforms, user-generated content, verifiability scoring, claim classification, source credibility, knowledge graphs, and related academic literature from conferences such as ACL, EMNLP, NAACL, ICLR, ICWSM, and journals like IEEE, ACM, and Springer."}
{"original_query": "What research has been done on improving named entity recognition tasks by augmenting them with retrieval of external contexts?", "expanded_query": "What research has been done on improving named entity recognition tasks by augmenting them with retrieval of external contexts, including retrieval\u2011augmented NER, knowledge base integration, contextual embeddings, transformer\u2011based retrieval augmentation, entity linking, document\u2011level NER, cross\u2011document coreference, knowledge graph retrieval, few\u2011shot NER, domain adaptation, and recent surveys from 2020 to 2024?"}
{"original_query": "What research should I explore to understand methods for matching word senses in contexts using embeddings without relying on external lexical resources?", "expanded_query": "What research should I explore to understand methods for matching word senses in contexts using embeddings without relying on external lexical resources, including contextualized word embeddings, sense induction, unsupervised sense clustering, transformer-based sense representations, sense similarity metrics, sense alignment techniques, and evaluation of sense mapping in raw corpora?"}
{"original_query": "When using pretrained transformer models for generating sentence embeddings, I've heard different strategies such as mean pooling and using the CLS token's embedding. What study shows that mean pooling outperforms CLS in semantic similarity tasks?", "expanded_query": "What empirical study or benchmark, such as the STS Benchmark, SICK, or STS 2017/2018 datasets, demonstrates that mean pooling of transformer hidden states (e.g., BERT\u2011base, BERT\u2011large, RoBERTa\u2011base, DistilBERT) outperforms using the CLS token embedding for sentence embeddings in semantic similarity tasks, and what are the key methodological details, experimental results, and conclusions of that study, including references to Sentence\u2011BERT, cosine similarity, and pooling strategy comparisons?"}
{"original_query": "Where can I find a corpus of CCG annotations for natural language processing tasks, and what notable work has leveraged this corpus specifically in the domain of supertagging?", "expanded_query": "Where can I find a corpus of CCG annotations for natural language processing tasks, such as the CCGbank treebank derived from the Penn Treebank, and what notable work has leveraged this corpus specifically in the domain of supertagging, including studies by Zettlemoyer and Collins, Brent, the Stanford CCG parser, the Berkeley CCG parser, the RNN-based supertagger by Kiperwasser and Goldberg, the Transformer-based supertagger by Liu et al., and the recent BERT/XLNet fine\u2011tuning experiments on CCGbank?"}
{"original_query": "Where can I find a detailed discussion on automating the assessment of clarifications in instructional text, including tasks for grading these clarifications as plausible, implausible, or neutral and ranking them on a scale?", "expanded_query": "Where can I find a detailed discussion on automating the assessment of clarifications in instructional text, including tasks for grading these clarifications as plausible, implausible, or neutral and ranking them on a scale, with references to natural language processing, machine learning, deep learning, transformer models such as BERT, RoBERTa, GPT\u20113, GPT\u20114, evaluation metrics, semantic similarity, contradiction detection, knowledge graphs, instructional design, learning analytics, educational technology, annotation guidelines, dataset creation, benchmarking, and evaluation frameworks from institutions like Stanford NLP, MIT, Harvard, OpenAI, Hugging Face, and research papers on clarification detection, plausibility scoring, neutral classification, and ranking algorithms?"}
{"original_query": "Where can I find a discourse treebank tailored to Chinese newswire articles that's large enough to make training, development, and test splits?", "expanded_query": "Where can I find a large-scale, open-source Chinese newswire discourse treebank with annotated discourse trees for Chinese newswire articles, suitable for creating training, development, and test splits for NLP research, discourse parsing, and machine learning experiments, and what resources or repositories provide such a dataset?"}
{"original_query": "Where can I find a large corpus of annotated social media posts concerning a variety of health conditions?", "expanded_query": "Where can I find a large publicly available corpus of annotated social media posts from platforms such as Twitter, Reddit, Facebook, and Instagram covering a variety of health conditions including mental health, chronic diseases, and medical conditions, with annotations for disease labels, sentiment, and medical ontologies like SNOMED CT, ICD-10, MeSH, and UMLS, suitable for NLP, text mining, health informatics, and digital epidemiology research, and what data repositories, licensing terms, privacy considerations, and data sharing agreements are relevant?"}
{"original_query": "Where can I find a multilingual corpus that includes reviews on DVDs, music, and books for the purposes of cross-language sentiment classification research?", "expanded_query": "Where can I find a publicly available multilingual corpus of DVD, music, and book reviews in multiple languages such as English, French, German, Spanish, and Chinese for cross-language sentiment classification research in NLP, including cross-lingual sentiment analysis datasets, multilingual sentiment datasets, open source review corpora, and free downloadable resources for text classification?"}
{"original_query": "Where can I find a paper that discusses annotating events in text with degrees of factuality, including categories such as certain, probable, and underspecified?", "expanded_query": "Where can I find a scholarly article or conference paper that discusses annotating events in text with degrees of factuality, including categories such as certain, probable, and underspecified, and covers factuality annotation frameworks, event factuality corpora, the FEVER dataset, FactBank, event coreference, and related NLP tasks such as RTE, SemEval, TAC, news or scientific text factuality detection?"}
{"original_query": "Where can I find guidelines on standard practices for probing machine learning models to discern what information the models have captured without training them on a new task?", "expanded_query": "Where can I find guidelines on standard practices for probing machine learning models to discern what information the models have captured without training them on a new task, including model interpretability, representation analysis, feature attribution, knowledge extraction, model introspection, zero\u2011shot probing, pretrained transformer models, BERT, GPT, Hugging Face, PyTorch, TensorFlow, scikit\u2011learn, model auditing, evaluation metrics, benchmarks, tutorials, research papers, workshops, conferences, and community resources?"}
{"original_query": "Where can I find information on self-attentive parsers that have been trained in a few-shot learning setting, including their official code and hyperparameters?", "expanded_query": "Where can I find information on self-attentive parsers trained in a few-shot learning setting, including official code, hyperparameters, GitHub repositories, training scripts, config files, transformer-based architecture, attention heads, hidden size, embedding dimension, dropout, layer normalization, positional encoding, tokenizer, pretrained embeddings like BERT, RoBERTa, GPT, ELMo, GloVe, fastText, as well as evaluation metrics such as LAS, UAS, F1 on benchmark datasets like CoNLL, Universal Dependencies, GLUE, SuperGLUE, and details on few-shot fine-tuning, meta-learning approaches (MAML, Reptile), optimizer settings (AdamW, learning rate, weight decay, gradient clipping), batch size, epochs, steps, and any published papers or conference proceedings from Stanford NLP, AllenNLP, Hugging Face, Google AI, OpenAI, and other NLP research groups."}
{"original_query": "Where can I find interdisciplinary research that investigates how creative natural language generation (NLG) systems are evaluated?", "expanded_query": "interdisciplinary research on evaluating creative natural language generation systems, creative NLG evaluation methods, human-centered evaluation, automatic evaluation metrics, qualitative and quantitative analysis, human-computer interaction, cognitive science, psycholinguistics, computational creativity, AI ethics, evaluation frameworks, benchmark datasets, user studies, cross-disciplinary evaluation, fluency, coherence, originality, relevance, semantic accuracy, evaluation protocols, interdisciplinary collaboration, research venues, journals, conferences, digital libraries, Google Scholar, Semantic Scholar, arXiv, ACM Digital Library, IEEE Xplore, SpringerLink, Elsevier, Wiley Online Library, Open Access, interdisciplinary research databases, research communities, NLG evaluation workshops, NLG evaluation challenges, creative NLG evaluation frameworks, interdisciplinary evaluation methods, evaluation of creative AI, creative natural language generation evaluation, NLG evaluation interdisciplinary research, creative NLG evaluation interdisciplinary studies, interdisciplinary research on evaluating creative NLG systems, creative NLG evaluation interdisciplinary research sources"}
{"original_query": "Where can I find multilingual datasets used for the task of intended sarcasm detection, particularly involving English and Arabic?", "expanded_query": "Where can I find multilingual sarcasm detection datasets, particularly English\u2011Arabic bilingual corpora, cross\u2011lingual sarcasm detection resources, Arabic sarcasm corpora, English sarcasm datasets, SARC dataset, SemEval sarcasm detection, Arabic\u2011English sarcasm detection datasets for NLP research, machine learning, transformer models, BERT, sentiment analysis, cross\u2011lingual NLP, academic resources, and other relevant corpora?"}
{"original_query": "Where can I find research about automatic evaluation metrics in summarization tasks disagree with each other?", "expanded_query": "Where can I find research papers, surveys, and benchmark studies on automatic evaluation metrics for summarization tasks, including ROUGE, BLEU, METEOR, BERTScore, SummEval, NIST, CIDEr, and their inter\u2011metric disagreement, correlation, reliability, validity, bias, and coverage, as well as human evaluation comparisons, inter\u2011rater reliability, and evaluation frameworks, using sources such as Google Scholar, Semantic Scholar, ACL Anthology, arXiv, IEEE Xplore, SpringerLink, ACM Digital Library, and research institutions like Stanford NLP, MIT CSAIL, CMU NLP, and OpenAI?"}
{"original_query": "Where might I find a dataset annotated specifically for patronizing and condescending language to use in computational linguistics research?", "expanded_query": "Where might I find a publicly available, annotated dataset of patronizing, condescending, sarcastic, or negative politeness language for computational linguistics research, including sentiment analysis, discourse analysis, and speech act annotation, possibly sourced from social media platforms like Twitter, Reddit, or StackExchange, or from corpora such as the Corpus of Contemporary American English, the Corpus of Historical American English, the British National Corpus, or open-source repositories like Kaggle, Google Dataset Search, ACL Anthology, the UCI Machine Learning Repository, or the Open Multilingual WordNet, and what annotation guidelines, licensing terms, and ethical considerations apply to such data?"}
{"original_query": "Where might I find research on the evaluation of consistency in generated summaries?", "expanded_query": "Where can I find academic research, conference proceedings, journal articles, and datasets on evaluating consistency, coherence, factual accuracy, and semantic alignment in automatically generated summaries, including evaluation metrics such as ROUGE, BLEU, METEOR, BERTScore, semantic textual similarity, consistency evaluation frameworks, hallucination detection, and consistency evaluation methods in NLP summarization literature from sources like ACL Anthology, EMNLP, NAACL, COLING, LREC, arXiv, Google Scholar, IEEE Xplore, SpringerLink, ACM Digital Library, and related NLP conferences?"}
{"original_query": "Which corpora are frequently used in research to benchmark English readability assessment tools?", "expanded_query": "Which English language corpora, such as the Corpus of Contemporary American English, the British National Corpus, the Open American National Corpus, Project Gutenberg, Common Crawl, and the New York Times Annotated Corpus, are frequently used in research to benchmark English readability assessment tools like Flesch\u2011Kincaid, Gunning Fog, SMOG, Coleman\u2011Liau, Automated Readability Index, and other readability metrics in NLP, computational linguistics, and educational assessment studies?"}
{"original_query": "Which paper specifies the typical configurations used in fine-tuning deep bidirectional transformers like BERT and RoBERTa for language understanding tasks?", "expanded_query": "Which paper specifies the typical configurations used in fine\u2011tuning deep bidirectional transformers like BERT and RoBERTa for language understanding tasks, detailing hyperparameters such as learning rate, batch size, optimizer (AdamW), warmup steps, epoch count, sequence length, tokenization, dropout, regularization, gradient clipping, mixed precision, and evaluation metrics on GLUE, SQuAD, MNLI, QNLI, CoLA, RTE, STS\u2011B, WSC, WinoGrande, and other benchmarks, and provides a config file or code snippet in the Hugging Face transformers library?"}
{"original_query": "Which papers should I refer to for learning about the application of transformer language models to the generation of argumentative text conclusions, including the assessment of their novelty and validity?", "expanded_query": "Which research papers, surveys, and case studies should I consult to understand how transformer language models such as GPT\u20113, BERT, T5, XLNet, and other transformer architectures are applied to generate argumentative text conclusions, and how to evaluate their novelty, validity, coherence, fluency, semantic similarity, factual correctness, and ethical implications using automatic metrics like BLEU, ROUGE, BERTScore, semantic role labeling, dependency parsing, human judgment, crowdsourcing, novelty detection, plagiarism detection, fact\u2011checking frameworks, and evaluation guidelines for natural language generation and argument mining?"}
{"original_query": "Which studies should I look into that have explored the task of factuality in abstractive summarization and proposed ways to detect hallucinated content in generated text?", "expanded_query": "Which studies should I look into that have explored the task of factuality in abstractive summarization and proposed ways to detect hallucinated content in generated text, including research on factual consistency, faithfulness metrics, hallucination detection methods, transformer-based summarization models such as BART, T5, GPT\u20113, BERT, RoBERTa, evaluation frameworks like FactCC, SummaC, datasets such as CNN/DailyMail, XSum, Gigaword, and conferences like ACL, EMNLP, NAACL, ICLR, NeurIPS, as well as methods involving semantic similarity, knowledge graphs, fact\u2011checking, entity linking, and post\u2011hoc correction techniques?"}
{"original_query": "Which work should I explore to understand the techniques that expand the scope of open infomation extraction beyond verbs, to include various parts of speech such as nouns and adjectives?", "expanded_query": "Which research papers, books, or surveys on Open Information Extraction, dependency-based OIE, transformer-based OIE, contextual embeddings, and state-of-the-art frameworks such as OpenIE\u202f5.0 through\u202f10.0 that extend beyond verb\u2011centric extraction to include nouns, adjectives, and other parts of speech, and cover techniques like dependency parsing, semantic role labeling, and contextual embeddings?"}
{"original_query": "*Could you suggest a dataset with legally or ethically contentious content, and labels for acceptable and non-acceptable questions.", "expanded_query": "Could you suggest a dataset containing legally or ethically contentious content such as hate speech, defamation, privacy violations, sexual content, graphic violence, misinformation, political persuasion, or advice on illegal activities, with labels indicating acceptable and non-acceptable questions for use in content moderation, AI safety, and bias detection research, including annotation guidelines and compliance with EU GDPR, US First Amendment, and UK defamation law?"}
{"original_query": "Are there any papers on training video-language models with contrastive approahes and evaluation on temporal localization tasks?", "expanded_query": "Search for recent research papers on training video-language models with contrastive learning methods such as InfoNCE or triplet loss, focusing on temporal localization tasks including temporal action localization, temporal language grounding, and event detection, evaluated on datasets like ActivityNet Captions, THUMOS14, HowTo100M, YouCook2, MSR-VTT, using metrics such as mAP@IoU, and referencing works such as VideoCLIP, CLIP4Clip, TALL, TALL-CLIP, and other contrastive video-text grounding studies."}
{"original_query": "Are there any recent papers investigating the use of expert and anti-expert models together to guide text generation and mitigate toxic output?", "expanded_query": "Are there any recent 2023-2024 papers, including ACL 2024, EMNLP 2024, NeurIPS 2023, ICLR 2024, arXiv preprints, that investigate the use of expert and anti-expert models together to guide text generation, controlled generation, prompt engineering, or fine-tuning of large language models such as GPT\u20114, LLaMA, Claude, to mitigate toxic output, reduce bias, improve content moderation, and enhance AI safety, employing techniques like adversarial training, negative prompting, reinforcement learning from human feedback, human evaluation, and safety alignment research?"}
{"original_query": "Are there any research papers investigating the improvement of radiology report summarization through the application of graph neural networks in conjunction with biomedical entity extraction?", "expanded_query": "Are there any research papers or systematic reviews that investigate the improvement of radiology report summarization through the application of graph neural networks, such as graph convolutional or graph attention networks, in conjunction with biomedical entity extraction, named entity recognition, entity linking, and integration of medical ontologies like UMLS or SNOMED CT, within a clinical NLP pipeline that uses deep learning models like BioBERT or ClinicalBERT, and evaluate summarization quality with metrics such as ROUGE, BLEU, or clinical relevance scores, while also addressing extractive and abstractive summarization, knowledge graph integration, semantic graph representation, domain adaptation, transfer learning, and clinical decision support?"}
{"original_query": "Are there any studies investigating example-based approaches to predict user intent in few-shot learning contexts?", "expanded_query": "Are there any studies investigating example-based approaches to predict user intent in few-shot learning contexts, such as deep learning, transformer models, meta-learning, case-based reasoning, few-shot intent classification, NLP, BERT, GPT, and other recent research on few-shot learning for intent detection?"}
{"original_query": "Are there any studies investigating sentiment analysis through text-to-graph conversion models that incorporate contextual embeddings?", "expanded_query": "Which research studies and academic papers investigate sentiment analysis through text-to-graph conversion models that incorporate contextual embeddings such as BERT, RoBERTa, XLNet, and other transformer-based embeddings, using graph neural network architectures like GCN, GraphSAGE, GAT, graph transformer, or graph attention networks, applied to datasets such as SemEval, Twitter sentiment, Amazon product reviews, Yelp reviews, or other social media corpora, and how do these approaches leverage contextualized word embeddings for graph representation learning, node classification, edge prediction, and sentiment polarity detection in graph-based sentiment analysis tasks?"}
{"original_query": "Are there any studies on incorporating external commonsense knowledge into conversational models to enhance emotional support?", "expanded_query": "Studies on integrating external commonsense knowledge bases such as ConceptNet, ATOMIC, COMET, and Wikidata into conversational AI models like GPT, BERT, and Transformer-based dialog systems to enhance emotional support, affective computing, empathy, emotion recognition, mental health counseling, supportive chatbot performance, user satisfaction, clinical evaluation metrics, dialogue policy learning, reinforcement learning, knowledge graph embeddings, semantic grounding, context-aware response generation, affective dialogue, human-computer interaction, psychological well-being, knowledge injection, commonsense reasoning, knowledge retrieval, dialogue datasets such as EmpatheticDialogues and EmotionLines, evaluation frameworks, user studies, AI therapy, and social support."}
{"original_query": "Are there studies examining how well question answering systems perform on queries that cannot be directly recalled from their training data?", "expanded_query": "Are there studies examining how well question answering systems, including neural language models, transformer-based architectures, retrieval-augmented generation, and knowledge-graph integrated models, perform on queries that cannot be directly recalled from their training data, such as out-of-distribution, unknown, or non-recallable questions, and how these systems handle knowledge gaps, domain shifts, and adversarial prompts, evaluated on benchmark datasets like SQuAD, Natural Questions, MMLU, ARC, OpenBookQA, and using metrics such as accuracy, F1, EM, BLEU, ROUGE, METEOR, and human evaluation, and what insights have been drawn about generalization, robustness, hallucination, and knowledge integration in open-domain QA research?"}
{"original_query": "Are there studies that investigate debiasing language models automatically using prompting?", "expanded_query": "Are there studies that investigate debiasing language models automatically using prompting, prompt-based debiasing, prompt engineering, prompt-based bias mitigation, prompt-based bias reduction, prompt-based bias detection, prompt-based bias interventions, prompt-based bias mitigation methods, prompt-based bias mitigation techniques, prompt-based bias mitigation approaches, prompt-based bias mitigation experiments, prompt-based bias mitigation literature, prompt-based bias mitigation research, prompt-based bias mitigation studies, prompt-based bias mitigation in large language models, prompt-based bias mitigation in transformer models, prompt-based bias mitigation in GPT-3, GPT-4, BERT, RoBERTa, T5, LLMs, AI language models, NLP, transformer-based models, OpenAI, bias mitigation, automatic debiasing, bias reduction, fairness, bias detection, bias mitigation research, bias mitigation literature review, bias mitigation survey, bias mitigation analysis"}
{"original_query": "Can you give me a paper that does self-supervised contrastive learning of sentence embeddings by sampling in-batch negatives?", "expanded_query": "self-supervised contrastive learning sentence embeddings in-batch negative sampling research paper deep learning NLP SimCSE BERT representation learning semantic similarity negative sampling contrastive loss sentence similarity research article"}
{"original_query": "Can you recommend a dialogue summarization dataset mined from broadcast interviews on the TV or radio?", "expanded_query": "Can you recommend a publicly available dialogue summarization dataset mined from broadcast interviews on TV or radio, including TV interview transcripts, radio interview transcripts, audio\u2011to\u2011text transcripts, natural language processing resources, machine learning datasets, speech recognition datasets, interview summarization datasets, broadcast media datasets, open\u2011source speech summarization datasets, transcribed interviews, and any other relevant data sources?"}
{"original_query": "Can you recommend research that uses an LLM to generate better prompts/tempates given task input/output?", "expanded_query": "Can you recommend research papers or case studies on LLM-based prompt engineering and template generation that improve prompt quality for task-specific input/output scenarios, including methods for automated prompt generation, prompt optimization, prompt tuning, and transformer model-based prompt design?"}
{"original_query": "Can you show me a paper that built a large structured knowledge base from wikipedia, that can then be used for entity linking and ranking tasks?", "expanded_query": "large structured knowledge base built from Wikipedia for entity linking and ranking tasks knowledge graph construction entity disambiguation ranking evaluation semantic search information retrieval natural language processing knowledge base population OpenIE DBpedia Wikidata graph embeddings link prediction entity resolution benchmark research paper"}
{"original_query": "Can you suggest research that deals with the multiple input sources in clinical text processing by combining several recurrent modules which are each responsible for a single source of information?", "expanded_query": "Can you suggest research that deals with the multiple input sources in clinical text processing by combining several recurrent modules which are each responsible for a single source of information, including multi\u2011modal data fusion, recurrent neural networks such as LSTM and GRU, attention mechanisms, hierarchical attention, source\u2011specific modules, early and late fusion strategies, clinical natural language processing, electronic health records, structured and unstructured data, named entity recognition, clinical ontologies like SNOMED CT and UMLS, contextual embeddings such as BERT, clinicalBERT, BioBERT, graph neural networks, knowledge graph integration, transfer learning, multitask learning, domain adaptation, privacy\u2011preserving de\u2011identification, and predictive modeling for risk stratification and clinical decision support?"}
{"original_query": "Could you direct me to research that evaluates few-shot slot tagging model performance by averaging micro-F1 scores across different test episodes?", "expanded_query": "Could you direct me to research that evaluates few-shot slot tagging model performance by averaging micro\u2011F1 scores across different test episodes, including studies on few\u2011shot learning, slot filling, sequence labeling, cross\u2011domain adaptation, benchmark datasets such as SNIPS, ATIS, MultiWOZ, and state\u2011of\u2011the\u2011art models like BERT, RoBERTa, T5, and ELECTRA, as well as evaluation methodologies that compute average micro\u2011F1 across multiple test episodes?"}
{"original_query": "Could you direct me to studies investigating the enhancement of bi-encoder text matching performance through the application of knowledge distillation methods?", "expanded_query": "Which research studies or papers examine how knowledge distillation improves bi\u2011encoder text matching performance, particularly in transformer\u2011based sentence embeddings, using teacher\u2011student frameworks, distillation losses, temperature scaling, and soft label transfer, and evaluate on benchmark datasets such as MS\u202fMARCO, SNLI, Quora Question Pairs, Natural Questions, and others, with metrics like accuracy, recall, F1, and embedding similarity, in conferences like ACL, EMNLP, NAACL, ICLR, NeurIPS, and on arXiv or Google Scholar?"}
{"original_query": "Could you point me to research that tackles the issue of disambiguating word senses in infrequent and zero-shot scenarios?", "expanded_query": "Could you point me to research on word sense disambiguation for infrequent and zero-shot scenarios, including few-shot learning, zero-shot WSD, contextualized embeddings, transformer-based models like BERT, RoBERTa, GPT, sense induction, sense embeddings, knowledge graphs, WordNet, SemCor, SemEval, sense disambiguation benchmarks, low-resource languages, domain adaptation, semantic ambiguity, and related datasets?"}
{"original_query": "Could you recommend a contemporary research paper that has advanced natural language watermarking quality through algorithmic methods?", "expanded_query": "Could you recommend a recent state\u2011of\u2011the\u2011art research paper published in 2023 or 2024 that demonstrates significant improvements in natural language watermarking quality through advanced algorithmic methods such as transformer\u2011based embedding, neural network watermarking, adversarial robustness, and semantic fidelity metrics, and that has been presented at top venues like ACL, NeurIPS, ICLR, or published on arXiv, and includes evaluations using BLEU, ROUGE, perplexity, and watermark extraction robustness against adversarial attacks?"}
{"original_query": "Could you recommend a paper that builds a writing assistant with autocomplete capabilities conditioned on user intent?", "expanded_query": "Could you recommend a research paper that develops a writing assistant system with autocomplete functionality conditioned on user intent, incorporating natural language processing, intent classification, context\u2011aware suggestions, transformer\u2011based language models, user modeling, interactive writing tools, evaluation metrics for assistive writing, and adaptive, personalized suggestion algorithms?"}
{"original_query": "Could you recommend a study that does data-augmentation for biomedical named-entity recognition by replacing entities in the original sentence with entities of the same type from semantically similar sentences?", "expanded_query": "Could you recommend a study that performs data augmentation for biomedical named\u2011entity recognition by replacing entities in the original sentence with entities of the same type from semantically similar sentences, leveraging sentence similarity search using sentence transformers, entity type matching for proteins, genes, diseases, drugs, chemicals, and clinical entities, employing transformer\u2011based models such as BioBERT, SciBERT, or ClinicalBERT, and evaluating the augmentation pipeline on benchmark datasets like BioCreative, PubMed abstracts, and clinical notes, reporting precision, recall, F1, and cross\u2011validation results, and discussing synthetic biomedical text generation, entity substitution strategies, and domain adaptation for electronic health records?"}
{"original_query": "Could you recommend a study that examines how cross project code summarization evaluation methodologies compare to time-segmented eval methodology.", "expanded_query": "Could you recommend a study that examines how cross-project code summarization evaluation methodologies compare to time-segmented evaluation methodology, covering metrics such as BLEU, ROUGE, METEOR, semantic similarity, human evaluation, dataset selection from open-source GitHub projects, transfer learning, domain adaptation, statistical analysis, effect size, confidence intervals, reproducibility, and evaluation protocols in software engineering conferences and journals?"}
{"original_query": "Could you recommend a study that examines how incorporating external commonsense knowledge into conversational agents can better interpret user emotions and refine their response formulation techniques?", "expanded_query": "Could you recommend a study that examines how incorporating external commonsense knowledge from sources such as ATOMIC, ConceptNet, and WordNet into conversational agents, leveraging transformer-based language models like GPT\u20113, GPT\u20114, BERT, RoBERTa, or LLaMA, can enhance emotion recognition, affective computing, sentiment analysis, and user intent detection, thereby improving response formulation techniques, dialogue policy, and overall user satisfaction in affective dialogue systems, and what datasets, evaluation metrics, and knowledge grounding methods were used?"}
{"original_query": "Could you recommend a study that examines the intricacies of few-shot relation extraction challenges and introduces an approach integrating both global and local attributes alongside external descriptions of relations?", "expanded_query": "Could you recommend a study that examines the intricacies of few-shot relation extraction challenges, including cross-domain generalization and prototype-based meta-learning, and introduces an approach integrating both global attributes such as entity type and relation schema, local attributes such as contextualized token embeddings from transformer models, and external descriptions of relations sourced from knowledge graphs like Freebase or ConceptNet, while evaluating on benchmark datasets such as FewRel or TACRED and reporting metrics like F1, precision, recall, and zero-shot transfer performance?"}
{"original_query": "Could you recommend a study that explores a pre-trained multilingual text-to-text transformer applicable to text summarization tasks?", "expanded_query": "Could you recommend a study that explores a pre-trained multilingual text-to-text transformer such as mBART, mT5, or XLM-R for cross-lingual text summarization tasks, including zero-shot summarization, fine-tuning on multilingual datasets, evaluation with ROUGE or BLEU, and analysis of transfer learning and domain adaptation?"}
{"original_query": "Could you recommend a study that explores employing variational autoencoders to standardize open knowledge graphs?", "expanded_query": "Could you recommend a study that explores employing variational autoencoders and graph neural networks for standardizing open knowledge graphs, covering ontology alignment, entity resolution, knowledge graph embeddings, semantic interoperability, RDF, OWL, SPARQL, Linked Data, and evaluating performance on benchmark datasets such as Open Graph Benchmark and Open Knowledge Graph datasets using graph VAE, variational inference, and deep generative models?"}
{"original_query": "Could you recommend a study that explores how language models are not robust to the surface form editing when testing commonsense knowledge?", "expanded_query": "Could you recommend a recent empirical study or research paper that investigates the robustness of large neural language models such as GPT\u20113, BERT, or RoBERTa to surface form editing\u2014including syntactic perturbations, lexical substitutions, and adversarial rewrites\u2014when evaluating commonsense knowledge on benchmarks like Winograd Schema, CommonsenseQA, or ATOMIC, and that discusses the impact of surface\u2011level changes on model performance, semantic drift, and the reliability of commonsense reasoning?"}
{"original_query": "Could you recommend a study that explores mitigating bias in natural language understanding via example reweighting?", "expanded_query": "Could you recommend a peer-reviewed study that explores mitigating bias in natural language understanding via example reweighting, focusing on fairness metrics, dataset bias, class imbalance, importance weighting, adversarial debiasing, and empirical evaluation of language models such as BERT, GPT, and transformer-based classifiers in tasks like sentiment analysis, named entity recognition, question answering, and machine translation?"}
{"original_query": "Could you recommend a study that explores strategies for improving multi-label text classification by incorporating information about label distribution directly into the loss function?", "expanded_query": "Could you recommend a study or research paper that explores strategies for improving multi-label text classification by incorporating label distribution information directly into the loss function, such as weighted cross-entropy, focal loss, label smoothing, cost-sensitive learning, or label distribution aware loss, especially in the context of deep learning neural networks for NLP tasks, addressing class imbalance on datasets like Reuters-21578, MS MARCO, or Yelp, and published in venues such as ACL, EMNLP, NeurIPS, ICML, or on arXiv?"}
{"original_query": "Could you recommend a study that explores the difficulties in creating shared multilingual vocabularies, especially focusing on the problem of over segmentation in low-resource languages?", "expanded_query": "Could you recommend a study that explores the difficulties in creating shared multilingual vocabularies, especially focusing on the problem of over segmentation in low-resource languages, addressing cross-lingual lexicon induction, morphological segmentation, subword tokenization, data sparsity, semantic ambiguity, polysemy, word sense disambiguation, lexical resource creation, annotation guidelines, crowdsourcing, machine translation, neural language models, BERT multilingual, XLM\u2011R, cross-lingual embeddings, semantic alignment, lexical gap, language typology, language families, lexical borrowing, semantic shift, ontology alignment, and leveraging resources such as Open Multilingual Wordnet, BabelNet, Wikidata, Europarl, Universal Dependencies, CLDR, Glottolog, Ethnologue, and the Linguistic Data Consortium."}
{"original_query": "Could you recommend a study that explores the improvement of Chinese sequence labeling with BERT through the incorporation of lexical data via a character-to-word bilinear attention approach?", "expanded_query": "Find studies on Chinese sequence labeling with BERT improved by lexical data via a character-to-word bilinear attention approach, covering tasks such as named entity recognition, part-of-speech tagging, and word segmentation, using datasets like OntoNotes, MSRA, SIGHAN, and reporting F1, precision, recall, comparing to baseline BERT, published in ACL, EMNLP, NAACL, TACL."}
{"original_query": "Could you recommend a study that initializes embeddings in multilingual transformer for subwords common with original vocabulary with original embeddings?", "expanded_query": "Could you recommend a research study or paper that investigates initializing embeddings in a multilingual transformer model such as mBERT or XLM\u2011R for subwords that are common with an original vocabulary, using the original embeddings for initialization, and explores techniques like shared subword tokenization, embedding alignment, cross\u2011lingual transfer, embedding reuse, zero\u2011shot learning, and embedding fine\u2011tuning across multiple languages, referencing works from ACL, EMNLP, or arXiv?"}
{"original_query": "Could you recommend a study that investigates employing graph neural networks to produce replies within multi-party conversational contexts?", "expanded_query": "Could you recommend a study that investigates employing graph neural networks, graph convolutional networks, or graph attention networks to produce replies within multi-party conversational contexts such as group chat logs, online forums, video conferencing transcripts, or social media threads, that incorporates dialogue act classification, contextual embeddings, knowledge graph integration, and evaluates response relevance, coherence, diversity, and engagement using metrics like BLEU, ROUGE, METEOR, or human evaluation, and that is published in conferences such as ACL, EMNLP, NAACL, ICLR, NeurIPS, or journals and is available on arXiv or Google Scholar?"}
{"original_query": "Could you recommend a study that investigates employing prefix vectors for conditional natural language generation?", "expanded_query": "Could you recommend a study or academic paper that investigates employing prefix vectors or prefix tuning for conditional natural language generation in transformer-based language models such as GPT\u20113, GPT\u20114, BERT, RoBERTa, T5, or XLNet, focusing on conditional text generation tasks like dialogue systems, conditional summarization, conditional translation, or conditional story generation, and discussing methods, results, and future research directions in NLP and deep learning?"}
{"original_query": "Could you recommend a study that investigates enhancing prompt engineering techniques for generative models using meta-learning strategies?", "expanded_query": "Could you recommend a study that investigates enhancing prompt engineering techniques for transformer-based generative language models using meta-learning strategies such as MAML, Reptile, or FOMAML, focusing on few-shot and zero-shot performance, adaptive prompt design, prompt tuning, cross-domain generalization, and evaluation on benchmark datasets like GLUE, SuperGLUE, and OpenAI GPT\u20114, PaLM, LLaMA, and Codex, incorporating reinforcement learning, Bayesian optimization, and neural architecture search for prompt optimization, robustness, interpretability, and scalability?"}
{"original_query": "Could you recommend a study that investigates enhancing token alignment in speech processing by employing inverse document frequency (idf)?", "expanded_query": "Recommend a study investigating enhancing token alignment in speech processing using inverse document frequency weighting, focusing on acoustic modeling, phoneme alignment, forced alignment, dynamic time warping, HMM, neural network, transformer, attention mechanisms, evaluation metrics like WER and alignment error rate, on datasets such as LibriSpeech and TIMIT, exploring subword tokenization, BPE, SentencePiece, cross\u2011lingual alignment, domain adaptation, and idf weighting in speech recognition literature."}
{"original_query": "Could you recommend a study that investigates graph-based modeling of interactivity among various modalities over time within non-aligned multimodal sequential datasets?", "expanded_query": "Could you recommend a study that investigates graph-based modeling of interactivity among various modalities over time within non-aligned multimodal sequential datasets, including dynamic graph neural networks, temporal graph convolutional networks, graph attention mechanisms, multimodal fusion techniques, cross-modal interaction analysis, temporal dependency modeling, and applications to audio, video, text, and sensor data in non-aligned multimodal sequences, published in top conferences such as NeurIPS, ICLR, CVPR, ACL, EMNLP, or journals like IEEE TPAMI, IEEE Transactions on Multimedia, JMLR, or arXiv?"}
{"original_query": "Could you recommend a study that investigates guiding abstractive summarization through assessing sentence informativeness?", "expanded_query": "Recommend a study on guiding abstractive summarization through sentence informativeness assessment, covering sentence importance scoring, sentence salience, information content, neural summarization models, transformer-based approaches like BERT and GPT, evaluation metrics such as ROUGE, BLEU, human evaluation, related work in ACL, EMNLP, NAACL, NeurIPS, ICLR, arXiv, Google Scholar."}
{"original_query": "Could you recommend a study that investigates how a subset with clean, annotated datasets improve denoising methods?", "expanded_query": "Could you recommend a study that investigates how a subset of clean, annotated datasets improves image denoising methods, including deep learning models such as DnCNN, Noise2Noise, Noise2Void, and Noise2Self, and evaluates performance using PSNR, SSIM, and visual quality metrics on benchmark datasets like BSD, Urban100, Set12, and ImageNet, while considering noise types such as Gaussian, Poisson, salt-and-pepper, and real-world camera sensor noise, and addressing dataset curation, annotation quality, subset selection criteria, cross-validation, transfer learning, domain adaptation, and statistical significance, published in venues such as CVPR, ICCV, NeurIPS, or on arXiv?"}
{"original_query": "Could you recommend a study that investigates how contrastive learning enhances sentence-level embeddings in natural language processing, especially for subsequent applications?", "expanded_query": "Could you recommend a study that investigates how contrastive learning enhances sentence-level embeddings in natural language processing, especially for subsequent applications such as semantic similarity, text classification, sentiment analysis, information retrieval, and knowledge graph integration, and that examines pretraining methods like SimCSE, sentence transformers, BERT-based models, evaluates embedding quality on benchmark datasets such as STS-B, SentEval, GLUE, and SQuAD, uses metrics like cosine similarity, clustering, downstream task performance, embedding dimensionality, and visualizations, while exploring self-supervised learning, negative sampling, temperature hyperparameters, contrastive loss variants (NT\u2011Xent, triplet), and transfer learning fine\u2011tuning strategies?"}
{"original_query": "Could you recommend a study that investigates how integrating model quantization with knowledge distillation?", "expanded_query": "Could you recommend a research paper or study that investigates the integration of model quantization with knowledge distillation for neural network compression, focusing on techniques such as quantization\u2011aware training, post\u2011training quantization, teacher\u2011student frameworks, temperature scaling, and cross\u2011entropy loss, and evaluating performance trade\u2011offs on benchmark datasets like ImageNet or CIFAR\u201110 in terms of accuracy, compression ratio, runtime speedup, and energy efficiency, with references to top conferences such as NeurIPS, ICLR, CVPR, or journals such as IEEE Transactions on Neural Networks?"}
{"original_query": "Could you recommend a study that investigates incorporating a fact memory component into neural networks to improve language modeling activities without requiring retraining or fine-tuning?", "expanded_query": "Could you recommend a study that investigates incorporating a fact memory component or external knowledge base integration into neural networks, such as memory\u2011augmented transformer or retrieval\u2011augmented generation models, to improve language modeling tasks like text generation or question answering without requiring retraining or fine\u2011tuning, focusing on zero\u2011shot or few\u2011shot learning, knowledge retrieval, semantic memory, and memory networks in recent ACL, NeurIPS, ICLR, or arXiv publications?"}
{"original_query": "Could you recommend a study that investigates knowledge transfer from structured databases to unstructured data sources for improving sophisticated question-answering systems?", "expanded_query": "Could you recommend a study that investigates knowledge transfer from structured databases, relational data warehouses, and SQL schemas to unstructured text corpora, document repositories, and web\u2011scraped data for improving sophisticated question\u2011answering systems using transfer learning, knowledge graph embeddings, semantic entity linking, ontology alignment, schema mapping, data harmonization, graph neural networks, transformer\u2011based models, and evaluation metrics such as precision, recall, and F1 score?"}
{"original_query": "Could you recommend a study that investigates representing entities in knowledge graphs with intricate geometric shapes, emphasizing probabilistic analysis and uncertainty modeling?", "expanded_query": "Could you recommend a study that investigates representing entities in knowledge graphs with intricate geometric shapes such as hyperbolic, spherical, or manifold embeddings, emphasizing probabilistic analysis, Bayesian inference, uncertainty modeling, probabilistic shape embeddings, and uncertainty\u2011aware knowledge graph completion using geometric deep learning, graph neural networks, and probabilistic graphical models for entity representation learning?"}
{"original_query": "Could you recommend a study that investigates text generation from tabular data, considering elements such as titles, column headings, and cell content, while also integrating numerical reasoning?", "expanded_query": "Could you recommend a study that investigates text generation from tabular data, considering elements such as titles, column headings, and cell content, while also integrating numerical reasoning, including neural network models, transformer-based table-to-text architectures, attention mechanisms, numeric inference, data-to-text summarization, evaluation metrics like BLEU, ROUGE, METEOR, human evaluation, benchmark datasets such as WikiTableQuestions, TableGenie, and recent research on structured data summarization with quantitative analysis?"}
{"original_query": "Could you recommend a study that investigates the implementation of sparsity within attention mechanisms to enhance the performance of models processing extremely lengthy documents?", "expanded_query": "Could you recommend a study that investigates the implementation of sparsity within attention mechanisms, such as sparse transformers, Longformer, BigBird, Reformer, or Linformer, to enhance the performance, speed, and memory efficiency of models processing extremely lengthy documents, including long\u2011range attention, block\u2011sparse attention, global\u2011local attention, and attention pruning techniques, and evaluates them on benchmark datasets for document classification, summarization, and information retrieval?"}
{"original_query": "Could you recommend a study that investigates the improvement of knowledge representation learning by incorporating global context into graph attention networks?", "expanded_query": "Could you recommend a study that investigates the improvement of knowledge representation learning by incorporating global context into graph attention networks, specifically focusing on knowledge graph embeddings, graph neural network architectures, attention mechanisms, global graph context integration, contextualized graph attention, empirical evaluation on benchmark datasets, node classification, link prediction, performance metrics, and comparative analysis with graph convolutional networks and other graph attention models?"}
{"original_query": "Could you recommend a study that investigates the use of trainable prompts to enhance the parameter optimization process in machine learning models?", "expanded_query": "Could you recommend a study that investigates the use of trainable prompts to enhance the parameter optimization process in machine learning models, including prompt engineering, prompt tuning, prompt embeddings, parameter-efficient fine-tuning, hyperparameter optimization, Bayesian optimization, meta-learning, gradient descent, and empirical evaluation on benchmark datasets such as ImageNet, GLUE, and COCO, with performance metrics like accuracy, F1, runtime, scalability, and resource efficiency, published in top conferences like NeurIPS, ICML, ICLR, or on arXiv?"}
{"original_query": "Could you recommend a study that uses feedback-driven decoding for producing mathematical proofs using language models?", "expanded_query": "Could you recommend a recent research paper or study that uses feedback\u2011driven decoding, such as reinforcement learning from human feedback or iterative self\u2011consistency, to generate rigorous mathematical proofs with large language models like GPT\u20114 or other transformer architectures, possibly integrating formal proof assistants such as Coq, Lean, or Z3, and discussing decoding strategies like beam search, top\u2011k/top\u2011p sampling, temperature control, and human\u2011in\u2011the\u2011loop refinement, with references to arXiv, ACL, NeurIPS, ICLR, or other conferences from 2022 to 2024?"}
{"original_query": "Could you recommend articles that explore the role of late interaction in dense retrieval systems and its influence on the performance of information retrieval?", "expanded_query": "Could you recommend scholarly articles and recent research papers that investigate the role of late interaction mechanisms in dense retrieval systems, such as ColBERT, ANCE, DPR, and cross\u2011encoder architectures, and analyze their impact on information retrieval performance metrics like precision, recall, MRR, NDCG, as well as retrieval latency, scalability, and computational cost in benchmark datasets like MS MARCO, TREC, and web search scenarios?"}
{"original_query": "Could you recommend research articles that explore the application of contrastive learning methods to improve sentence embedding efficacy in natural language processing?", "expanded_query": "Could you recommend research articles that explore the application of contrastive learning methods, such as SimCSE, SimCLR, MoCo, to improve sentence embedding efficacy in natural language processing, including transformer-based models like BERT, RoBERTa, XLM\u2011R, and Universal Sentence Encoder, focusing on self\u2011supervised learning, semantic similarity, semantic textual similarity, evaluation on STS benchmark, SentEval, GLUE, SuperGLUE, downstream tasks such as text classification, clustering, semantic search, information retrieval, cross\u2011lingual embeddings, discussing contrastive loss, temperature scaling, hard negatives, data augmentation, negative sampling, embedding quality metrics, cosine similarity, t\u2011SNE visualization, and transfer learning."}
{"original_query": "Could you recommend research papers that explore applying knowledge distillation to information retrieval, specifically those that concentrate on methods using in-batch negatives", "expanded_query": "Could you recommend recent research papers that explore applying knowledge distillation to information retrieval, specifically those that concentrate on methods using in-batch negatives, contrastive learning, teacher-student models, BERT-based neural ranking, embedding distillation, negative sampling strategies, hard negatives, and evaluation on datasets such as MS MARCO, TREC, or Cranfield, with a focus on retrieval efficiency, latency, and ranking metrics like MAP, NDCG, recall@k, and including state-of-the-art transformer models like DistilBERT, RoBERTa, and ELECTRA, as well as surveys or reviews on deep learning for IR and knowledge transfer techniques?"}
{"original_query": "Could you recommend research papers that investigate employing Transformer-based architectures for completing knowledge graphs?", "expanded_query": "Could you recommend research papers, conference proceedings, or journal articles that investigate employing Transformer-based architectures, such as BERT, GPT, Graph Transformer Networks, or self-attention mechanisms, for knowledge graph completion tasks, including link prediction, entity resolution, relation extraction, and knowledge graph inference, on benchmark datasets like FB15k-237, WN18RR, YAGO, Freebase, or OpenKG, and covering performance metrics such as accuracy, precision, recall, F1, and comparing against graph neural networks, knowledge graph embeddings, and other state-of-the-art methods, with references to top conferences (NeurIPS, ICLR, ACL, EMNLP, KDD, ICML) and journals, and including citation counts, publication years, and authors from leading research labs such as Google AI, Facebook AI, Microsoft Research, and OpenAI?"}
{"original_query": "Could you recommend research that analyses prompt tuning as a method to improve the generalizability of pre-trained models while avoiding catastrophic forgetting?", "expanded_query": "Could you recommend research papers, surveys, and benchmark studies that analyze prompt tuning as a parameter\u2011efficient fine\u2011tuning method to improve generalizability of pre\u2011trained transformer language models (e.g., GPT\u20113, BERT, RoBERTa, T5) while mitigating catastrophic forgetting in continual learning scenarios, including techniques such as LoRA, adapter modules, elastic weight consolidation, memory replay, and meta\u2011learning, and covering domains like domain adaptation, cross\u2011domain evaluation, few\u2011shot and zero\u2011shot learning, and multi\u2011task or cross\u2011lingual transfer, with references from NeurIPS, ICLR, ACL, arXiv, and relevant GLUE/SuperGLUE/SQuAD benchmarks."}
{"original_query": "Could you recommend research that assesses how well language learning models, such as ChatGPT, perform in creating reading comprehension tasks for educational software?", "expanded_query": "Search for peer\u2011reviewed research evaluating the performance of large language models like ChatGPT and GPT\u20114 in generating AI\u2011generated reading comprehension tasks for educational software, focusing on natural language processing, educational technology, task generation quality, curriculum alignment, student engagement, learning outcomes, assessment metrics, human evaluation, automated content creation, adaptive learning, instructional design, and AI in education."}
{"original_query": "Could you recommend research that assesses how well large language models, such as GPT-3, perform at coreference resolution when tested in a few-shot learning context?", "expanded_query": "research on large language models such as GPT-3 coreference resolution performance in few-shot learning context, evaluation metrics precision recall F1, benchmark datasets CoNLL-2012 OntoNotes, prompt engineering few-shot zero-shot transfer learning NLP academic studies papers articles coreference resolution evaluation GPT-3 few-shot research."}
{"original_query": "Could you recommend research that assesses techniques to mitigate intersectional biases within Transformer-based models?", "expanded_query": "Could you recommend research studies, surveys, or reviews that evaluate bias mitigation methods such as adversarial training, data augmentation, debiasing embeddings, fairness constraints, and bias auditing frameworks for transformer-based language models like BERT, GPT\u20113, RoBERTa, and XLNet, focusing on intersectional biases across gender, race, age, and socioeconomic status, and including empirical evaluations of mitigation effectiveness in NLP tasks and algorithmic fairness metrics?"}
{"original_query": "Could you recommend research that employs a relaxed l0 regularization for structured pruning to downsize language models with transformer architectures?", "expanded_query": "Could you recommend research papers, arXiv preprints, or conference proceedings that use relaxed L0 regularization or continuous L0 relaxation such as hard\u2011concrete distribution for structured pruning of transformer\u2011based language models like BERT, GPT, RoBERTa, or DistilBERT to achieve model size reduction, parameter sparsity, channel or block pruning, and efficient inference on edge devices, GPUs, or TPUs, including works from Google AI, OpenAI, DeepMind, Microsoft Research, Facebook AI Research, NVIDIA, and relevant conferences such as ICLR, NeurIPS, ACL, JMLR, and IEEE?"}
{"original_query": "Could you recommend research that evaluates the performance decline in various language models, like BLOOM, under 4-bit integer columnar weight-only quantization?", "expanded_query": "Could you recommend research papers, benchmark studies, or experimental evaluations that analyze the performance decline, accuracy drop, perplexity increase, inference latency, throughput, and energy consumption of large language models such as BLOOM, GPT\u20113, PaLM, LLaMA, BERT, and RoBERTa when subjected to 4\u2011bit integer columnar weight\u2011only quantization, including comparisons with int8, per\u2011layer, per\u2011head, and mixed\u2011precision quantization schemes, and covering evaluation metrics on GLUE, SuperGLUE, SQuAD, WikiText, and other NLP benchmarks, as well as insights from quantization\u2011aware training, post\u2011training static quantization, and hardware acceleration on GPUs, TPUs, and FPGAs?"}
{"original_query": "Could you recommend research that examines how an annotator's individual attributes, like their gender, ethnicity, and political views, influence their judgment of content deemed offensive?", "expanded_query": "Could you recommend research that examines how an annotator's individual attributes, such as gender identity, ethnicity, race, nationality, age, education level, political ideology, religious beliefs, and social identity, influence their judgment of content deemed offensive, including hate speech, harassment, and culturally sensitive material, and how these factors affect inter\u2011annotator agreement, annotation reliability, bias, and the effectiveness of content moderation guidelines in online platforms?"}
{"original_query": "Could you recommend research that examines how decoding strategies like top-k impact hallucinatory in generated text?", "expanded_query": "Could you recommend research that examines how decoding strategies like top\u2011k, top\u2011p (nucleus) sampling, beam search, temperature scaling, and other sampling methods impact hallucination rates, semantic fidelity, factual consistency, and overall quality in large language model\u2011generated text, including studies on GPT\u20113, GPT\u20114, BERT, T5, retrieval\u2011augmented generation, hallucination detection metrics, and mitigation techniques in NLP and generative AI?"}
{"original_query": "Could you recommend research that examines how multihead attention networks discern word interrelations in news content for detecting political perspectives?", "expanded_query": "Could you recommend research that examines how multihead attention networks discern word interrelations in news content for detecting political perspectives, including studies on transformer models such as BERT and RoBERTa, attention visualization and heatmaps, semantic dependency analysis, contextualized embeddings, political bias detection, political slant and ideology classification, news media bias, political stance detection, and interpretability of attention mechanisms in deep learning NLP for news corpora?"}
{"original_query": "Could you recommend research that examines how optimized continuous prompts perform against discrete prompts in relational tasks?", "expanded_query": "Could you recommend research that examines how optimized continuous prompts perform against discrete prompts in relational tasks, including studies on prompt engineering, prompt optimization techniques, continuous vs discrete prompt comparison, relational reasoning, knowledge graph extraction, semantic relation identification, transformer-based models such as GPT\u20114 and BERT, prompt-based learning, prompt tuning, prompt design strategies, and their performance metrics, evaluation methods, and empirical results in natural language processing and machine learning?"}
{"original_query": "Could you recommend research that examines how syntactic configurations affect aspect-level sentiment analysis when employing a pretrained model such as RoBERTa?", "expanded_query": "Could you recommend research papers or studies that investigate how syntactic configurations, such as dependency tree structures and constituency parse patterns, influence aspect\u2011level sentiment analysis performance when fine\u2011tuning pretrained transformer models like RoBERTa, BERT, XLNet, or Electra on benchmark datasets such as SemEval, SentiHood, or SentiWordNet, and what methodological insights or findings have emerged regarding syntactic feature integration, aspect extraction, sentiment polarity detection, and domain adaptation in aspect\u2011based sentiment analysis?"}
{"original_query": "Could you recommend research that examines the challenges faced by pre-trained language models in learning inferential commonsense knowledge when the context is sparse?", "expanded_query": "Could you recommend research on transformer\u2011based pre\u2011trained language models such as BERT, GPT, RoBERTa, and T5 that investigates the challenges of acquiring inferential commonsense knowledge under sparse contextual conditions, including few\u2011shot and zero\u2011shot learning, low\u2011resource settings, knowledge graph integration (ATOMIC, ConceptNet), neural\u2011symbolic reasoning, knowledge distillation, data augmentation, contextualized embeddings, semantic gap, bias mitigation, and evaluation on commonsense benchmarks like Winograd Schema, COPA, Social IQA, and the ATOMIC inference tasks?"}
{"original_query": "Could you recommend research that examines the effect of example sequencing on machine learning model efficacy in few-shot learning scenarios?", "expanded_query": "Could you recommend research that examines the effect of example sequencing and data ordering on machine learning model efficacy, sample efficiency, and generalization in few-shot learning scenarios, including few-shot classification and regression, meta-learning, transfer learning, benchmark datasets such as Omniglot, mini-ImageNet, CIFAR-FS, and studies from top conferences like ICML, NeurIPS, ACL, CVPR, IEEE, and institutions such as Stanford, MIT, Harvard, University of Toronto, DeepMind, OpenAI, Meta AI, and Microsoft Research?"}
{"original_query": "Could you recommend research that explores how the loss of spatial information impacts the effectiveness of global features in visual tasks?", "expanded_query": "Could you recommend research papers, journal articles, or conference proceedings that investigate how the loss of spatial information, such as through spatial pooling, global pooling, or positional encoding removal, affects the effectiveness of global features in visual tasks like image classification, object detection, semantic segmentation, scene understanding, and object localization, particularly in deep learning models including CNNs, vision transformers, and attention-based architectures, and that compare global vs local feature representations, analyze spatial context, spatial hierarchy, multi-scale analysis, and evaluate performance metrics such as accuracy, robustness, and computational efficiency on benchmark datasets like ImageNet, COCO, PASCAL VOC, and OpenImages, and are published in venues such as CVPR, ICCV, NeurIPS, IEEE, arXiv, and other relevant sources?"}
{"original_query": "Could you recommend research that explores identifying excess and insufficient translations in evaluating machine translation, especially regarding resolving label discrepancies in cases of omitted content?", "expanded_query": "Could you recommend research that explores identifying excess and insufficient translations in evaluating machine translation, especially regarding resolving label discrepancies in cases of omitted content, covering over-translation, under-translation, content omission, annotation guidelines, human evaluation, automatic detection, error analysis, semantic coverage, source-target alignment, translation quality estimation, MT evaluation metrics such as BLEU, METEOR, TER, human judgment, post-editing, quality estimation models, neural machine translation, statistical machine translation, translation coverage, semantic fidelity, translation adequacy, fluency, MT evaluation research, label noise, annotation noise, labeling inconsistencies, label discrepancy resolution, content preservation, semantic similarity, source-target alignment metrics, over-translation detection, under-translation detection, omission detection, label mismatch, labeling guidelines, annotation standards."}
{"original_query": "Could you recommend research that has introduced a dual-phase method for sentence paraphrasing?", "expanded_query": "Could you recommend research papers, articles, or conference proceedings that have introduced a dual-phase method or dual-phase approach for sentence paraphrasing, including dual-stage or dual-phase paraphrase generation frameworks, neural network or transformer-based models, sequence-to-sequence architectures, contextual embeddings such as BERT or GPT, evaluation metrics like BLEU, ROUGE, METEOR, and datasets such as ParaNMT, PPDB, or paraphrase corpora, and that discuss the methodology, pipeline, pre-processing, post-processing, semantic similarity, and dual-phase sentence paraphrasing research?"}
{"original_query": "Could you recommend research that improves knowledge base generation using non-differentiable evaluation metrics like BLEU, METEOR, and chrF++?", "expanded_query": "Could you recommend research papers, surveys, or conference proceedings on knowledge base generation or knowledge graph construction that use reinforcement learning, policy gradient methods, self-critical sequence training, or reward shaping with non-differentiable evaluation metrics such as BLEU, METEOR, and chrF++ as reward signals, including works from ACL, EMNLP, NeurIPS, ICLR, ICML, KDD, AAAI, IJCAI, and authors from Stanford NLP, Allen Institute for AI, Google AI, Facebook AI Research, and OpenAI, and datasets like WikiData, Freebase, YAGO, OpenIE, and benchmark corpora for knowledge extraction, entity linking, relation extraction, and semantic parsing?"}
{"original_query": "Could you recommend research that introduces a metric for assessing Text-to-Image synthesis, emphasizing the semantic congruence between the text and the produced image rather than solely the visual quality?", "expanded_query": "Could you recommend research that introduces a metric for assessing Text-to-Image synthesis, emphasizing the semantic congruence between the text and the produced image rather than solely the visual quality, including concepts such as semantic alignment, cross-modal evaluation, CLIP-based similarity, semantic fidelity, image-text matching, semantic relevance score, semantic consistency, benchmark datasets like COCO, Flickr30k, Visual Genome, and evaluation frameworks for generative models such as DALL\u2011E, Stable Diffusion, GLIDE, and metric learning approaches?"}
{"original_query": "Could you recommend research that investigates applying conventional data augmentation methods such as removing words and truncating sequences to contrastive learning in NLP applications?", "expanded_query": "Search for research on applying conventional data augmentation methods such as word removal, random deletion, synonym replacement, sequence truncation, sentence cropping, and token masking to contrastive learning in NLP, including studies on SimCSE, contrastive sentence embeddings, self\u2011supervised pretraining with BERT, T5, GPT, and transformer models, exploring negative sampling, contrastive loss, and performance on GLUE, SuperGLUE, and other benchmark datasets, with literature from ACL, EMNLP, NAACL, ICLR, NeurIPS, arXiv, and recent conference papers."}
{"original_query": "Could you recommend research that investigates generative modeling approaches for event extraction, specifically focusing on leveraging large pre-trained language models to reduce the complexity of template engineering?", "expanded_query": "Could you recommend research that investigates generative modeling approaches for event extraction, specifically focusing on leveraging large pre-trained language models such as BERT, GPT\u20113, PaLM, LLaMA, Claude, and RoBERTa to reduce the complexity of template engineering by adopting template\u2011free, end\u2011to\u2011end, prompt\u2011based, few\u2011shot, zero\u2011shot, retrieval\u2011augmented generation, and encoder\u2011decoder transformer methods, while addressing event detection, trigger identification, argument role labeling, coreference resolution, entity linking, knowledge graph integration, semantic role labeling, and event schema induction across domains like news, social media, clinical, financial, and multilingual settings, and evaluating performance on benchmarks such as ACE\u00a02005, TAC\u00a0KBP, OntoNotes, SemEval, and ACE\u00a02012 using precision, recall, F1, macro, and micro metrics, and exploring transfer learning, domain adaptation, cross\u2011lingual transfer, model scaling, parameter efficiency, knowledge distillation, model compression, neural architecture search, structured prediction, self\u2011supervised learning, and retrieval\u2011augmented generation techniques in state\u2011of\u2011the\u2011art surveys and papers?"}
{"original_query": "Could you recommend research that investigates how to use contrastive learning for improving logical reasoning over text?", "expanded_query": "Could you recommend research that investigates how to use contrastive learning for improving logical reasoning over text, including studies on contrastive representation learning, self\u2011supervised contrastive objectives, transformer\u2011based models such as BERT, RoBERTa, GPT, and XLNet, logical reasoning benchmarks like ReClor, LogiQA, and the Logical Reasoning Benchmark, textual entailment, deductive, abductive, and inductive reasoning tasks, and applications of contrastive learning to semantic similarity, knowledge graph embeddings, and reasoning over natural language corpora?"}
{"original_query": "Could you recommend research that investigates how to use data augmentation for improving logical reasoning over text?", "expanded_query": "Could you recommend research papers, surveys, or systematic reviews that investigate data augmentation techniques\u2014such as back\u2011translation, paraphrasing, synthetic sentence generation, knowledge\u2011graph\u2011based augmentation, rule\u2011based augmentation, and adversarial example creation\u2014for improving logical reasoning over text, including natural language inference, multi\u2011hop reasoning, chain\u2011of\u2011thought prompting, deductive or abductive reasoning tasks, evaluated on benchmark datasets like GLUE, SuperGLUE, and other reasoning benchmarks, and published in venues such as ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, or on arXiv?"}
{"original_query": "Could you recommend research that investigates merging speech and text modalities in a unified representation space for processing spoken language through encoder-decoder models?", "expanded_query": "Could you recommend research that investigates merging speech and text modalities in a unified representation space for processing spoken language through encoder-decoder models, including multimodal representation learning, joint embedding, cross-modal attention, transformer-based architectures, self-supervised pretraining, contrastive learning, speech-to-text, text-to-speech, speech translation, speech summarization, speech emotion recognition, speech diarization, low-resource language adaptation, noise robustness, cross-lingual transfer, datasets such as LibriSpeech, TED-LIUM, Common Voice, VoxPopuli, models like Wav2Vec2, HuBERT, Conformer, BERT, T5, GPT, BART, frameworks like PyTorch, TensorFlow, fairseq, transformers, and conferences such as ACL, EMNLP, NeurIPS, ICLR, ICASSP, Interspeech, and research groups like Google Brain, DeepMind, OpenAI, Microsoft Research, FAIR, CMU, MIT, Stanford, Oxford, Cambridge, Toronto?"}
{"original_query": "Could you recommend research that investigates methods for fine-tuning generative language models with a focus on parameter efficiency to reduce computational demands?", "expanded_query": "Could you recommend research that investigates parameter\u2011efficient fine\u2011tuning methods for generative language models, such as LoRA, adapter modules, low\u2011rank adaptation, pruning, quantization, and knowledge distillation, focusing on reducing computational demands, GPU memory usage, training time, and energy consumption, while evaluating performance on benchmark datasets and large\u2011scale transformer architectures like GPT, BERT, T5, and other LLMs?"}
{"original_query": "Could you recommend research that investigates techniques for creating counterfactual examples to enhance question-answering systems, such as training a T5 model augmented with retrieval?", "expanded_query": "Could you recommend research papers on counterfactual data augmentation techniques for enhancing retrieval\u2011augmented question answering systems, such as training a T5 model with retrieval\u2011augmented generation (RAG), including works on counterfactual example generation, retrieval\u2011augmented language models, fine\u2011tuning T5 with retrieval, adversarial and counterfactual reasoning, and recent ACL, EMNLP, NeurIPS, ICLR publications on counterfactual QA and retrieval\u2011augmented T5, as well as studies from Google Research, OpenAI, DeepMind, and arXiv on transformer\u2011based models, pretraining, fine\u2011tuning, BERT, RoBERTa, GPT\u20113, ChatGPT, and retrieval\u2011augmented generation methods?"}
{"original_query": "Could you recommend research that investigates the enhancement of neural passage retrieval through the application of dual encoders and the generation of synthetic questions?", "expanded_query": "Could you recommend research papers or studies that investigate the enhancement of neural passage retrieval through dual encoder architectures, transformer-based models such as BERT, RoBERTa, or sentence transformers, contrastive learning, data augmentation, synthetic question or query generation, retrieval-augmented generation, fine-tuning, zero-shot retrieval, cross-encoder comparisons, semantic search, information retrieval, neural ranking, retrieval pipelines, retrieval-based QA, evaluation metrics like MRR and Recall@k, on datasets such as MS\u202fMARCO, Natural Questions, SQuAD, TREC, and involving synthetic question generation models, seq2seq, transformer-based question generation, and retrieval performance improvement in neural IR literature."}
{"original_query": "Could you recommend research that investigates the impact of randomly removing words from sentences as a data augmentation strategy to mitigate overfitting in NLP models?", "expanded_query": "Could you recommend research papers or conference proceedings that investigate the impact of random word deletion or token dropout as a data augmentation strategy for mitigating overfitting in NLP models such as BERT, GPT, RoBERTa, XLNet, or T5, focusing on tasks like text classification, sentiment analysis, named entity recognition, machine translation, summarization, and evaluating generalization, robustness, and performance metrics across benchmark datasets in ACL, EMNLP, NAACL, NeurIPS, ICLR, and ICML?"}
{"original_query": "Could you recommend research that investigates the influence of cognitive biases on human interpretation of AI-generated explanations, specifically within the realm of explainable natural language processing?", "expanded_query": "Could you recommend research that investigates the influence of cognitive biases such as confirmation bias, anchoring bias, availability heuristic, overconfidence, framing effect, hindsight bias, and self-serving bias on human interpretation of AI-generated explanations, specifically within the realm of explainable natural language processing, explainable AI (XAI), human\u2011AI interaction, explanation fidelity, transparency, trust, comprehension, evaluation metrics, user studies, and design of textual and visual explanation modalities?"}
{"original_query": "Could you recommend research that investigates using autoencoder architectures to generate e-commerce product descriptions by integrating product titles, features, and supplementary descriptions crafted by marketers?", "expanded_query": "Could you recommend research papers, conference proceedings, or journal articles that investigate using autoencoder architectures\u2014such as variational, denoising, or sequence-to-sequence autoencoders\u2014to generate e-commerce product descriptions by integrating product titles, product features, marketing copy, and supplementary descriptions crafted by marketers, with a focus on natural language processing, deep learning, transformer-based models, latent variable modeling, attention mechanisms, evaluation metrics like BLEU, ROUGE, and human evaluation, and case studies on platforms such as Amazon, Alibaba, Shopify, and other e-commerce catalogs, including datasets like Amazon Reviews, product description datasets, and references to works presented at ACL, EMNLP, NeurIPS, ICLR, IJCAI, AAAI, IEEE, Springer, Elsevier, and available on Google Scholar or arXiv?"}
{"original_query": "Could you recommend research that proposed enhancing the RoBERTa model for event extraction by adding a Bi-LSTM and a CRF layer?", "expanded_query": "Could you recommend research papers or academic sources that propose enhancing the RoBERTa transformer-based model for event extraction tasks by integrating a Bi\u2011directional LSTM layer and a Conditional Random Field layer, including details on datasets such as ACE 2005 or CoNLL\u20112003, evaluation metrics like F1, precision, recall, comparisons with baseline models, and insights into sequence labeling, token classification, event trigger detection, argument extraction, and overall pipeline improvements?"}
{"original_query": "Could you recommend scholarly articles that investigate the practice of refining language models through the exclusive modification of bias parameters in their linear components?", "expanded_query": "scholarly articles, peer-reviewed research papers, academic studies, conference proceedings, journal articles, and arXiv preprints that investigate bias-only fine-tuning, bias parameter optimization, bias-only adaptation, and linear layer bias adjustment in transformer-based language models such as GPT, BERT, and other large language models, focusing on parameter-efficient fine-tuning techniques, bias-only tuning, bias-only adaptation, and bias modification in linear components, published in venues like ACL, EMNLP, NeurIPS, ICLR, ICML, AAAI, IJCAI, JMLR, Nature Machine Intelligence, Science Advances, and Proceedings of the Association for Computational Linguistics."}
{"original_query": "Could you recommend studies on hierarchical modeling of user interests for tailoring news recommendation systems?", "expanded_query": "Recommend studies on hierarchical modeling of user interests for tailoring news recommendation systems, including hierarchical Bayesian models, hierarchical topic models, hierarchical latent variable models, hierarchical attention networks, hierarchical recurrent neural networks, multi-level user profiling, interest hierarchy, content-based filtering, collaborative filtering, hybrid recommendation, deep learning approaches, knowledge graph integration, ontology-based interest modeling, hierarchical clustering, multi-task learning, contextual bandits, reinforcement learning, evaluation metrics such as click-through rate and user engagement, A/B testing, literature reviews, empirical studies, and theoretical frameworks in personalized news recommendation."}
{"original_query": "Could you recommend studies that concentrate on analyzing and constructing models for discourse organization in conversations involving multiple turns and parties, aimed at separating dialogues?", "expanded_query": "Studies on discourse analysis, conversation analysis, turn-taking, multi-party dialogue, multi-turn conversation, dialogue segmentation, discourse segmentation, discourse structure modeling, dialogue act modeling, computational discourse analysis, natural language processing, machine learning, hierarchical clustering, topic segmentation, discourse coherence, dialogue management, conversation modeling frameworks, segmentation algorithms, segmentation datasets, segmentation evaluation, segmentation techniques, segmentation of multi-turn dialogues, segmentation of multi-party dialogues, segmentation of conversations, segmentation of dialogues, segmentation of multi-party conversations, segmentation of multi-turn conversations, segmentation of multi-party dialogues, segmentation of multi-turn dialogues"}
{"original_query": "Could you recommend studies that investigate fine-tuning pre-trained language models using weakly supervised learning, especially those employing techniques like contrastive regularization or self-training?", "expanded_query": "Could you recommend studies that investigate fine-tuning pre\u2011trained language models such as BERT, RoBERTa, GPT\u20113, and XLNet using weakly supervised learning, especially those employing contrastive regularization, contrastive loss, or self\u2011training with pseudo\u2011labels, confidence thresholding, iterative refinement, or knowledge distillation, and that evaluate on benchmarks like GLUE, SuperGLUE, SQuAD, MNLI, CoNLL\u20112003, or domain adaptation tasks, citing works by Zhang et al. 2022, Li et al. 2023, Chen et al. 2021, Ruder et al. 2020, Gururangan et al. 2020, and frameworks such as Hugging Face Transformers, Fairseq, DeepSpeed, and Snorkel for weak supervision?"}
{"original_query": "Could you recommend studies that tackle the issue of popularity bias within news recommendation engines and offer techniques to distinguish between user interests and the popularity of news items?", "expanded_query": "Could you recommend studies that tackle the issue of popularity bias within news recommendation engines, including works on collaborative filtering, content-based filtering, matrix factorization, and neural recommendation models, that offer techniques to distinguish between user interests and the popularity of news items, such as bias mitigation methods, fairness-aware algorithms, user modeling, topic modeling, click-through rate analysis, position bias correction, exploration-exploitation trade-offs, and evaluation metrics like precision, recall, diversity, serendipity, and novelty, and that reference relevant datasets (MIND, News-20, Reuters, NYTimes, Google News, Microsoft News) and conferences (RecSys, KDD, WWW, SIGIR, ACM, IEEE, AAAI)?>"}
{"original_query": "Could you recommend studies which explore how to optimally select demonstrations for few-shot in-context learning?", "expanded_query": "Could you recommend studies that investigate optimal demonstration selection strategies for few-shot in-context learning with large language models such as GPT\u20113, GPT\u20114, and other transformer\u2011based LLMs, covering prompt engineering, demonstration diversity, relevance scoring, adaptive selection, meta\u2011learning approaches, benchmark evaluations, best practices for prompt design and demonstration weighting, as well as datasets, metrics, and algorithms for prompt selection in transformer models?"}
{"original_query": "Could you suggest a dataset containing diverse, intricate natural language queries that necessitate multi-step reasoning, comparing attributes, and performing set operations for answering questions from a knowledge base, without depending on entity linking?", "expanded_query": "Could you recommend a benchmark dataset comprising diverse, intricate natural language questions that require multi-step reasoning, attribute comparison, and set operations (union, intersection, difference) for answering queries over a knowledge base or knowledge graph, without relying on entity linking, and covering semantic parsing, NL2SQL, NL2SPARQL, graph query languages, multi-hop reasoning, and logical form decomposition, suitable for evaluating question answering systems, knowledge graph embeddings, and semantic search, with metrics such as accuracy, precision, recall, F1, and including real-world and synthetic examples from sources like WikiSQL, Spider, ComplexWebQuestions, MetaQA, OpenBookQA, GraphQA, and other knowledge base QA benchmarks?"}
{"original_query": "Could you suggest a dataset for question-answering frameworks utilizing temporal knowledge graphs with broad coverage?", "expanded_query": "Could you suggest a large-scale, publicly available dataset for question answering frameworks that utilize temporal knowledge graphs with broad coverage, including temporal reasoning, time-aware question answering, knowledge graph embeddings, benchmark datasets, and entities such as ICEWS, GDELT, or other temporal KG datasets for dynamic knowledge graphs and time-series knowledge graphs?"}
{"original_query": "Could you suggest a research article that explores generative methods for extracting information, specifically one that covers the generation of surface forms, labeling of entities, and classification of entity types?", "expanded_query": "Could you suggest a research article that explores generative methods for extracting information, specifically one that covers the generation of surface forms, labeling of entities, and classification of entity types, including NLP, named entity recognition, entity linking, entity normalization, entity disambiguation, entity resolution, entity extraction pipelines, deep learning models such as BERT, GPT, transformer-based seq2seq generation, variational autoencoders, GANs, CRF, semantic role labeling, knowledge base integration, ontology mapping, entity embeddings, entity taxonomy, entity hierarchy, evaluation metrics, benchmark datasets like CoNLL, OntoNotes, ACE, TAC, OpenIE, cross-lingual entity extraction, domain-specific entity extraction in biomedical, legal, news, social media, multilingual, low-resource languages, and state-of-the-art survey or review articles."}
{"original_query": "Could you suggest a study examining how transformer models utilize feed-forward neural networks (FFNs) to encode factual information?", "expanded_query": "Could you suggest a study examining how transformer models such as BERT, GPT\u20113, RoBERTa, XLNet, and ALBERT utilize feed\u2011forward neural networks (FFNs) within their encoder layers to encode factual information, represent knowledge, and extract factual knowledge, including analyses of self\u2011attention, positional encoding, the role of the feed\u2011forward sublayer, representation learning, knowledge distillation, and the impact on transformer\u2011based language model performance and interpretability?"}
{"original_query": "Could you suggest a study that evaluates cross-encoder BERT rankers?", "expanded_query": "Could you suggest a study that evaluates cross-encoder BERT rankers, including performance comparison on benchmark datasets such as MS MARCO, TREC Deep Learning, and Natural Questions, using evaluation metrics like NDCG@10, MAP, precision@k, recall@k, analyzing transformer-based rankers, deep learning rankers, relevance judgments, IR retrieval tasks in natural language processing, academic papers, recent publications, state-of-the-art ranking models, query-document pairs, fine-tuning, pretraining, BERT, RoBERTa, ELECTRA, ranking loss functions, pairwise ranking, listwise ranking, evaluation protocols, cross-validation, statistical significance, ablation studies, error analysis, computational cost, latency, GPU inference time, ranking pipelines, re-ranking models, cross-encoder architecture, bi-encoder, dual-encoder, query encoder, document encoder, embedding, similarity scoring, dot product, cosine similarity, attention, transformer layers, fine-tuning, domain adaptation, domain-specific corpora, domain-specific rankers."}
{"original_query": "Could you suggest a study that examines how well contrastive learning performs in unimodal representation learning, specifically for sentence embeddings?", "expanded_query": "Could you suggest a study that examines how well contrastive learning methods such as SimCLR, MoCo, or CPC perform in unimodal representation learning for sentence embeddings, evaluating semantic similarity on benchmark datasets like STS Benchmark and SentEval, using metrics such as cosine similarity and Pearson correlation, comparing against supervised models like BERT, RoBERTa, and Sentence\u2011BERT, and including details on data augmentation, contrastive loss functions (NT\u2011Xent), experimental setup, hyperparameters, ablation studies, and results, preferably published in venues like ICLR, ACL, or EMNLP, with authors and year?"}
{"original_query": "Could you suggest a study that explores a cohesive pre-training method for code representation learning across different modalities?", "expanded_query": "Suggest a research study or paper that investigates a cohesive pre-training method for code representation learning across multiple modalities such as source code, natural language documentation, API usage, and execution traces, using techniques like transformer-based self-supervised learning, contrastive learning, or graph neural networks, and published in venues like ICLR, NeurIPS, ACL, ICSE, or on arXiv."}
{"original_query": "Could you suggest a study that explores a compression method that merges product quantization with integer quantization for token-level retrieval?", "expanded_query": "Could you suggest a study that explores a compression method that merges product quantization with integer quantization for token-level retrieval, focusing on embedding compression for transformer-based language models, evaluating recall@k, precision, MAP, NDCG on benchmark datasets such as MS MARCO and SQuAD, comparing hybrid quantization schemes with FAISS, Annoy, and HNSW indexing, and analyzing memory footprint, compression ratio, retrieval latency, and accuracy trade-offs in large-scale semantic search and retrieval-augmented generation pipelines?"}
{"original_query": "Could you suggest a study that explores data annotation paradigms that help assuage concerns in lack of annotator expertise when using crowdsourcing?", "expanded_query": "Could you suggest a study that explores data annotation paradigms, quality control mechanisms, training protocols, gold standard integration, active learning, hierarchical labeling, worker incentive models, interface design, annotation guidelines, expert review, inter-annotator agreement metrics, annotation error analysis, and scalability solutions to assuage concerns about lack of annotator expertise in crowdsourcing?"}
{"original_query": "Could you suggest a study that explores employing a beta distribution to sample span sizes within unsupervised learning frameworks for text representation?", "expanded_query": "Could you suggest a recent academic study or paper that explores employing a beta distribution to sample span sizes within unsupervised learning frameworks for text representation, including applications to transformer-based language models, tokenization strategies, contextual embeddings, and evaluation on intrinsic and extrinsic NLP tasks such as semantic similarity, clustering, and classification, and that discusses hyperparameter settings for the alpha and beta parameters, sampling strategies, and performance metrics in the context of natural language processing research presented at conferences like ACL, EMNLP, NAACL, ICLR, or NeurIPS?"}
{"original_query": "Could you suggest a study that explores improved training methods for dense passage retrieval within open-domain question answering systems?", "expanded_query": "Could you recommend a recent academic study or conference paper that explores advanced training methods such as contrastive learning, triplet loss, hard negative mining, or self\u2011supervised fine\u2011tuning for dense passage retrieval models like BERT, RoBERTa, sentence transformers, or ELECTRA, evaluates them on large\u2011scale open\u2011domain QA datasets such as MS\u202fMARCO, Natural Questions, TriviaQA, or WikiPassage, reports retrieval metrics like recall@k, mean reciprocal rank, and latency, discusses scalability and integration with retrieval\u2011augmented generation in systems like GPT\u20114 or T5, and cites works from Google Research, Microsoft Research, or OpenAI presented at ACL, EMNLP, NeurIPS, or ICLR?"}
{"original_query": "Could you suggest a study that explores the idea of using prompts to fine-tune language models, potentially providing an alternative viewpoint to standard optimization and regularization methods?", "expanded_query": "Could you suggest a study that explores the idea of using prompts to fine-tune language models, potentially providing an alternative viewpoint to standard optimization and regularization methods, including prompt engineering, prompt-based fine-tuning, parameter-efficient techniques such as LoRA, prefix tuning, adapter modules, few-shot and zero-shot learning, transfer learning with GPT-3, BERT, T5, and evaluating with metrics like perplexity, BLEU, ROUGE, while also investigating regularization strategies like dropout, weight decay, early stopping, alternative optimizers such as Adam, SGD, and exploring prompt-based domain adaptation, continual learning, meta-learning, interpretability, bias mitigation, privacy, security, and robustness?"}
{"original_query": "Could you suggest a study that explores the use of multi-modal pre-training techniques to improve the comprehension of documents with a high visual content?", "expanded_query": "Could you suggest a recent research study or publication that investigates multi-modal pre-training techniques, such as vision-language transformers (e.g., VisualBERT, LXMERT, UNITER, LayoutLMv3), self-supervised learning objectives, cross-modal attention mechanisms, and large-scale multimodal datasets (e.g., PubLayNet, DocVQA, RVL-CDIP) to enhance document comprehension, visual question answering, and information extraction in high-visual-content documents, and that compares performance on benchmark datasets like DocVQA, PubLayNet, and the ICDAR 2019 Layout Analysis competition, possibly published in conferences such as CVPR, ICCV, ACL, EMNLP, or on arXiv?"}
{"original_query": "Could you suggest a study that investigates the incorporation of human intervention in generating adversarial examples to attack conversational agents, with the aim of improving classifier efficacy", "expanded_query": "Could you recommend a study that examines human\u2011in\u2011the\u2011loop adversarial example generation to attack conversational agents such as chatbots and voice assistants, focusing on intent classification, slot filling, and dialogue management, and explores how human\u2011guided perturbations can improve classifier robustness, adversarial training, and overall efficacy of NLP models in the context of machine learning security, human\u2011computer interaction, and ethical considerations?"}
{"original_query": "Could you suggest a study that proposes high-parameter efficeint fine-tuning techinque that only trains the bias terms?", "expanded_query": "parameter efficient fine tuning bias only training study high parameter efficient technique bias-only fine tuning large language models transformer bias-only fine tuning methods research paper"}
{"original_query": "Could you suggest a thorough comparative analysis or review of the performance of different pretrained transformer architectures, such as BERT, in text ranking applications?", "expanded_query": "Could you suggest a thorough comparative analysis or review of the performance of different pretrained transformer architectures, such as BERT, RoBERTa, ALBERT, ELECTRA, GPT\u20113, T5, DistilBERT, and sentence\u2011transformers, in text ranking applications, covering evaluation metrics like precision@k, recall@k, NDCG, ranking loss functions (pairwise, listwise, pointwise), benchmarking on datasets such as MS\u202fMARCO, TREC\u202fDL, Natural\u202fQuestions, using frameworks like PyTorch and TensorFlow, considering fine\u2011tuning, cross\u2011encoder vs bi\u2011encoder, embedding space, vector search with FAISS, scalability, latency, GPU acceleration, model compression, quantization, pruning, edge deployment, privacy\u2011preserving federated learning, bias mitigation, interpretability, attention visualization, human\u2011in\u2011the\u2011loop studies, A/B testing, and real\u2011world use cases in e\u2011commerce, academic literature, legal and medical document retrieval, multilingual cross\u2011lingual retrieval, and zero\u2011shot cross\u2011lingual scenarios."}
{"original_query": "Could you suggest a triplet-formatted structured dataset suitable for training table-to-text generation models?", "expanded_query": "Could you recommend a triplet\u2011formatted structured dataset, such as one derived from knowledge graphs like Wikidata or DBpedia, that is suitable for training transformer\u2011based table\u2011to\u2011text generation models (e.g., T5, BERT, GPT) for natural language generation tasks, including entity\u2011relation triples, subject\u2011predicate\u2011object representation, and supporting evaluation metrics like BLEU, ROUGE, and METEOR, while also providing guidance on preprocessing, tokenization, entity linking, and data augmentation for high\u2011quality NLG?"}
{"original_query": "Could you suggest an article that leverages the spatial information available in documents for multi-modal LMs by using a spatially-aware attention mechanism?", "expanded_query": "Could you suggest an article that leverages spatial information available in documents for multi-modal language models by using a spatially-aware attention mechanism, including concepts such as layout-aware transformers, document layout analysis, spatial embeddings, graph neural networks, visual grounding, layout-based representation learning, and spatial context in document understanding?"}
{"original_query": "Could you suggest datasets that can benchmark LLM performance in achieving conversational continuity and recall over long multi-session conversations?", "expanded_query": "Could you suggest datasets that can benchmark LLM performance in achieving conversational continuity and recall over long multi-session conversations, including open-source corpora such as ConvAI2, EmpatheticDialogues, MultiWOZ, PersonaChat, Reddit thread logs, Twitter conversation datasets, Microsoft Dialogue Corpus, Amazon Alexa Prize data, OpenAI ChatGPT logs, GPT\u20114 evaluation sets, LLaMA, Claude, PaLM, and evaluation frameworks that measure context retention, memory capacity, context window size, multi\u2011turn QA, dialogue state tracking, conversation coherence, recall accuracy, human evaluation, automated metrics, and benchmark suites for open\u2011domain and task\u2011oriented dialogue?"}
{"original_query": "Could you suggest papers that tackle conversational search by using retrieval and conversational response ranking?", "expanded_query": "Suggest academic papers on conversational search that combine retrieval and conversational response ranking, covering retrieval-augmented generation, dense and sparse retrieval, neural ranking models, transformer-based ranking, learning-to-rank, retrieval-based dialogue systems, and recent conference publications from ACL, EMNLP, SIGIR, CIKM, NeurIPS, ICLR."}
{"original_query": "Could you suggest research on detecting common errors like additions and omissions in machine translation?", "expanded_query": "Suggest research on detecting common errors such as additions, omissions, insertions, and deletions in machine translation, covering error typology, error analysis, automatic post\u2011editing, translation quality estimation, neural and statistical machine translation, sequence labeling approaches, transformer models, attention mechanisms, BLEU, METEOR, TER metrics, human evaluation, error annotation datasets, and related NLP error detection methods and algorithms."}
{"original_query": "Could you suggest research that assesses if language models use extended contextual information, with experiments on the book dataset from Project Gutenberg?", "expanded_query": "Could you suggest research that evaluates whether large language models such as GPT\u20114, BERT, or other transformer\u2011based architectures effectively utilize extended contextual information\u2014including longer context windows, long\u2011range dependencies, contextual span, contextual memory, and contextual recall\u2014in their representations, with experimental studies on the Project Gutenberg book dataset, analyzing contextual coverage, bias, influence, and generalization in textual modeling, evaluation, and performance?"}
{"original_query": "Could you suggest research that concentrates on pinpointing sentence components that carry hateful expressions, potentially aiding in sentence-level standardization for content moderation?", "expanded_query": "Could you recommend recent research on pinpointing sentence components that carry hateful expressions using NLP methods such as transformer\u2011based models (BERT, RoBERTa, GPT), hate lexicon construction, contextual embeddings, dependency parsing, semantic role labeling, tokenization, lemmatization, and word embeddings (GloVe, fastText), with a focus on explainable AI (attention maps, SHAP, LIME), bias mitigation (fairness metrics, demographic bias detection), cross\u2011lingual transfer learning, domain adaptation, and evaluation metrics (precision, recall, F1, ROC\u2011AUC), to support sentence\u2011level standardization for content moderation on platforms like Twitter, Facebook, and YouTube, while considering regulatory compliance (GDPR, AI policy) and ethical guidelines?"}
{"original_query": "Could you suggest research that examines a system for multimodal annotation intended to assist people in analyzing dialogues within video content?", "expanded_query": "multimodal annotation system for video dialogue analysis, speech recognition, natural language processing, computer vision, audiovisual content segmentation, semi-automatic annotation tools, annotation interfaces, user studies, annotation accuracy, machine learning, deep learning, semantic annotation, dialogue act tagging, multimodal datasets, annotation guidelines, assistive technology, human-computer interaction, video analytics, multimodal interaction, annotation efficiency, annotation frameworks, multimodal annotation research"}
{"original_query": "Could you suggest research that examines how coreference resolution affects dialogue summarization quality?", "expanded_query": "Could you suggest recent research papers, surveys, or literature reviews that examine how coreference resolution affects dialogue summarization quality, including studies on coreference resolution models such as AllenNLP and SpanBERT, dialogue summarization datasets like Ubuntu Dialogue Corpus, Persona\u2011Chat, and Dialog bAbI, evaluation metrics such as ROUGE, BLEU, METEOR, human evaluation, and the impact of coreference resolution accuracy on extractive and abstractive summarization performance in multi\u2011party dialogues, as presented in ACL, EMNLP, NAACL, COLING, ICLR, NeurIPS, or arXiv?"}
{"original_query": "Could you suggest research that examines how prompt tuning can be used for domain transfer?", "expanded_query": "Could you suggest research papers or studies that examine how prompt tuning can be used for domain transfer in natural language processing, including domain adaptation, transfer learning, cross-domain performance, few-shot and zero-shot learning, prompt engineering, transformer models such as GPT-3 and BERT, domain-specific corpora, domain shift, domain adaptation benchmarks, and evaluation metrics for domain transfer?"}
{"original_query": "Could you suggest research that examines how prompt tuning efficacy in language model training is affected by scaling?", "expanded_query": "Could you suggest research that examines how prompt tuning efficacy in language model training is affected by scaling, including model size, dataset size, compute budget, scaling laws, few-shot and zero-shot performance, parameter efficiency, prompt engineering, prompt selection, prompt length, prompt composition, prompt tuning vs. full fine-tuning, adapter modules, LoRA, prefix tuning, scaling experiments, benchmark datasets, OpenAI, DeepMind, Google AI, Microsoft Research, arXiv, ACL, NeurIPS, ICLR, ICML, JMLR, cross-lingual prompt tuning, multilingual models, scaling effects on performance, efficiency, generalization, training dynamics, hyperparameters, learning rate schedules, batch size, optimization, gradient scaling, compute cost, sample efficiency, data efficiency, model architecture, and related empirical studies and theoretical analyses."}
{"original_query": "Could you suggest research that examines how strangers exchange information during discussions, specifically concentrating on the kinds of information typically shared during first encounters?", "expanded_query": "Could you suggest research that examines how strangers exchange information during discussions, specifically concentrating on the kinds of information typically shared during first encounters, including self-disclosure, small talk, personal and professional details, social norms, cultural and gender differences, nonverbal cues, and the role of social exchange theory and information asymmetry in first contact communication?"}
{"original_query": "Could you suggest research that examines how the order of in-context examples influences the efficacy of in-context learning in language models?", "expanded_query": "Could you recommend recent research papers, conference proceedings, or arXiv preprints that investigate the impact of example ordering, prompt engineering, and in\u2011context example selection strategies on the performance, generalization, and learning dynamics of transformer\u2011based language models such as GPT\u20113, GPT\u20114, BERT, and other few\u2011shot or prompt\u2011tuned architectures, including studies on ordering effects, sequence dynamics, semantic similarity, syntactic structure, attention weight analysis, statistical significance, effect size, and bias mitigation across benchmarks like GLUE, SuperGLUE, SQuAD, RACE, and other datasets, while covering topics such as meta\u2011learning, prompt optimization, context relevance, sample bias, cross\u2011domain transfer, prompt scheduling, and prompt\u2011based transfer learning?"}
{"original_query": "Could you suggest research that examines how well language models work with creole languages, particularly in relation to their effectiveness with Nigerian Pidgin, given its close linguistic relationship with English?", "expanded_query": "Could you suggest research that examines how well language models work with creole languages, particularly in relation to their effectiveness with Nigerian Pidgin, given its close linguistic relationship with English, including studies on transformer-based models such as GPT, BERT, mBERT, and XLM\u2011R, cross\u2011lingual transfer and fine\u2011tuning on Nigerian Pidgin corpora, evaluation metrics like BLEU, ROUGE, and perplexity, analyses of syntactic, lexical, and phonological similarities between Nigerian Pidgin and English, investigations into code\u2011switching, low\u2011resource language adaptation, and multilingual NLP benchmarks for creole languages."}
{"original_query": "Could you suggest research that examines how well prompt tuning using soft embeddings works for utilizing pretrained language models in downstream applications with minimal finetuning?", "expanded_query": "Could you suggest research that examines how well prompt tuning using soft embeddings works for utilizing pretrained language models in downstream applications with minimal finetuning, including studies on parameter\u2011efficient fine\u2011tuning, soft prompt embeddings, performance on benchmark datasets, comparisons with full fine\u2011tuning, evaluation metrics, and applications across NLP tasks such as text classification, question answering, summarization, translation, dialogue, and domain adaptation for models like GPT, BERT, T5, LLaMA, and other large language models?"}
{"original_query": "Could you suggest research that examines how well structured pruning techniques perform in developing both small and precise models in natural language processing?", "expanded_query": "Could you suggest research that examines how well structured pruning techniques perform in developing both small and precise models in natural language processing, including transformer pruning, BERT, GPT, RoBERTa, DistilBERT, TinyBERT, MobileBERT, pruning strategies, structured vs unstructured pruning, sparsity, fine-tuning, evaluation metrics, GLUE, SQuAD, latency, FLOPs, energy consumption, edge devices, model compression, neural network pruning literature, survey, case studies, comparative analysis, state\u2011of\u2011the\u2011art, pruning algorithms, importance scores, activation\u2011based pruning, gradient\u2011based pruning, post\u2011pruning fine\u2011tuning, pruning in transformers, attention pruning, feed\u2011forward pruning, pruning in LSTM, RNN, pruning in GPT\u20113, LLaMA, pruning benchmarks, datasets, metrics, performance, effectiveness, impact, results, evaluation, research papers, review, survey, literature."}
{"original_query": "Could you suggest research that examines the application of specialized architecture in pre-trained language models to enhance text-to-SQL tasks?", "expanded_query": "Could you suggest research that examines the application of specialized architecture in pre\u2011trained language models, such as transformer\u2011based BERT, GPT, or T5, to enhance text\u2011to\u2011SQL tasks, including semantic parsing, schema linking, encoder\u2011decoder attention mechanisms, graph neural networks, and benchmark evaluation on datasets like Spider, SQLova, and SQLNet, focusing on transfer learning, fine\u2011tuning, prompt engineering, few\u2011shot learning, cross\u2011lingual performance, execution accuracy, runtime efficiency, and model interpretability?"}
{"original_query": "Could you suggest research that examines the challenges faced by neural networks in discerning causation from correlation?", "expanded_query": "Could you suggest research that examines the challenges faced by neural networks in discerning causation from correlation, including causal inference, counterfactual reasoning, structural causal models, causal discovery algorithms, confounding variables, bias\u2011variance tradeoff, causal representation learning, causal effect estimation, causal regularization, causal attention mechanisms, causal convolutional networks, causal inference in deep learning, causal inference in reinforcement learning, computer vision, natural language processing, time series, graph neural networks, generative models, unsupervised learning, transfer learning, domain adaptation, meta\u2011learning, federated learning, privacy\u2011preserving ML, robust ML, adversarial ML, explainable AI, fairness, bias mitigation, policy evaluation, healthcare, economics, social sciences, biology, neuroscience, robotics, autonomous driving, and key researchers such as Judea Pearl, Peter Spirtes, Ilya Sutskever, Yoshua Bengio, along with relevant literature, methods, benchmarks, and datasets."}
{"original_query": "Could you suggest research that examines the difficulties in employing weakly labeled datasets for named entity recognition and offers techniques to mitigate the associated data noise?", "expanded_query": "research on challenges of using weakly labeled datasets for named entity recognition, noise in weak supervision, label noise, annotation errors, distant supervision, semi-supervised learning, self-training, bootstrapping, active learning, human-in-the-loop, noise-aware models, noise-aware loss functions, robust NER, noise mitigation techniques, error analysis, noise filtering, data cleaning, synthetic data augmentation, noise modeling, noise-tolerant training, transformer-based NER, BiLSTM-CRF, BERT, RoBERTa, XLM-R, domain adaptation, cross-lingual NER, biomedical NER, datasets like CoNLL-2003, OntoNotes, WikiANN, methods to reduce data noise, confidence weighting, pseudo-labeling, curriculum learning, ensemble methods, noise-aware architecture, noise-aware embeddings, noise-aware pretraining, noise-aware fine-tuning"}
{"original_query": "Could you suggest research that examines the effects of starting language models with weights from pretrained nondiffusion models on the convergence behavior of diffusion losses?", "expanded_query": "Could you suggest research that examines the effects of initializing transformer-based language models with weights from pretrained non-diffusion models such as BERT, GPT, RoBERTa, T5, XLNet, or Microsoft Turing on the convergence behavior, loss landscapes, optimization dynamics, training stability, and diffusion loss convergence of diffusion-based generative models like DDPM, DDIM, and latent diffusion for text, including studies on weight transfer, fine-tuning strategies, learning rate schedules, Adam optimizer, gradient clipping, score matching, noise schedules, stochastic differential equations, cross-modal diffusion, knowledge distillation, and the impact on performance on benchmark datasets such as GLUE, SuperGLUE, WMT, and OpenAI GPT-3, as well as theoretical analyses and empirical studies from the research community?"}
{"original_query": "Could you suggest research that explores a pre-trained encoder-decoder architecture aimed at comprehending and generating code, potentially beneficial for enhancing automated code repair systems?", "expanded_query": "Suggest research on pre-trained encoder-decoder architectures for code comprehension and generation, such as transformer-based models like CodeBERT, GPT-Code, or GraphCodeBERT, focusing on applications to automated code repair, program synthesis, bug detection, semantic code search, code summarization, and code translation, including datasets like CodeNet, Defects4J, and benchmarks for neural program repair."}
{"original_query": "Could you suggest research that explores employing independently sampled dropout masks to generate positive pairs in contrastive learning for sentence embeddings?", "expanded_query": "Could you suggest research that explores employing independently sampled dropout masks to generate positive pairs in contrastive learning for sentence embeddings, including studies on SimCSE, BERT-based contrastive learning, self-supervised sentence representation, transformer models, data augmentation techniques, dropout-based augmentation, positive pair mining, negative sampling, semantic similarity, SimCLR, MoCo, contrastive loss, state-of-the-art NLP conferences such as ACL, EMNLP, NAACL, ICLR, NeurIPS, ICML, arXiv papers, and related deep learning frameworks for sentence embeddings, as well as investigations into self-distillation, noise injection, semantic embeddings, embedding space analysis, contrastive objective design, sentence-level contrastive learning, independent dropout mask sampling, and contrastive learning research in natural language processing."}
{"original_query": "Could you suggest research that explores generating synthetic labels for sentences within a vast text collection to pre-train models for few-shot learning applications?", "expanded_query": "Suggest research on synthetic sentence labeling, large-scale text corpora, pretraining language models, few-shot learning, transfer learning, self-supervised and semi-supervised annotation, weak supervision, active learning, prompt-based learning, meta-learning, data augmentation, contrastive learning, graph-based labeling, knowledge graphs, entity linking, semantic role labeling, topic modeling, clustering, embedding, self-training, iterative refinement, human-in-the-loop annotation pipelines, synthetic data generation, language generation, text classification, domain adaptation, transformer models such as BERT, GPT, RoBERTa, XLNet, and state-of-the-art benchmarks from ACL, EMNLP, NeurIPS, ICLR, ICML, arXiv, and major AI research labs."}
{"original_query": "Could you suggest research that explores the drawbacks of dense retrieval systems especially with large-scale indices?", "expanded_query": "Could you suggest research papers, survey articles, or conference proceedings that explore the drawbacks, scalability challenges, latency, memory footprint, indexing overhead, and trade-offs of dense retrieval systems, especially with large-scale indices, including approximate nearest neighbor methods such as FAISS, Annoy, HNSW, GPU acceleration, model compression, embedding dimensionality, curse of dimensionality, index maintenance, distributed retrieval, cloud infrastructure costs, retrieval error, precision@k, recall@k, and related benchmarks like MS MARCO, C4, OpenAI embeddings, BERT, sentence-transformers, contrastive learning, and retrieval-augmented generation?"}
{"original_query": "Could you suggest research that explores the idea of training soft prompts rather than identifying fixed ones within the realm of prompt tuning?", "expanded_query": "Could you suggest research that explores the idea of training soft prompts rather than identifying fixed ones within the realm of prompt tuning, including soft prompt learning, prompt engineering, prompt optimization, gradient-based prompt training, prompt embeddings, parameter-efficient fine-tuning, transformer-based language models such as GPT\u20113, BERT, T5, prompt tuning literature, recent arXiv papers, ACL, NeurIPS, ICLR, EMNLP, soft prompt training algorithms, prompt tuning for few\u2011shot and zero\u2011shot learning, domain adaptation, multilingual transfer, and cross\u2011modal prompt tuning?"}
{"original_query": "Could you suggest research that explores the improvement of Recurrent Neural Network Transducers (RNN-T) through the integration of cross-attention mechanisms for use in simultaneous translation?", "expanded_query": "Could you suggest research that explores the improvement of Recurrent Neural Network Transducers (RNN\u2011T) through the integration of cross\u2011attention mechanisms for use in simultaneous translation, including low\u2011latency real\u2011time speech\u2011to\u2011speech translation, multilingual encoder\u2011decoder architectures, attention\u2011based RNN\u2011T variants, cross\u2011modal attention, domain adaptation, transfer learning, fine\u2011tuning on speech corpora such as MuST\u2011C and TED talks, evaluation on benchmarks like WMT, OpenSLR, and real\u2011time latency metrics, as well as related concepts such as Transformer, BERT, XLM\u2011R, speech recognition, CTC, joint network, beam search, speaker adaptation, noise robustness, speaker diarization, audio\u2011visual, lip\u2011reading, speech enhancement, low\u2011resource languages, domain\u2011specific corpora, pre\u2011training, and latency\u2011aware training and decoding?"}
{"original_query": "Could you suggest research that includes an online community dataset used to examine machine learning models for hate speech identification?", "expanded_query": "Could you suggest research that includes an online community dataset such as the Reddit Hate Corpus, Twitter Hate Corpus, Jigsaw Toxic Comment dataset, or HateXplain, used to examine machine learning models like BERT, RoBERTa, XLNet, or transformer-based classifiers for hate speech identification, covering dataset annotation guidelines, inter-annotator agreement, class imbalance handling, evaluation metrics (precision, recall, F1, ROC\u2011AUC), bias mitigation, fairness, interpretability techniques (SHAP, LIME), cross\u2011lingual transfer learning, domain adaptation, real\u2011time deployment on social media platforms, and references to studies from institutions like Stanford, MIT, Harvard, Oxford, and research papers from SemEval\u00a02019\u00a0Task\u00a05, Davidson\u00a0et\u00a0al., and others."}
{"original_query": "Could you suggest research that investigates a clustering-based efficient attention mechanism within Transformer models?", "expanded_query": "clustering-based efficient attention mechanism Transformer models research papers sparse attention linear attention hierarchical clustering k-means graph clustering transformer efficiency computational complexity memory efficiency large-scale language models state-of-the-art transformer variants efficient transformer architectures attention scaling transformer variants efficient attention research transformer research attention mechanism clustering transformer efficient attention clustering studies transformer efficient attention clustering literature review survey references arXiv ACL NeurIPS ICLR CVPR"}
{"original_query": "Could you suggest research that investigates across multiple languages how dictionary definitions and word embedding models compare?", "expanded_query": "Could you suggest research that investigates across multiple languages how dictionary definitions and word embedding models compare, including cross-lingual lexical semantics, multilingual word embeddings, distributional semantics, sense embeddings, dictionary definition embeddings, cross-lingual evaluation frameworks, multilingual corpora, bilingual lexicon induction, cross-lingual semantic similarity metrics, and related lexical resources such as WordNet, BabelNet, Wiktionary, and multilingual BERT?"}
{"original_query": "Could you suggest research that investigates applying combinatorial optimization techniques in unsupervised entity matching?", "expanded_query": "Could you suggest research that investigates applying combinatorial optimization techniques such as graph matching, assignment problem, set covering, branch and bound, genetic algorithms, simulated annealing, constraint programming, ILP, MIP, OR-Tools, CP-SAT to unsupervised entity matching, record linkage, entity resolution, data cleaning, data integration, knowledge graph construction, semantic matching, entity disambiguation, clustering, similarity metrics, distance measures, blocking, indexing, scalability, big data, distributed computing frameworks like Spark, Hadoop, Flink, graph databases such as Neo4j, Cypher queries, privacy-preserving methods including differential privacy, federated learning, privacy constraints, data quality, provenance, lineage, governance, and embedding-based matching using graph embeddings, node2vec, GraphSAGE, GNNs, and related concepts."}
{"original_query": "Could you suggest research that investigates aspect-based sentiment analysis across different languages, incorporating methods such as code-switching of aspect terms?", "expanded_query": "multilingual aspect-based sentiment analysis cross-lingual aspect term extraction code-switching aspect terms multilingual BERT XLM-R cross-lingual embeddings low-resource languages aspect sentiment lexicons domain adaptation aspect-level sentiment classification multilingual datasets cross-lingual evaluation code-switching detection research papers studies methods techniques applications social media product reviews multilingual corpora"}
{"original_query": "Could you suggest research that investigates efficient finetuning methods that only trains very very few parameters in language models?", "expanded_query": "Could you suggest research that investigates efficient finetuning methods that only trains very very few parameters in language models, such as parameter-efficient fine-tuning, LoRA, adapter modules, prefix tuning, prompt tuning, sparse fine-tuning, low-rank adaptation, dynamic adapters, and related techniques for large language models like GPT, BERT, and transformer-based architectures, including recent papers, benchmarks, and state-of-the-art methods in resource-constrained settings?"}
{"original_query": "Could you suggest research that investigates employing graph attention techniques for integrating multiple modalities for identifying emotions?", "expanded_query": "Could you suggest research that investigates employing graph attention techniques, such as graph attention networks and graph convolutional networks, for integrating multiple modalities\u2014including facial expression, speech, physiological signals, and textual data\u2014for identifying emotions in multimodal emotion recognition, affective computing, and human-computer interaction contexts, covering multimodal fusion strategies, attention-based fusion, real-time emotion detection, benchmark datasets, and recent advances in deep learning and attention mechanisms?"}
{"original_query": "Could you suggest research that investigates enhancing zero-shot question answering with prompts from language models integrated with knowledge graph data?", "expanded_query": "Could you suggest research that investigates enhancing zero-shot question answering with prompts from language models integrated with knowledge graph data, exploring techniques such as prompt engineering, graph embeddings, knowledge graph completion, semantic similarity, transformer-based models, few-shot learning, cross-lingual QA, knowledge graph integration, graph neural networks, knowledge base completion, and evaluation metrics like F1, accuracy, MRR, and datasets such as HotpotQA, WebQuestions, OpenBookQA, Wikidata, Freebase, DBpedia, and methods like KGQA, KG embeddings, and prompt-based fine-tuning?"}
{"original_query": "Could you suggest research that investigates expanding keyword collections through embedding similarity in weakly-supervised document categorization?", "expanded_query": "Could you suggest research that investigates expanding keyword collections through embedding similarity in weakly-supervised document categorization, focusing on vector embeddings, semantic similarity metrics, BERT-based sentence transformers, cosine similarity, weak supervision techniques such as label propagation and self-training, and applications to text classification, topic modeling, knowledge graph enrichment, cross-lingual transfer, and domain adaptation?"}
{"original_query": "Could you suggest research that investigates graph-based methods for predicting connections in knowledge graphs, focusing on n-ary relational facts rather than just simple triple structures?", "expanded_query": "Could you suggest research that investigates graph-based methods for predicting connections in knowledge graphs, focusing on n-ary relational facts rather than just simple triple structures, including link prediction, knowledge graph completion, hypergraph neural networks, graph convolutional networks, graph attention networks, knowledge graph embeddings, tensor factorization, multi-relational data, semantic embeddings, RDF, OWL, SPARQL, Neo4j, Apache Jena, OpenKE, PyKEEN, TransE, RotatE, ComplEx, GraphSAGE, GAT, graph transformer, graph-based inference, knowledge graph reasoning, semantic web, knowledge base completion, entity resolution, knowledge graph schema, ontology, representation learning, hypergraph representation, n-ary relation modeling, multi-relational learning, graph-based relational learning, graph-based link prediction, knowledge graph inference, knowledge graph reasoning methods, knowledge graph embedding techniques, knowledge graph completion tasks, knowledge graph inference methods, knowledge graph representation learning."}
{"original_query": "Could you suggest research that investigates how cross-attention mechanisms improve the interplay between token and syntax data within automated program repair systems?", "expanded_query": "Could you suggest research that investigates how cross-attention mechanisms improve the interplay between token embeddings and syntactic structures such as ASTs, dependency parses, and graph representations within automated program repair systems, covering transformer-based models, encoder-decoder architectures, graph neural networks with graph attention, attention-based code repair, neural program repair literature, datasets like Defects4J, CodeNet, GitHub bug repositories, cross-modal learning between code and natural language, semantic analysis, code summarization, code completion, neural code synthesis, bug detection, bug fixing, and state-of-the-art studies on token-syntax interplay in neural code repair?"}
{"original_query": "Could you suggest research that investigates how hierarchical structures within transformers enhance task-oriented dialogue systems?", "expanded_query": "Could you suggest research papers, studies, and experiments that investigate how hierarchical transformer architectures, multi-level attention mechanisms, and hierarchical modeling techniques enhance task-oriented dialogue systems, including dialogue state tracking, intent classification, slot filling, and end-to-end neural dialogue modeling, with references to datasets such as MultiWOZ, DSTC, Schema-Guided Dialogue, and frameworks like BERT, GPT, T5, and Hugging Face Transformers, as well as concepts such as hierarchical attention networks, transformer hierarchy, long-context modeling, context-aware dialogue, hierarchical transformer for multi-turn dialogue, hierarchical transformer for dialogue state tracking, hierarchical transformer for slot filling, hierarchical transformer for intent detection, and transformer hierarchical representation?"}
{"original_query": "Could you suggest research that investigates how many evidence sentences are needed for document-level RE?", "expanded_query": "document-level relation extraction evidence sentence selection minimum evidence sentences required for relation extraction performance evaluation datasets DocRED transformer models BERT RoBERTa XLNet graph neural networks attention mechanisms sentence ranking semantic similarity evaluation metrics precision recall F1 human annotation crowdsourcing automatic evidence extraction relation extraction literature survey ACL EMNLP NAACL ICLR NeurIPS arXiv Google Scholar"}
{"original_query": "Could you suggest research that investigates how neural language models' forecasts correlate with human linguistic processing, especially in terms of syntactic surprisal?", "expanded_query": "Could you suggest research that investigates how transformer-based neural language models' predictions correlate with human linguistic processing, especially in terms of syntactic surprisal, predictive coding, eye-tracking, fMRI, EEG, reading times, syntactic complexity, language model perplexity, and the neural correlates of syntax in psycholinguistics and computational linguistics?"}
{"original_query": "Could you suggest research that investigates how undetectable backdoor attacks are in NLP models?", "expanded_query": "Could you suggest research that investigates how undetectable backdoor attacks are in NLP models, covering hidden trigger patterns, data poisoning, model poisoning, stealthy adversarial attacks, trigger insertion in transformer architectures like BERT, GPT, RoBERTa, detection methods, defense mechanisms, adversarial training, model auditing, threat models, evaluation metrics, and recent literature from ACL, EMNLP, ICLR, NeurIPS, and systematic reviews on backdoor detection and robustness in natural language processing?"}
{"original_query": "Could you suggest research that investigates improving retrieval-based conversational systems using by combining masked language modeling and relevance classification objectives?", "expanded_query": "Retrieval-based conversational systems, masked language modeling, relevance classification, neural ranking models, transformer architectures, BERT, RoBERTa, sentence transformers, embedding space, semantic similarity, contrastive learning, multi-task learning, joint training, retrieval-augmented generation, knowledge base retrieval, dialogue response selection, Ubuntu Dialogue Corpus, Persona-Chat, Microsoft Dialogue Corpus, evaluation metrics BLEU ROUGE METEOR, retrieval-augmented generation, retrieval-based chatbot, retrieval-based dialogue, response ranking, semantic search, knowledge retrieval, neural IR, embedding-based retrieval, multi-turn dialogue, dialogue coherence, ranking loss, triplet loss, cross-entropy loss, contrastive loss, end-to-end, reinforcement learning, dialogue policy, state-of-the-art research papers, recent studies, retrieval-based conversational AI, retrieval-based conversational systems"}
{"original_query": "Could you suggest research that investigates the use of past dialogues for enhancing query expansion in conversational search systems?", "expanded_query": "Could you suggest research that investigates the use of past dialogues, dialogue history, and conversational context for enhancing query expansion in conversational search systems, focusing on techniques such as contextual embeddings, transformer-based models, semantic relevance, entity linking, topic modeling, neural retrieval, and evaluation metrics like precision, recall, and F1 in the domains of information retrieval, natural language processing, and human-computer interaction?"}
{"original_query": "Could you suggest research that investigates training BERT-based classifiers with Wikipedia data for zero-shot text classification in open domains?", "expanded_query": "Could you suggest research that investigates training BERT-based classifiers with Wikipedia data for zero-shot text classification in open domains, including studies on pre-trained language models, transfer learning, domain adaptation, knowledge graphs, entity linking, semantic similarity, cross-lingual zero-shot classification, multilingual BERT, XLM-R, prompt-based learning, sentence transformers, zero-shot learning, open-domain NLU, benchmark datasets such as GLUE and SuperGLUE, evaluation metrics like accuracy, F1, macro-F1, micro-F1, domain shift, domain generalization, cross-domain evaluation, and recent conference proceedings from ACL, EMNLP, NAACL, ICLR, NeurIPS, arXiv, Google Scholar, and citations of state-of-the-art papers on Wikipedia-based training, knowledge base embeddings, entity embeddings, and knowledge distillation."}
{"original_query": "Could you suggest research that offers an in-depth examination of the shortcomings associated with pretraining evaluation measures such as BERTScore, particularly regarding their alignment with human evaluative assessments?", "expanded_query": "Could you recommend scholarly studies that critically analyze the limitations of pretraining evaluation metrics such as BERTScore, focusing on their reliability, validity, robustness, consistency, and correlation with human judgment in NLP tasks like text generation, summarization, translation, and dialogue, and exploring issues of evaluation bias, coverage, sensitivity, generalizability, and alignment with human evaluative assessments across transformer-based models and benchmark datasets, including comparisons with BLEU, ROUGE, METEOR, and cross\u2011lingual evaluation?"}
{"original_query": "Could you suggest research that shows multilingual language models can understand plural/singular verb agreement across multiple languages?", "expanded_query": "Could you suggest research papers, datasets, and evaluation studies that demonstrate multilingual transformer\u2011based language models such as mBERT, XLM\u2011R, and multilingual GPT can accurately capture and predict plural versus singular verb agreement across diverse language families (e.g., Germanic, Romance, Slavic, Turkic, Sino\u2011Tibetan), including cross\u2011lingual transfer, zero\u2011shot performance, and syntactic probing experiments using Universal Dependencies, CoNLL\u20112018, and other multilingual benchmarks?"}
{"original_query": "Could you suggest research that trains language models specifically on mental health-related social media data and the model is helpful for identifying mental health issues?", "expanded_query": "Could you recommend recent NLP research that fine\u2011tunes transformer\u2011based language models on mental\u2011health\u2013related social media data from platforms such as Twitter, Reddit, Facebook, and Instagram, employing techniques like sentiment analysis, emotion detection, self\u2011harm, depression, and anxiety identification, while addressing data privacy, ethical considerations, bias mitigation, annotation strategies, clinical validation, and public health surveillance applications?"}
{"original_query": "Could you suggest research that tries to interpret how bi-directional RNNs manage to carry out Named Entity Recognition (NER) tasks?", "expanded_query": "Could you recommend research papers, surveys, or review articles that investigate the interpretability of bidirectional recurrent neural networks, including biLSTM and biGRU architectures, for named entity recognition tasks, focusing on model explanation techniques such as attention visualization, gradient-based attribution, saliency maps, layer-wise relevance propagation, and other explainable AI methods, and covering datasets like CoNLL-2003, OntoNotes, WikiANN, and evaluation metrics such as precision, recall, and F1, as well as error analysis and case studies, and referencing conferences like ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, and sources like arXiv and Google Scholar?"}
{"original_query": "Could you suggest some work that develops multimodal models with contrastive learning approaches?", "expanded_query": "Could you suggest some recent research papers, datasets, and projects that develop multimodal models using contrastive learning approaches such as CLIP, ALIGN, SimCLR, MoCo, VILBERT, BLIP, and contrastive predictive coding for vision\u2011language, audio\u2011visual, and cross\u2011modal retrieval tasks, including self\u2011supervised learning, InfoNCE loss, negative sampling, positive pairs, cross\u2011modal alignment, multimodal embeddings, zero\u2011shot learning, multimodal transformers, and applications in medical imaging, robotics, and video understanding?"}
{"original_query": "Could you suggest studies focused on emotion-classification within group dialogues?", "expanded_query": "Could you suggest studies focused on emotion classification within group dialogues, including machine learning approaches, natural language processing techniques, sentiment analysis, affective computing, deep learning models such as transformers (BERT, RoBERTa), multimodal emotion detection, emotion taxonomies (valence\u2011arousal, discrete emotions), group dynamics analysis, social media group chat datasets, human\u2011computer interaction research, and recent conference proceedings from ACL, EMNLP, NAACL, IEEE, and journal articles on group conversation emotion recognition?"}
{"original_query": "Has any research explored using other off-the-shelf summarization techniques to improve neural abstractive summarization?", "expanded_query": "Has any research explored using other off-the-shelf summarization techniques such as extractive summarization, graph-based summarization, or transformer-based models like BERTSum, XLNet, GPT\u20113, BART, T5, PEGASUS, Longformer, or BigBird to improve neural abstractive summarization on datasets such as CNN/DailyMail, XSum, multi\u2011document summarization, or domain adaptation tasks, evaluated with ROUGE, METEOR, BLEU, human evaluation, and reported in conferences like ACL, EMNLP, NAACL, ICLR, NeurIPS, or in systematic reviews, meta\u2011analyses, or surveys on summarization pipelines, hybrid summarization, and cross\u2011lingual summarization?"}
{"original_query": "Has any research tried to mitigate overfitting in weakly-supervised settings by introducing an adversarial framework where the influence of the labeling function is a hyperparameter for the feature representation?", "expanded_query": "Has any research investigated mitigating overfitting in weakly-supervised learning by employing adversarial frameworks where the influence of labeling functions is treated as a hyperparameter for feature representation, including domain-adversarial neural networks, adversarial regularization for weak supervision, Snorkel data programming with hyperparameter tuning, label noise robustness, self-supervised pretraining, curriculum learning, hyperparameter optimization methods such as Bayesian optimization, random search, Hyperband, population-based training, meta-learning, and works presented at NeurIPS, ICML, ICLR, CVPR, AAAI, ACL, JMLR, and arXiv that explore controlling labeling function influence in representation learning?"}
{"original_query": "Has any study explored the zero-shot extraction of persona characteristics within conversational dialogues?", "expanded_query": "Has any study explored the zero-shot extraction of persona characteristics within conversational dialogues using zero-shot learning, natural language processing, transformer-based models, dialogue datasets such as Persona-Chat, focusing on persona extraction, dialogue analysis, dialogue persona modeling, entity recognition, semantic role labeling, contextual embeddings, information extraction, dialogue act recognition, and unsupervised or few-shot learning approaches?"}
{"original_query": "Has there been any research that uses multiple models to learn the preferences of individual annotators, and then ensembles these models to obtain majority-vote preference scores while also having an uncertainty measure?", "expanded_query": "Has there been any research that uses multiple models to learn the preferences of individual annotators, such as crowdsourced or expert labeling, and then ensembles these models via majority\u2011vote or soft voting to obtain aggregated preference scores while also providing an uncertainty measure through Bayesian neural networks, deep ensembles, Monte Carlo dropout, entropy\u2011based confidence, or probabilistic label aggregation, and that addresses inter\u2011annotator agreement, annotator reliability, label noise, and incorporates techniques like Bradley\u2011Terry, Thurstone\u2011Mosteller, ordinal regression, learning\u2011to\u2011rank, pairwise comparisons, Bayesian inference, latent variable modeling, ensemble diversity, meta\u2011learning, stacking, bagging, transformer\u2011based models, uncertainty calibration, and active learning?"}
{"original_query": "Has there been any work that improves the work on integrated gradients by leveraging interpolation strategies to improve gradient accuracies?", "expanded_query": "Has there been any recent research or literature that improves upon the Integrated Gradients attribution method by employing interpolation strategies\u2014such as path sampling, smoothgrad, noise injection, higher-order derivative approximations, or Monte Carlo integration\u2014to enhance gradient accuracy, reduce estimation bias, and improve attribution consistency and robustness on benchmark datasets like ImageNet, CIFAR\u201110, or MNIST, and how do these methods compare with other gradient\u2011based attribution techniques such as DeepLIFT, Layer\u2011wise relevance propagation, Grad\u2011CAM, and smooth gradient\u2011based explanations in terms of computational efficiency and explainable AI performance?"}
{"original_query": "Have any new metrics been developed to assess the factual alignment of machine-generated summaries with their original source texts?", "expanded_query": "Have any new metrics such as FactCC, QAGS, SummaC, or other source alignment, factual consistency, groundedness, and faithfulness measures been developed to evaluate the factual alignment and source fidelity of machine-generated summaries against their original source texts in NLP summarization research, including benchmarks like XSum, CNN/DailyMail, and evaluation frameworks using BERTScore, ROUGE, BLEU, and human judgments?"}
{"original_query": "Have any papers tried to address the background-shift problem in named entity recognition by identifying non-entity type tokens belonging to old entity types through knowledge distillation from an old model?", "expanded_query": "Have any papers addressed the background\u2011shift problem in named entity recognition by identifying non\u2011entity type tokens belonging to old entity types through knowledge distillation from an old model, exploring domain adaptation, concept drift, entity type shift, background class identification, teacher\u2011student distillation, incremental learning, continual learning, semantic drift, label noise, transfer learning, fine\u2011tuning transformer\u2011based models such as BERT, contextualized embeddings, and domain shift detection in token\u2011level classification for NER?"}
{"original_query": "Have any recent publications explored the use of neural network methods, like transformer architectures, in creating novel readability metrics?", "expanded_query": "Have any recent publications (2020\u20112025) explored the use of neural network methods, such as transformer architectures (BERT, GPT, RoBERTa, XLNet, T5, BART, ELECTRA, ALBERT, DistilBERT), in creating novel readability metrics for text complexity assessment, incorporating lexical, syntactic, and semantic features, evaluated on standard readability datasets (Flesch\u2011Kincaid, SMOG, Coleman\u2011Liau, and human\u2011annotated corpora from Wikipedia, Project Gutenberg, Common Crawl), and reported performance using accuracy, F1, correlation with human judgments, and published in venues like ACL, EMNLP, NAACL, IEEE, Springer, Elsevier, or arXiv?"}
{"original_query": "Have any research efforts been made to gather dialogue data via crowdworkers to enhance conversational information retrieval systems?", "expanded_query": "Have any research efforts, studies, or publications investigated crowdsourcing dialogue data via crowdworkers, such as on Amazon Mechanical Turk, to enhance conversational information retrieval systems, dialogue-based search, retrieval-augmented generation, semantic search, or contextualized embeddings for question answering and user intent modeling, including annotation quality, task design, incentive schemes, and evaluation benchmarks?"}
{"original_query": "Have any research papers been published on models for representing sentences in under-resourced languages like Slovenian or Romanian?", "expanded_query": "Have any research papers been published on models for representing sentences in under-resourced languages like Slovenian or Romanian, including sentence embeddings, semantic representation, multilingual BERT, XLM\u2011R, cross\u2011lingual transfer, low\u2011resource NLP, data augmentation, transfer learning, pre\u2011trained models, sentence\u2011level tasks such as classification, similarity, clustering, semantic role labeling, syntactic parsing, evaluation metrics, benchmark datasets, and open\u2011source resources for Slovenian and Romanian linguistic corpora in conferences such as ACL, EMNLP, COLING, LREC, and on platforms like ACL Anthology and arXiv?"}
{"original_query": "Have any research papers collected feedback from real users who were using LLMs for scientific writing?", "expanded_query": "Are there any peer\u2011reviewed research papers that report user studies or empirical evaluations of real scientists or academic authors collecting feedback while using large language models (LLMs) such as GPT\u20114, Claude, or other LLM\u2011based writing assistants for scientific manuscript drafting, literature review, or data analysis tasks?"}
{"original_query": "Have any research papers critically analyzed the performance speed of non-autoregressive translation models compared to autoregressive models", "expanded_query": ""}
{"original_query": "Have any research papers examined the efficacy of multilingual text-to-text transformers across various languages, particularly those less represented in pretraining corpora?", "expanded_query": "Have any research papers examined the efficacy of multilingual text-to-text transformers such as mT5, mBART, and XLM\u2011R across various languages, particularly low\u2011resource and underrepresented languages in pretraining corpora, assessing cross\u2011lingual transfer, zero\u2011shot performance, fine\u2011tuning effectiveness, benchmark datasets, evaluation metrics, language coverage bias, typologically diverse language families, language representation learning, and pretraining data imbalance?"}
{"original_query": "Have any research papers examined whether using language models for providing evidence in fact-checking systems risks propagating biases?", "expanded_query": "Have any research papers examined whether using language models such as GPT\u20114, BERT, or other transformer\u2011based LLMs for providing evidence in fact\u2011checking systems risks propagating biases, including algorithmic bias, bias amplification, or bias in evidence generation, and what bias mitigation or detection strategies have been proposed in peer\u2011reviewed academic studies or systematic reviews?"}
{"original_query": "Have any research papers explored methods to improve BERT's efficiency on long-text tasks, such as early exiting or self-distillation strategies?", "expanded_query": "Which research papers have investigated improving BERT's efficiency on long\u2011text tasks through early exiting, self\u2011distillation, knowledge distillation, model pruning, sparse attention, dynamic inference, adaptive computation, and other compression techniques for transformer models such as Longformer, BigBird, Reformer, Linformer, and Sparse Transformer, focusing on document\u2011level classification, summarization, and QA benchmarks like GLUE, SuperGLUE, SQuAD, CNN/DailyMail, and evaluating trade\u2011offs in latency, GPU memory, and accuracy?"}
{"original_query": "Have any research papers introduced a dedicated pre-training architecture designed to improve dense retrieval system efficacy?", "expanded_query": "Which research papers or academic publications have introduced a dedicated pre\u2011training architecture or methodology, such as contrastive learning, masked language modeling, or triplet loss, specifically designed to improve dense retrieval system efficacy, including performance metrics like Recall@k or NDCG on benchmarks such as MS\u202fMARCO, Natural Questions, or TREC Deep Learning, and how do these pre\u2011training objectives enhance semantic search or sentence embedding quality in dense passage retrieval frameworks like DPR or BERT\u2011based retrieval models?"}
{"original_query": "Have any research papers investigated human capacity to distinguish AI-generated text from human-authored text?", "expanded_query": "Have any research papers, studies, or academic articles examined human capacity, perception, or accuracy in distinguishing AI-generated text from human-authored text, including investigations of human detection performance, bias, error rates, and the impact of language models such as GPT-3, GPT-4, and other large language models on human discernment in contexts such as academic writing, journalism, social media, content moderation, creative writing, marketing content, advertising, news articles, essays, research articles, content creation, content verification, content labeling, and content classification?"}
{"original_query": "Have any research papers investigated the creation of datasets through model-generated data for annotators to identify hallucinations in the results, specifically for developing diagnostic evaluation datasets?", "expanded_query": "Have any peer\u2011reviewed studies, conference proceedings, or arXiv preprints investigated the creation of diagnostic evaluation datasets through model\u2011generated synthetic data for annotators to identify hallucinations in large language model outputs, including annotation guidelines, inter\u2011annotator agreement metrics, semantic consistency checks, factual consistency verification, knowledge base alignment, hallucination taxonomy, human\u2011in\u2011the\u2011loop annotation workflows, crowdsourcing versus expert annotation, annotation toolkits, evaluation metrics, benchmark dataset development, AI safety and alignment research, and how these datasets support evaluation frameworks for OpenAI GPT\u20114, ChatGPT, Anthropic Claude, Google PaLM, Meta LLaMA, and Microsoft Azure OpenAI services?"}
{"original_query": "Have any research papers suggested methods for summarizing arbitrary length documents without truncating the input, by using memory networks?", "expanded_query": "Have any research papers suggested methods for summarizing arbitrary length documents without truncating the input by using memory networks, neural memory networks, memory\u2011augmented neural networks, hierarchical attention, transformer memory, pointer\u2011generator networks, attention over memory, long\u2011range dependencies, sequence\u2011to\u2011sequence models, deep learning, or other memory\u2011network architectures for long document summarization, and what are the key findings in ACL, EMNLP, NeurIPS, ICLR, or arXiv studies?"}
{"original_query": "Have any research papers suggested techniques for automatically choosing in-context examples?", "expanded_query": "Have any research papers suggested techniques for automatically selecting in-context examples for few-shot learning in large language models such as GPT-3 or GPT-4, using methods like semantic similarity, clustering, retrieval-augmented generation, example weighting, or prompt optimization?"}
{"original_query": "Have any research papers tried to create conversational agents with inner states represented by a knowledge graph that can be continually updated based on the agent\u2019s environment?", "expanded_query": "Have any research papers explored building conversational agents whose internal states are modeled as dynamic knowledge graphs that can be continuously updated from environmental inputs, leveraging graph neural networks, knowledge graph embeddings, semantic memory, real\u2011time context awareness, adaptive dialogue management, reinforcement learning, and continuous learning for improved human\u2011computer interaction?"}
{"original_query": "Have any studies explored the creation of memory management systems in AI to improve sustained conversational capabilities and tackle the challenge of contextual retention over extended periods?", "expanded_query": "Which research studies, papers, or experiments have investigated the design and implementation of memory management systems in artificial intelligence, including neural memory networks, transformer memory modules, episodic and working memory architectures, knowledge graph integration, and neuroscience\u2011inspired memory consolidation mechanisms, to enhance sustained conversational capabilities, maintain contextual retention, and improve dialogue coherence over extended periods in chatbots and conversational AI?"}
{"original_query": "Have there been any advancements in language models that operate without tokenization and emphasize encoding at the character level, and what benefits might they have compared to conventional methods using subword tokenization?", "expanded_query": "Have there been any recent advancements in tokenization\u2011free language models that emphasize character\u2011level encoding, such as character\u2011level transformers, Charformer, or byte\u2011level BPE models, and what benefits might they offer over conventional subword tokenization methods in terms of lexical coverage, out\u2011of\u2011vocabulary handling, multilingual transfer, computational efficiency, and model performance?"}
{"original_query": "How can I locate a dataset containing toxic sentence pairs alongside their non-toxic paraphrased counterparts for studying text detoxification?", "expanded_query": "Where can I find a publicly available dataset of toxic sentence pairs and their non\u2011toxic paraphrased counterparts for text detoxification research, including resources such as the Toxic Comment Classification Challenge, Hatebase, Waseem and Hovy dataset, OpenAI toxicity dataset, Kaggle, Hugging Face datasets, GitHub repositories, and academic corpora, with annotations for toxicity, sentiment, paraphrase quality, and contextualized language use, to support transformer\u2011based detoxification models, data augmentation, bias mitigation, and ethical AI studies?"}
{"original_query": "How can SQL-to-text be utilized to improve text-to-SQL parsing through data augmentation techniques?", "expanded_query": "How can SQL-to-text generation be leveraged to enhance text-to-SQL parsing accuracy and robustness through synthetic data augmentation, back-translation, paraphrase generation, schema-aware template augmentation, transformer-based models such as T5 or BART, and evaluation metrics like BLEU, ROUGE, and accuracy across multiple SQL dialects (PostgreSQL, MySQL, SQLite) and multilingual contexts?"}
{"original_query": "How can dense retrieval models for open-domain question answering be improved, specifically through the use of hard negative mining techniques?", "expanded_query": "How can dense retrieval models for open-domain question answering be improved, specifically through the use of hard negative mining techniques, contrastive learning, triplet loss, BERT-based bi-encoders, sentence transformers, FAISS approximate nearest neighbor search, embedding space optimization, negative sampling strategies, in-batch negatives, adversarial example generation, dataset augmentation, fine-tuning on domain-specific corpora, evaluation metrics such as Recall@k, MRR, NDCG, hyperparameter tuning of learning rate, batch size, optimizer, temperature, margin, GPU acceleration, distributed training, model compression, pruning, quantization, knowledge distillation, and integration with retrieval-augmented generation pipelines for real-time retrieval?"}
{"original_query": "I know about prompt tuning, but have any works tried learning embeddings that are inputted to every transformer layer in a language model?", "expanded_query": "I know about prompt tuning, but have any works tried learning embeddings that are inputted to every transformer layer in a language model, such as BERT, GPT\u20113, GPT\u20114, T5, RoBERTa, or XLNet, exploring layer\u2011wise prompt tuning, deep prompt, prefix tuning, adapter modules, embedding injection per layer, parameter\u2011efficient tuning, or other in\u2011context learning techniques?"}
{"original_query": "In multi-hop question answering, is there a paper that explores \"per-hop\" retrieval evaluation that treats each hop of retrieval independently?", "expanded_query": "In multi\u2011hop question answering, is there a paper that explores per\u2011hop retrieval evaluation treating each hop of retrieval independently, analyzing hop\u2011level retrieval quality, metrics such as recall@k, precision@k, MRR, NDCG, and F1, in contexts of passage retrieval, document retrieval, graph\u2011based retrieval, knowledge\u2011graph retrieval, chain\u2011of\u2011thought reasoning, and retrieval\u2011augmented generation, using datasets like HotpotQA, WikiHop, Natural Questions, and methods such as DPR, ColBERT, ANCE, REALM, and RAG?"}
{"original_query": "Is it possible to adatp named entity recognition systems to learn new entities by combining knowledge distillation and synthetic data augmentation by using a two-stage training approach?", "expanded_query": "Can we adapt named entity recognition systems to learn new entities by combining knowledge distillation and synthetic data augmentation using a two-stage training pipeline that incorporates transformer-based models, teacher\u2011student fine\u2011tuning, semi\u2011supervised learning, entity span detection, type classification, synthetic corpus generation, paraphrasing, entity replacement, noise injection, and cross\u2011lingual domain adaptation to improve entity recall, precision, and F1 on domain\u2011specific benchmarks?"}
{"original_query": "Is there a benchmark designed to assess language models' social biases within question answering frameworks, specifically one encompassing diverse categories of social bias and offering instances of ambiguity and its resolution?", "expanded_query": "Is there a benchmark designed to assess language models' social biases within question answering frameworks, specifically one encompassing diverse categories of social bias such as gender, racial, religious, sexual orientation, class, and intersectional biases, offering instances of contextual ambiguity and its resolution, along with bias detection, measurement, mitigation tasks, annotation guidelines, and evaluation metrics for large language models in AI fairness and ethics research?"}
{"original_query": "Is there a comprehensive dataset available for summarizing broad-spectrum conversational dialogues?", "expanded_query": "Is there a comprehensive publicly available dataset for summarizing broad-spectrum conversational dialogues, such as multi-turn open-domain chat logs, customer support transcripts, chatbot interactions, and multi-party conversations, that can be used for dialogue summarization research and evaluation with metrics like ROUGE and BLEU, and that includes annotations for dialogue acts, context, and speaker roles?"}
{"original_query": "Is there a dataset available for open-domain targeted sentiment analysis containing user reviews from platforms like Yelp and Amazon?", "expanded_query": "open-domain targeted sentiment analysis dataset user reviews Yelp Amazon product reviews restaurant reviews movie reviews consumer feedback sentiment lexicon aspect-based sentiment fine-grained sentiment polarity annotation guidelines domain adaptation cross-domain sentiment analysis multi-domain sentiment analysis review corpus natural language processing machine learning dataset availability open-source academic resources Kaggle GitHub data repository"}
{"original_query": "Is there a dataset containing question-answer pairs used in psychological counseling available for research?", "expanded_query": "Is there a publicly available, de-identified dataset of question-answer pairs from psychological counseling or psychotherapy sessions, including therapy transcripts, counseling conversation logs, mental health counseling Q&A data, clinical trial data, or open-source counseling data repositories for NLP or machine learning research?"}
{"original_query": "Is there a research paper that has developed a customer service conversation dataset aimed at forecasting customer intentions, taking into account the limitations imposed by agent protocols?", "expanded_query": "Is there a research paper that has developed a customer service conversation dataset aimed at forecasting customer intentions, taking into account the limitations imposed by agent protocols, incorporating natural language processing, machine learning, intent classification, sentiment analysis, dialogue act tagging, context-aware embeddings, policy compliance, agent guideline constraints, privacy regulations such as GDPR and CCPA, bias mitigation, annotation guidelines, cross-validation, evaluation metrics like precision recall F1, and real-time analytics for call center, chat, and ticketing systems, while providing an open-source repository, multilingual support, domain adaptation, and transfer learning for predictive modeling of customer churn, satisfaction, loyalty, and experience?"}
{"original_query": "Is there a specialized question answering dataset that concentrates on intricate tables within certain sectors, like the airline industry?", "expanded_query": "Is there a specialized question answering dataset that focuses on complex, structured tables in specific sectors such as the airline industry, covering flight schedules, delays, cancellations, revenue management, load factors, aircraft maintenance, and other aviation operational data, and that supports table-based QA, table reasoning, and semantic parsing tasks?"}
{"original_query": "Is there a study that investigates if large language models can assist with generating ideas pertinent to scientific concepts during the scientific writing ideation stage?", "expanded_query": "Is there an empirical study that investigates how large language models, such as GPT\u20114 or other transformer\u2011based LLMs, can assist researchers with generating ideas pertinent to scientific concepts during the scientific writing ideation stage, including the impact on idea relevance, novelty, coherence, and overall writing quality, and evaluating human\u2011AI collaboration through qualitative and quantitative methods, prompt engineering, domain\u2011specific fine\u2011tuning, and metrics like BLEU, ROUGE, and human judgment, while addressing ethical considerations, bias, and trustworthiness in AI\u2011assisted ideation for academic writing and scientific communication?"}
{"original_query": "Is there any paper that tried fine-tuning mBERT to enhance word-level alignment in a multilingual setting?", "expanded_query": "Is there any research paper that explored fine\u2011tuning multilingual BERT (mBERT) to improve word\u2011level alignment across languages, using cross\u2011lingual word alignment techniques, parallel corpora, token alignment, alignment evaluation metrics such as alignment error rate and alignment accuracy, multilingual embedding alignment methods, and applied to cross\u2011lingual NLP tasks such as NER, QA, sentiment analysis, and machine translation in a multilingual setting?"}
{"original_query": "Is there any research that investigates how to use Transformer decoders to extract interactive shared representations from CNN networks on clinical notes?", "expanded_query": "Is there any research that investigates how to use Transformer decoders to extract interactive shared representations from CNN networks on clinical notes, including studies on clinical natural language processing, electronic health records, deep learning, bidirectional transformer architectures, attention mechanisms, shared representation learning, semantic embeddings, multimodal data integration, transfer learning, clinical decision support, knowledge graphs, clinical notes preprocessing, tokenization, word embeddings, BERT, ClinicalBERT, CNN-based feature extraction, transformer decoder architecture, cross-modal representation, clinical data integration, deep learning frameworks such as PyTorch and TensorFlow, journal articles, conference proceedings, PubMed, arXiv, NLP for healthcare, medical informatics, clinical text mining, deep learning for clinical notes, CNN-transformer hybrid models, shared latent space, clinical note embeddings, semantic similarity, clinical concept extraction, UMLS, SNOMED CT, clinical NLP pipelines, and clinical ontology?"}
{"original_query": "Is there research examining if multilingual pre-trained models utilize identical sets of neural units to encode morphosyntactic features in various languages?", "expanded_query": "Is there research examining whether multilingual pre\u2011trained models such as mBERT, XLM\u2011RoBERTa, or other cross\u2011lingual transformers utilize identical sets of neural units, attention heads, or embedding subspaces to encode morphosyntactic features across diverse languages, language families, and typological categories, and how this relates to cross\u2011lingual transfer, parameter sharing, representation similarity, and neural network interpretability methods like probing tasks, attention visualization, and activation pattern analysis?"}
{"original_query": "Is there research on a specialized language model designed to detect mental health issues on social media platforms?", "expanded_query": "research on specialized language model for detecting mental health issues on social media platforms such as Twitter, Reddit, Facebook, Instagram, including depression, anxiety, self-harm, suicide risk, using natural language processing, transformer models like BERT and GPT, clinical validation, digital phenotyping, sentiment analysis, emotion detection, privacy concerns, bias mitigation, public health surveillance, peer-reviewed studies, datasets, and ethical considerations."}
{"original_query": "Is there research that argues for transparency and open-access to the training data of LLMs and demontrates its importance with case studies of existing data?", "expanded_query": "Is there research that argues for transparency and open-access to the training data of large language models (LLMs), covering data provenance, documentation, bias mitigation, data governance, licensing, privacy, and lineage, and demonstrates its importance through case studies of existing datasets such as OpenAI GPT\u20113, GPT\u20114, Meta LLaMA, Google PaLM, and other open\u2011source corpora, while discussing implications for AI safety, accountability, and ethical AI development?"}
{"original_query": "Is there research that investigates embedding multi-bit data into watermarks to improve resilience to text corruption, particularly aimed at safeguarding keywords and syntactic elements from modification?", "expanded_query": "Is there research on embedding multi-bit data into textual watermarks to improve resilience to text corruption, focusing on safeguarding keywords, syntactic elements, and semantic integrity against editing, adversarial attacks, and corruption, including studies on robust watermarking techniques, data embedding capacity, textual steganography, NLP-based watermarking methods, watermark robustness metrics, and evaluation of keyword protection and syntactic integrity preservation?"}
{"original_query": "Is there work on text classification that explores using BERT for the text and using GNN for the label hierarchy?", "expanded_query": "Is there research on hierarchical multi-label text classification that uses BERT or transformer-based models for text representation and graph neural networks such as GCN, GAT, GraphSAGE, or GATv2 for modeling label hierarchies, label taxonomies, ontology-based label graphs, label dependency modeling, semantic label graph construction, joint learning of text and label embeddings, attention over label graph, label propagation, label graph representation learning, evaluation on datasets like Reuters-21578, RCV1, OHSUMED, and metrics such as micro/macro F1, hierarchical precision/recall, label propagation performance, end-to-end training, and hierarchical attention?"}
{"original_query": "What approaches have been suggested to lower the computational demands of basic attention mechanisms in transformers, and is there a study that explores a memory-saving technique through selective key-value pairing for each query?", "expanded_query": "What approaches have been suggested to lower the computational demands of basic attention mechanisms in transformers, including linear attention, sparse attention, low\u2011rank factorization, kernel\u2011based approximations, and memory\u2011efficient variants such as Linformer, Reformer, Performer, Longformer, and BigBird, and is there a study that explores a memory\u2011saving technique through selective key\u2011value pairing for each query, possibly involving query\u2011specific key\u2011value selection, attention pruning, or memory\u2011efficient transformer architectures, and can you provide details on the authors, publication venue, and key findings of such a paper?"}
{"original_query": "What are some scholarly articles that explore scaling laws for parameter-efficient prompt tuning techniques for fine-tuning language models?", "expanded_query": "scholarly articles exploring scaling laws for parameter-efficient prompt tuning techniques for fine-tuning language models, including transformer-based LLMs, prompt engineering, few-shot learning, parameter efficiency, prompt-based fine-tuning, scaling behavior, model size, dataset size, training dynamics, and related research literature."}
{"original_query": "What are some scholarly articles that explore the enhancement of dense retrieval in student models through the application of prediction distributions from teacher models?", "expanded_query": "scholarly articles on knowledge distillation for dense retrieval, teacher\u2011student models, student model enhancement via teacher prediction distributions, dense passage retrieval, transformer\u2011based student models, soft label transfer, teacher guidance in information retrieval, neural ranking, dense retrieval improvement, student\u2011teacher distillation, dense retrieval performance, academic literature on teacher\u2011student distillation in IR, dense retrieval enhancement through teacher predictions, student model fine\u2011tuning with teacher distributions, research on dense retrieval augmentation, teacher\u2011student knowledge transfer in retrieval models, dense retrieval improvement via prediction distributions, student model learning from teacher predictions, dense retrieval and knowledge distillation, student model performance with teacher guidance, academic papers on dense retrieval enhancement using teacher predictions."}
{"original_query": "What are some studies that explore data-poisoning strategies that only require very few poisoned training examples?", "expanded_query": "Which academic studies, research papers, or case studies investigate data\u2011poisoning strategies that require only a few poisoned training examples, such as minimal or few\u2011shot poisoning attacks, label\u2011flipping, gradient\u2011based or optimization\u2011based poisoning, and are presented in venues like NeurIPS, ICML, ICLR, CVPR, or published by researchers at MIT, Stanford, Google Research, Microsoft Research, or on arXiv?"}
{"original_query": "What are the latest advancements in predicting suicidal tendencies using innovative feature extraction methods?", "expanded_query": "What are the latest advancements in predicting suicidal tendencies using innovative feature extraction methods such as machine learning, deep learning, natural language processing, multimodal data fusion, electronic health records, social media sentiment analysis, neuroimaging biomarkers, speech and voice prosody analysis, wearable sensor data, graph neural networks, and explainable AI, and how do these approaches address ethical considerations, data privacy, bias mitigation, clinical decision support integration, real\u2011time monitoring, and population health surveillance?"}
{"original_query": "What are the latest developments in conversational agents that integrate external knowledge sources and employ diverse tactics to offer emotional support during interactions?", "expanded_query": "What are the latest developments in conversational agents, including AI chatbots, retrieval\u2011augmented generation, knowledge graph integration, semantic search, and external knowledge base retrieval, that employ diverse tactics such as affective computing, emotion detection, sentiment analysis, personalized therapy, cognitive behavioral therapy, mindfulness, and other mental health support techniques to offer emotional support during interactions, while addressing ethical considerations, privacy, bias mitigation, user safety, explainability, and continuous learning in real\u2011time, multimodal, and contextually grounded dialogue systems?"}
{"original_query": "What benchmarks have prior research utilized to assess models performing knowledge-rich language tasks?", "expanded_query": "What benchmarks, datasets, and evaluation metrics have prior research utilized to assess models performing knowledge-rich language tasks such as fact verification, open-domain question answering, multi-hop reasoning, commonsense inference, entity linking, relation extraction, knowledge graph completion, and knowledge graph reasoning, and which language models (e.g., BERT, RoBERTa, GPT\u20113, T5, XLNet, ELECTRA, ERNIE) and research institutions (e.g., Stanford, Allen Institute, OpenAI, Google AI) have contributed to these benchmarks (e.g., LAMBADA, SQuAD, TriviaQA, HotpotQA, LAMA, KILT, ATOMIC, ConceptNet, FEVER, WNLI, GLUE, SuperGLUE, WIKIPEDIA, WIKIDATA)?>"}
{"original_query": "What concerns or key points have been highlighted in scholarly articles about employing random divisions in machine learning datasets, especially with respect to contamination of the test set?", "expanded_query": "What concerns or key points have been highlighted in scholarly articles about employing random divisions in machine learning datasets, especially with respect to contamination of the test set, including issues of data leakage, overfitting, bias\u2011variance tradeoff, cross\u2011validation pitfalls, random train\u2011test split, holdout and validation sets, stratified sampling, random seed reproducibility, feature selection bias, distribution shift, sampling bias, hyperparameter tuning, model evaluation metrics, statistical significance, and best practices for dataset partitioning in machine learning research?"}
{"original_query": "What difficulties do neural conversational models face, particularly concerning the decoder's ability to produce precise and fact-based replies?", "expanded_query": "What challenges do neural conversational models, including transformer-based language models, sequence-to-sequence architectures, and retrieval-augmented generation systems, face, particularly concerning the decoder's ability to produce precise, fact-based, and contextually coherent replies, given issues such as hallucination, knowledge grounding, semantic accuracy, factual consistency, data sparsity, domain adaptation, evaluation metrics like BLEU, ROUGE, METEOR, human evaluation, sampling strategies (temperature, top\u2011k, top\u2011p), beam search, length and coverage penalties, diversity, repetition, fluency, relevance, topic drift, multi\u2011turn dialogue, long\u2011term dependencies, tokenization, subword units, pretraining, fine\u2011tuning, transfer learning, synthetic data, data augmentation, adversarial training, model calibration, confidence estimation, post\u2011processing, deterministic decoding, overfitting, underfitting, generalization, domain shift, memory networks, dialogue state tracking, entity linking, coreference resolution, knowledge graph integration, fact\u2011checking, source verification, citation generation, explainable AI, trustworthiness, reliability, robustness, model uncertainty, human\u2011in\u2011the\u2011loop, active learning, few\u2011shot and zero\u2011shot learning, meta\u2011learning, prompt engineering, prompt vulnerability, and prompt\u2011based generation?"}
{"original_query": "What literature is available on training semantic parsers with deep learning for knowledge base question answering systems, especially those employing tree-structured representations of queries?", "expanded_query": "What literature is available on training semantic parsers with deep learning for knowledge base question answering systems, especially those employing tree-structured representations of queries, abstract syntax trees, dependency trees, neural semantic parsing, tree\u2011LSTM, Transformer\u2011based models, BERT, RoBERTa, graph neural networks, tree\u2011structured neural networks, semantic parsing datasets such as Spider, GeoQuery, ATIS, and knowledge bases like Freebase, Wikidata, SPARQL, and OpenIE, and covering topics like structured prediction, tree\u2011based neural models, semantic parsing for KBQA, and recent survey papers on neural semantic parsing and tree\u2011structured query representation?"}
{"original_query": "What methods exist for tailoring news suggestions that consider a user's preferences as well as the current popularity of news stories?", "expanded_query": "What methods exist for tailoring news suggestions that consider a user's preferences as well as the current popularity of news stories, including personalized news recommendation algorithms, collaborative filtering, content-based filtering, hybrid recommendation systems, popularity bias mitigation, real-time trending topic integration, user profile enrichment, contextual relevance scoring, topic modeling, sentiment analysis, click-through rate optimization, engagement metrics, news aggregator personalization, algorithmic fairness, filter bubble avoidance, novelty and diversity enhancement, reinforcement learning approaches, contextual bandit models, time decay weighting, social media influence, user feedback loops, and evaluation metrics such as precision, recall, novelty, and serendipity?"}
{"original_query": "What papers discuss the effect of false negatives among hard negatives in dense retriever training?", "expanded_query": "Which research papers analyze the impact of false negatives among hard negatives on dense retriever training, including studies on negative sampling strategies, hard negative mining, contrastive learning, embedding space quality, retrieval effectiveness, and bias in dense retrieval models such as BERT, Sentence Transformers, and other neural ranking architectures?"}
{"original_query": "What papers explore replacing schema linking with human annotations to study the maximum potential benefit of schema linking for text-to-SQL tasks?", "expanded_query": "Which research papers investigate replacing automated schema linking with human annotations to evaluate the maximum potential benefit of schema linking for text-to-SQL tasks, covering semantic parsing, database schema alignment, entity linking, table linking, annotation effort, accuracy improvement, benchmark datasets like Spider and WikiSQL, neural semantic parsers such as SQLNet, SQLova, and Text-to-SQL, and human\u2011in\u2011the\u2011loop evaluation of schema linking modules?"}
{"original_query": "What recent developments in transformer architecture aim to improve the multi-head self-attention mechanism for better transmission of unprocessed attention scores and more stable training?", "expanded_query": "Recent developments in transformer architecture such as efficient attention, sparse attention patterns, relative positional encoding, adaptive attention, attention dropout, attention score scaling, attention score normalization, attention residual connections, attention gating, attention regularization, attention pruning, attention quantization, attention compression, attention bias correction, and transformer variants like Longformer, Reformer, Linformer, Performer, Sparse Transformer, Swin Transformer, Vision Transformer, BERT, GPT\u20114, T5, and Stable Diffusion aim to improve the multi\u2011head self\u2011attention mechanism for better transmission of unprocessed attention scores, more stable training, gradient clipping, learning rate warmup, AdamW optimizer, weight decay, layer normalization, batch normalization, and dropout."}
{"original_query": "What recent research has been conducted on improving few-shot learning in pre-trained language models through the use of prompt-based fine tuning techniques?", "expanded_query": "Recent research on improving few\u2011shot learning in pre\u2011trained language models through prompt\u2011based fine\u2011tuning techniques, including prompt engineering, prompt design, parameter\u2011efficient fine\u2011tuning methods such as LoRA, prefix tuning, adapter modules, and prompt\u2011based fine\u2011tuning for GPT\u20113, BERT, T5, and other transformer architectures, published in ACL, EMNLP, NeurIPS, ICLR, ICML, and on arXiv, focusing on benchmark datasets like GLUE, SuperGLUE, and few\u2011shot performance improvements."}
{"original_query": "What research articles should I consult to understand a method for quantitatively assessing how successful neuron interventions are at altering a model's predictions?", "expanded_query": "Which research articles on neural network interpretability, neuron-level ablation studies, neuron activation interventions, causal inference in deep learning, saliency and attribution methods, quantitative metrics for neuron importance, model prediction alteration analysis, neuron-level sensitivity analysis, and model debugging techniques should I consult to understand a method for quantitatively assessing how successful neuron interventions are at altering a model's predictions?"}
{"original_query": "What research exists comparing adapter-based tuning and full fine-tuning efficacy in limited data contexts?", "expanded_query": "What empirical studies and comparative analyses exist on adapter\u2011based tuning versus full fine\u2011tuning efficacy in limited data contexts, including few\u2011shot learning, low\u2011resource NLP, domain adaptation, cross\u2011lingual transfer, benchmark datasets such as GLUE, SuperGLUE, SQuAD, and evaluation metrics like accuracy, F1, BLEU, across pre\u2011trained models like BERT, RoBERTa, GPT, and methods such as LoRA, prefix tuning, prompt tuning, parameter\u2011efficient fine\u2011tuning, while considering computational cost, parameter count, overfitting, regularization, and future research directions?"}
{"original_query": "What research exists on employing generative models with latent variable to capture semantic dependencies in conversational systems?", "expanded_query": "What research exists on employing latent variable generative models, such as variational autoencoders, latent semantic analysis, and latent variable inference, to capture semantic dependencies, discourse coherence, and context\u2011aware dialogue in conversational systems, including dialogue state tracking, semantic role labeling, knowledge graph integration, and multi\u2011turn conversation modeling?"}
{"original_query": "What research exists on incorporating knowledge graphs into language models to improve their complex question-answering capabilities?", "expanded_query": "What research exists on incorporating knowledge graphs into language models to improve their complex question-answering capabilities, including knowledge graph embeddings, graph neural networks, semantic knowledge bases such as Wikidata and Freebase, neural-symbolic integration, retrieval-augmented generation, transformer-based language models like GPT\u20114, BERT, RoBERTa, knowledge graph reasoning, entity linking, relation extraction, graph attention networks, knowledge graph completion, and evaluation on datasets such as SQuAD, HotpotQA, OpenBookQA, and metrics such as precision, recall, and F1?"}
{"original_query": "What research exists on leveraging syntactic roles and semantic interpretations for backdoor attacks on natural language processing systems?", "expanded_query": "research on leveraging syntactic roles and semantic interpretations for backdoor attacks on natural language processing systems, including studies on dependency parsing, semantic role labeling, trigger-based trojan attacks, data poisoning, model integrity, adversarial examples, transformer-based models such as BERT, GPT, RoBERTa, and defenses in text classification, sentiment analysis, machine translation, named entity recognition, question answering, and text generation, survey papers, conference proceedings (ACL, EMNLP, NAACL, ICLR, NeurIPS, ICML), arXiv preprints, and literature on syntactic analysis, semantic analysis, and adversarial training in NLP."}
{"original_query": "What research exists on the impact of scaling on prompt tuning efficiency in pre-trained language models?", "expanded_query": "What research exists on the impact of scaling on prompt tuning efficiency in pre\u2011trained language models, including studies on large language models, scaling laws, GPT\u20113, BERT, T5, transformer architecture, prompt engineering, prompt tuning versus fine\u2011tuning, few\u2011shot and zero\u2011shot learning, resource constraints, compute budget, prompt length optimization, prompt selection, benchmark datasets, evaluation metrics, and scaling trade\u2011offs?"}
{"original_query": "What research exists on using reinforcement learning methods for event prediction in temporal knowledge graphs?", "expanded_query": "What research exists on applying reinforcement learning methods, such as policy gradient, Q\u2011learning, actor\u2011critic, and deep RL, to event prediction in temporal knowledge graphs, covering temporal link prediction, dynamic knowledge graph completion, temporal graph neural networks, temporal knowledge graph embeddings, temporal reasoning, and policy learning, and what datasets, benchmarks, evaluation metrics, key findings, and challenges are reported in these studies?"}
{"original_query": "What research has been conducted on applying contrastive techniques to distinguish normal from abnormal imagery for the creation of radiology reports?", "expanded_query": "What research has been conducted on applying contrastive techniques, including contrastive learning, self-supervised learning, triplet loss, contrastive predictive coding, and domain adaptation, to distinguish normal from abnormal medical imaging modalities such as CT, MRI, X-ray, and chest radiographs for the creation of radiology reports, encompassing deep learning models (CNNs, transformers), feature extraction, segmentation, attention mechanisms, multi-modal integration, natural language generation, evaluation metrics (AUC, F1), benchmark datasets (MIMIC-CXR, CheXpert, NIH ChestX-ray14), explainability, clinical decision support, and human-in-the-loop validation?"}
{"original_query": "What research has been conducted on creating neural network frameworks for parsing text into SQL?", "expanded_query": "What research has been conducted on creating neural network frameworks for parsing text into SQL, including neural network architectures, natural language processing, semantic parsing, deep learning, transformer models, encoder-decoder models, attention mechanisms, sequence-to-sequence models, BERT, GPT, T5, Seq2SQL, SQLNet, CoSQL, Spider dataset, NL2SQL, state-of-the-art benchmarks, evaluation metrics such as accuracy, precision, recall, F1, BLEU, ROUGE, semantic correctness, SQL execution, database schema linking, entity linking, relation extraction, SQL dialects, open-source implementations, PyTorch, TensorFlow, research papers, conference proceedings from ACL, EMNLP, NeurIPS, ICLR, SIGMOD, VLDB, arXiv, literature reviews, and relevant datasets and benchmarks?"}
{"original_query": "What research has been conducted on determining the ideal segment length for unsupervised keyphrase extraction?", "expanded_query": "What research has been conducted on determining the ideal segment length for unsupervised keyphrase extraction, including studies on window size optimization, sentence and paragraph segmentation, graph\u2011based ranking methods such as TextRank and TopicRank, embedding\u2011based approaches using BERT and transformer models, evaluation on benchmark datasets like SemEval\u20112010 and SemEval\u20112017, analysis of precision, recall, F1 metrics, cross\u2011validation, hyperparameter tuning, and future directions for scalability, cross\u2011lingual adaptation, and domain transfer?"}
{"original_query": "What research has been conducted on enhancing conversational generation models using knowledge sourced from the internet?", "expanded_query": "What research has been conducted on enhancing conversational generation models using knowledge sourced from the internet, including retrieval-augmented generation, knowledge grounding, web search APIs, knowledge graphs, semantic search, fact-checking, neural retrieval, large language models, open-domain dialogue, and knowledge-based dialogue systems?"}
{"original_query": "What research has been conducted on incorporating visual data into the text summarization process?", "expanded_query": "What research has been conducted on incorporating visual data into the text summarization process, including multimodal summarization, image captioning, visual feature extraction, CNNs, transformers, visual attention, image embeddings, CLIP, BERT, cross-modal learning, fusion strategies, image-text alignment, scene graph, knowledge graphs, datasets such as MS COCO, Visual Genome, VQA, evaluation metrics like ROUGE, BLEU, METEOR, human evaluation, automatic evaluation, and applications in news, scientific papers, e-commerce product descriptions?"}
{"original_query": "What research has been conducted on news recommendation engines that consider individual user preferences as well as the time-sensitive popularity of news content?", "expanded_query": "news recommendation engines, personalized news recommendation, user preference modeling, time-sensitive popularity, temporal dynamics, recency bias, real-time recommendation, content-based filtering, collaborative filtering, contextual bandits, reinforcement learning, NLP for news, topic modeling, sentiment analysis, topic drift, time decay, recency weighting, user engagement metrics, click-through rate, A/B testing, evaluation metrics, academic research, conference proceedings, journal articles, IEEE, ACM, SIGIR, RecSys, KDD, ICWSM, news recommendation literature review, MIND dataset, NewsCrawl dataset, cold start problem, personalization challenges, news consumption patterns, news article freshness, temporal relevance, news recommendation algorithms, contextual recommendation, news recommendation challenges, real-time news recommendation, news recommendation systems, news recommendation research, user modeling, time-aware recommendation, popularity-based ranking, news recommendation datasets, news aggregator, news recommendation engines"}
{"original_query": "What research has been conducted on scaling within-document coreference resolution in extended texts?", "expanded_query": "What research has been conducted on scaling within-document coreference resolution in extended texts, including long documents, large-scale datasets, transformer-based models such as BERT, Longformer, and BigBird, hierarchical attention mechanisms, document segmentation, entity linking and disambiguation, evaluation on benchmarks like OntoNotes, CoNLL-2012, WikiCoref, and the challenges of memory efficiency, computational complexity, cross-sentence coreference, and application to legal, scientific, news, and social media domains?"}
{"original_query": "What research has been conducted on the impact of intervening at intermediate layers in pretrained language models to alter the resulting text?", "expanded_query": "What research has been conducted on the impact of intervening at intermediate layers in pretrained language models such as GPT, BERT, and T5 to alter the resulting text, including studies on transformer layer manipulation, attention head editing, layer\u2011wise fine\u2011tuning, prompt engineering, controlled text generation, semantic and syntactic manipulation, bias mitigation, model interpretability, and neural editing techniques, and what datasets, evaluation metrics, and experimental results have been reported?"}
{"original_query": "What research is available on acquiring sentence embeddings through unsupervised approaches, possibly employing contrastive learning methods?", "expanded_query": "What research is available on acquiring sentence embeddings through unsupervised approaches, possibly employing contrastive learning methods, including self-supervised learning, transformer-based models such as BERT, SimCSE, sentence\u2011BERT, and evaluation on semantic textual similarity benchmarks like STS, GLUE, and downstream tasks, and covering contrastive loss functions, data augmentation, noise contrastive estimation, and triplet loss?"}
{"original_query": "What research is available on hybrid approaches that combine extractive and abstractive methods for summarizing extensive texts?", "expanded_query": "What research is available on hybrid extractive\u2011abstractive summarization approaches for extensive texts, covering transformer models (BERT, GPT, T5, BART, PEGASUS, Longformer, LED), pointer\u2011generator networks, coverage mechanisms, graph neural networks, joint training, multi\u2011task learning, knowledge distillation, and domain adaptation, evaluated on datasets such as CNN/DailyMail, XSum, Gigaword, PubMed, arXiv, and using metrics like ROUGE, BLEU, METEOR, BERTScore, and human evaluation, and published in venues like ACL, EMNLP, NAACL, ICLR, NeurIPS, and available via arXiv, Google Scholar, Hugging Face, Fairseq, OpenNMT, AllenNLP?"}
{"original_query": "What research is available on the concept of using prefix tokens as a parameter-efficient method for fine-tuning language models?", "expanded_query": "What research is available on the concept of using prefix tokens as a parameter\u2011efficient method for fine\u2011tuning language models, including studies on prefix tuning, prompt tuning, LoRA, adapters, and other lightweight fine\u2011tuning techniques, across transformer architectures such as GPT\u20113, GPT\u20114, LLaMA, PaLM, BERT, T5, and other large language models, covering parameter\u2011efficient fine\u2011tuning, few\u2011shot learning, cross\u2011lingual and domain adaptation, instruction tuning, performance benchmarks, comparisons to full fine\u2011tuning, LoRA, adapters, and prompt tuning, recent papers, surveys, literature reviews, code implementations, GitHub repositories, Hugging Face, DeepSpeed, Megatron\u2011LM, AllenNLP, and related frameworks and libraries."}
{"original_query": "What research should I consult regarding the application of continuous vector prompts in language models instead of the conventional discrete token-level prompts?", "expanded_query": "What research should I consult regarding the application of continuous vector prompts (soft prompts, prompt tuning, prefix tuning, embedding prompts) in transformer\u2011based language models and large language models (GPT, BERT, T5, LLaMA) instead of conventional discrete token\u2011level prompts, including key papers, surveys, and seminal works on prompt engineering, prompt\u2011based fine\u2011tuning, zero\u2011shot and few\u2011shot learning, transfer learning, domain adaptation, instruction following, text generation, classification, question answering, summarization, translation, dialogue, code generation, multimodal prompting, evaluation, benchmarking, and studies on interpretability, bias mitigation, fairness, robustness, and privacy from conferences such as ACL, EMNLP, NeurIPS, ICLR, and arXiv?"}
{"original_query": "What resources or toolkits are available to facilitate prompt-based learning model development in PyTorch?", "expanded_query": "What resources or toolkits such as Hugging Face Transformers, PyTorch Lightning, DeepSpeed, FairScale, ZeRO, torch.compile, torch.fx, torchmetrics, torchtext, torchvision, PEFT, LoRA, prompt engineering libraries, promptflow, promptflow-pytorch, promptflow-azure, promptflow-azureml, prompt datasets, prompt templates, prompt tuning, prompt-based learning frameworks, distributed training, GPU acceleration, CUDA, model parallelism, and prompt evaluation tools are available to facilitate prompt-based learning model development in PyTorch?"}
{"original_query": "What sources offer research on maintaining factual accuracy at the entity level in abstractive summary generation?", "expanded_query": "What academic journals, conferences, datasets, evaluation metrics, and research labs publish studies on maintaining factual accuracy at the entity level in abstractive summarization, including entity-level factual consistency, hallucination detection, knowledge graph integration, transformer-based summarization models, and entity-level factuality benchmarks such as FactCC, FEVER, and SummEval, and which institutions like Stanford NLP, MIT CSAIL, Allen Institute for AI, DeepMind, OpenAI, Microsoft Research, and Google AI contribute to this research?"}
{"original_query": "What techniques and frameworks have been suggested for summarizing extensive texts under resource-constrained conditions?", "expanded_query": "What techniques and frameworks, including extractive and abstractive summarization, transformer-based models like BERT, GPT, DistilBERT, MobileBERT, TinyBERT, knowledge distillation, model pruning, quantization, low\u2011resource language adaptation, few\u2011shot learning, transfer learning, efficient architectures, beam search, greedy decoding, nucleus sampling, and evaluation metrics such as ROUGE, BLEU, BERTScore, are suggested for summarizing extensive texts under resource\u2011constrained conditions such as limited GPU, memory, inference time, low\u2011latency real\u2011time, edge devices, cloud inference, and domain adaptation?"}
{"original_query": "What techniques exist for efficiently fine-tuning transformer language models by adjusting a limited set of parameters?", "expanded_query": "What parameter\u2011efficient fine\u2011tuning techniques such as LoRA, adapter modules, prefix tuning, prompt tuning, BitFit, hypernetworks, low\u2011rank adaptation, SVD\u2011based factorization, and other efficient fine\u2011tuning strategies exist for efficiently fine\u2011tuning transformer language models like BERT, GPT, T5, and DistilBERT by adjusting a limited set of parameters, while considering learning rate schedules, optimizer choice (AdamW, SGD), gradient accumulation, mixed precision training, distributed data parallelism, model compression (pruning, quantization), knowledge distillation, and transfer learning paradigms?"}
{"original_query": "What techniques exist for incorporating context in detecting emotions within dialogues by leveraging pre-trained language models?", "expanded_query": "What transformer-based techniques, such as BERT, RoBERTa, GPT, and contextualized embedding models, exist for incorporating multi-turn dialogue context, speaker identity, turn-level context, hierarchical attention, memory networks, and contextualized emotion classification in detecting emotions within dialogues, leveraging pre-trained language models and datasets like IEMOCAP, MELD, EmotionLines, and employing fine-tuning, zero-shot learning, transfer learning, and domain adaptation methods?"}
{"original_query": "What techniques exist to enhance the few-shot fine-tuning performance in small pre-trained language models?", "expanded_query": "What parameter\u2011efficient fine\u2011tuning techniques such as adapters, LoRA, prefix tuning, prompt tuning, meta\u2011learning (MAML), data augmentation, synthetic data generation, knowledge distillation, instruction tuning, and prompt engineering exist to enhance few\u2011shot fine\u2011tuning performance in small pre\u2011trained language models like DistilBERT, TinyBERT, MobileBERT, and other lightweight models, and how do these methods compare on benchmarks such as GLUE, SuperGLUE, SQuAD, and across cross\u2011domain, multilingual, and domain adaptation scenarios?"}
{"original_query": "What techniques have been investigated to enhance multimodal sentiment analysis by fusing different modalities?", "expanded_query": "What techniques have been investigated to enhance multimodal sentiment analysis by fusing different modalities, including early, late, and hybrid fusion strategies, attention-based cross-modal integration, multimodal transformers, multimodal BERT, feature-level fusion, decision-level fusion, audio\u2011visual and text\u2011video fusion, multimodal representation learning, and multimodal deep learning approaches?"}
{"original_query": "Where can I find a database of good prompts to use for prompting language models for in-context learning?", "expanded_query": "Where can I find a database or repository of high-quality prompts, prompt libraries, prompt datasets, prompt templates, and prompt examples for prompting language models such as GPT\u20114, ChatGPT, or other LLMs, specifically for in\u2011context learning, few\u2011shot learning, zero\u2011shot learning, prompt engineering, prompt optimization, and best practices, including resources from OpenAI, Hugging Face, PromptBase, PromptHub, PromptLibrary, PromptEngineering Stack Exchange, Reddit r/PromptEngineering, GitHub, and other AI communities?"}
{"original_query": "Where can I read about the using soft embeddings to elicit knowledge from large pre-trained models, at small tuning cost?", "expanded_query": "Where can I read about using soft embeddings to elicit knowledge from large pre\u2011trained language models with minimal tuning cost, including soft prompt tuning, prompt embeddings, parameter\u2011efficient fine\u2011tuning, knowledge distillation, in\u2011context learning, and related research papers, tutorials, and blog posts from ACL, EMNLP, ICLR, NeurIPS, arXiv, and industry resources?"}
{"original_query": "Which method involves training additional prompt tokens for every layer during the fine-tuning of language models, specifically evaluating their performance on generation tasks?", "expanded_query": "Which prompt\u2011based fine\u2011tuning method, such as P\u2011Tuning v2 or P\u2011Tuning++, involves training additional prompt tokens for every transformer layer during the fine\u2011tuning of language models, and how is its performance evaluated on generation tasks such as text completion, story generation, dialogue generation, summarization, and question answering using metrics like BLEU, ROUGE, METEOR, BERTScore, and perplexity?"}
{"original_query": "Which paper has conducted a thorough analysis of how language models of different architectures generate text that either aligns with or deviates from the properties of natural human language?", "expanded_query": "Which research paper provides a comprehensive linguistic analysis of text generated by language models across different architectures, examining how their outputs align with or diverge from natural human language properties such as syntax, semantics, pragmatics, fluency, coherence, and stylistic fidelity, and comparing models like transformers, RNNs, LSTMs, GPT, BERT, XLNet, and other architectures?"}
{"original_query": "Which paper introduced the task of creating extended, coherent dialogues from brief summaries?", "expanded_query": "Which paper introduced the task of generating extended coherent dialogues from brief summaries, short summaries, or concise summaries, also referred to as dialogue expansion, dialogue synthesis, or dialogue generation from short input or short context?"}
{"original_query": "Which paper presents a platform that emphasizes evaluating the robustness of models on benchmarks?", "expanded_query": "Which research paper introduces a platform for evaluating model robustness on standard benchmarks, detailing robustness metrics, adversarial testing, generalization assessment, and providing a comprehensive evaluation framework for machine learning models?"}
{"original_query": "Which paper shows that generated captions of models are still worse than human written ones?", "expanded_query": "Which research paper demonstrates that captions generated by image captioning models are still inferior to human-written captions, comparing model-generated captions with human-written captions using evaluation metrics such as BLEU, METEOR, CIDEr, and human evaluation, and discussing the limitations of current neural network-based captioning systems like CNN\u2011RNN and transformer models?"}
{"original_query": "Which paper shows that human experts and non-experts focus on very different aspects when identifying AI=generated texts?", "expanded_query": "Which paper demonstrates that human experts and non-experts focus on different aspects when identifying AI-generated texts, exploring expert vs. non-expert detection strategies, linguistic cues, syntactic patterns, semantic coherence, cognitive biases, human annotation, expertise level, text classification, authorship attribution, language models such as GPT\u20113 and BERT, human factors, human\u2011computer interaction, experimental design, methodology, study, publication venue, authors, year, and citation?"}
{"original_query": "Which work introduces sparse attention modules and evaluate specifically on summarization?", "expanded_query": "Which research paper introduces sparse attention modules such as Sparse Transformer, Longformer, or BigBird, and evaluates them specifically on summarization tasks like document summarization and abstract summarization using datasets such as CNN/DailyMail, XSum, or Gigaword, with evaluation metrics like ROUGE-1/2/L, appearing in conferences such as ACL, EMNLP, NeurIPS, ICLR, or journals such as TACL, and authored by researchers in NLP and deep learning?"}
{"original_query": "Which work pushes the limit of model quantization in BERT models by introducing a ternary network?", "expanded_query": "Which research paper demonstrates the state\u2011of\u2011the\u2011art limit of model quantization in BERT models by introducing a ternary network, exploring ternary weight and activation quantization, quantization\u2011aware training, and achieving significant model size reduction and inference speedup while maintaining accuracy on GLUE and SQuAD benchmarks?"}
{"original_query": "Which work shows that only emplying instance-level uncertainty metrics could results in sample redundancy in active learning?", "expanded_query": "Which research paper or study demonstrates that relying solely on instance\u2011level uncertainty metrics in active learning leads to sample redundancy, and what are the key findings, methodologies, and implications for query strategy design, redundancy mitigation, and data acquisition efficiency?"}
{"original_query": "Which work shows that reducing the number of training epochs effectively limits the impact of backdoor attack, but the method decreases the prediction accuracy?", "expanded_query": "Which research paper or study demonstrates that reducing the number of training epochs in deep neural network training effectively limits the impact of backdoor attacks while decreasing prediction accuracy, and what are the key findings, datasets (e.g., CIFAR\u201110, ImageNet, MNIST), evaluation metrics (clean accuracy, attack success rate), defense mechanisms, trade\u2011offs, authors, publication venue, and year discussed in this work?"}
{"original_query": "Which work suggests that machine translation models might get too confident and generate coherent but inadequant translations?", "expanded_query": "Which research paper or study discusses how neural machine translation models can become overconfident, producing coherent yet inadequate translations, and explores confidence calibration, hallucination, translation quality estimation, BLEU scores, human evaluation, error analysis, and mitigation strategies in transformer-based NMT systems?"}
{"original_query": "Which works shows that training large language models with purely mathematical and structural data can exhibit emergence of causal reasoning faster?", "expanded_query": "Which research papers, studies, or works demonstrate that training large language models with purely mathematical and structural data leads to faster emergence of causal reasoning abilities, including symbolic reasoning, causal graph learning, structural equation modeling, causal inference benchmarks, emergent causal reasoning, and which authors, institutions, conferences (NeurIPS, ICLR, ACL, EMNLP, ICML, AAAI, arXiv) and organizations (OpenAI, DeepMind, Google Brain, Microsoft Research, Stanford, MIT, Harvard) have reported such findings?"}
{"original_query": "ould you direct me to research that shows that the transfer of specialized knowledge between various ABSA tasks if trained under the same paradigm?", "expanded_query": "Could you direct me to research papers, case studies, and benchmark datasets that demonstrate transfer learning, cross\u2011task knowledge transfer, domain adaptation, or multitask learning for aspect\u2011based sentiment analysis (ABSA) tasks trained under the same paradigm, including studies on BERT, RoBERTa, XLNet, ALBERT, GPT\u20113, and Hugging Face Transformers, with evaluation metrics such as accuracy, F1, BLEU, ROUGE, on datasets like Stanford Sentiment Treebank, SemEval 2014\u20112020 Task\u202f5, Twitter, Yelp, Amazon reviews, and authors such as Zhang, Liu, Wang, Li, Chen?"}
{"original_query": "Are there any examples of using dense phrase retrieval systems in the automatic curation of entity dictionaries?", "expanded_query": "examples of using dense phrase retrieval systems for automatic curation of entity dictionaries, entity dictionary construction, entity extraction, entity linking, entity normalization, knowledge base population, semantic indexing, embedding-based retrieval, dense retrieval, FAISS, sentence transformers, BERT embeddings, OpenAI embeddings, phrase embeddings, entity resolution, entity disambiguation, knowledge graph enrichment, entity dictionary enrichment, case studies, academic papers, industry use, research examples, automatic entity dictionary curation using dense embeddings, dense retrieval for entity dictionary curation"}
{"original_query": "Are there any large-scale and open-source text simplification datasets dealing with long passages?", "expanded_query": "What large-scale open-source text simplification datasets are available for long passages, including paragraph-level or document-level simplification corpora such as WikiLarge, Newsela, Simple Wikipedia, and other open-source resources with long passage simplification pairs annotated for lexical and syntactic simplification, available under open licenses like CC-BY, MIT, or Apache?"}
{"original_query": "Are there any papers that build dense retrievers with mixture-of-experts architecture where each expert is responsible for different types of queries?", "expanded_query": "Find research papers on dense retrievers using mixture-of-experts architecture where each expert specializes in different query types, covering neural retrieval models, transformer-based bi-encoders, cross-encoders, semantic search, approximate nearest neighbor methods, retrieval augmentation, retrieval-augmented generation, expert specialization, query classification, retrieval pipeline design, evaluation metrics like recall and precision, recent works from ACL, EMNLP, NeurIPS, ICLR, arXiv, 2023-2024, including citations and state-of-the-art methods"}
{"original_query": "Could you recommend a dataset paper which presents relation extraction performance on translated data and compare it to English data?", "expanded_query": "Recommend a dataset paper that presents cross-lingual relation extraction performance on translated data versus English data, including precision, recall, F1, BLEU, using multilingual models such as mBERT, XLM\u2011R, GPT\u20114, comparing to English baseline on datasets like TACRED, ACE, WikiTables, discussing statistical significance, domain adaptation, translation quality, low\u2011resource language settings, data augmentation, annotation guidelines, and evaluation protocols."}
{"original_query": "Find the NLP paper that focuses on dialogue generation and introduces advancements in the augmentation of one-to-many or one-to-one dialogue data by conducting augmentation within the semantic space.", "expanded_query": "Find the NLP research paper that focuses on dialogue generation, introduces advancements in augmenting one-to-many or one-to-one dialogue data by performing augmentation within the semantic space, discusses semantic embeddings, contextualized representations, and semantic similarity, and is published in a top conference such as ACL, EMNLP, NeurIPS, or ICLR, or available on arXiv, detailing transformer-based methods for dialogue modeling and response generation."}
{"original_query": "Give me a paper proposing to circumvent a single-truth target in training generative language models.", "expanded_query": "Search for academic papers proposing methods to circumvent the single-truth target in training generative language models, focusing on multi-factuality, truthfulness, hallucination mitigation, knowledge distillation, dataset curation, adversarial training, and evaluation benchmarks such as TruthfulQA, OpenAI GPT-4, GPT-3, BERT, transformer models, and natural language processing research on factual consistency and model robustness."}
{"original_query": "How to achieve zero-shot lip reading?", "expanded_query": "How to achieve zero-shot lip reading using deep learning, computer vision, multimodal learning, cross-modal transfer, audio-visual fusion, Transformer, CNN, RNN, CTC, attention mechanisms, LipNet, Wav2Lip, Lip Reading in the Wild dataset, GRID dataset, zero-shot learning, few-shot learning, domain adaptation, speech-to-text from silent video, visual speech recognition, speech synthesis, lip sync, and related techniques?"}
{"original_query": "How to better attract readers to news articles by generating personalized headlines?", "expanded_query": "How can AI-driven personalized headline generation improve reader engagement, click-through rates, and time on page for news articles by leveraging demographic and psychographic segmentation, A/B testing, SEO optimization, social media amplification, content recommendation engines such as OpenAI GPT\u20114, IBM Watson, Amazon Personalize, Adobe Target, and best practices for headline relevance and emotional triggers?"}
{"original_query": "How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference?", "expanded_query": "How to faithfully and explicitly measure the helpfulness of human explanations to language models during finetuning and inference, using evaluation metrics such as explanation quality, fidelity, comprehensibility, relevance, usefulness, coverage, consistency, alignment, trust, transparency, interpretability, and explainable AI principles, through human\u2011in\u2011the\u2011loop assessments, automatic metrics, and impact studies on model performance, accuracy, robustness, calibration, bias, fairness, explainability, interpretability, reliability, safety, efficiency, latency, resource usage, training and inference time, and downstream task outcomes, while considering transformer architectures like GPT, BERT, RoBERTa, T5, and XLNet, and employing methods such as explanation\u2011based finetuning, explanation\u2011based inference, and scoring frameworks, tools, and algorithms for measuring explanation effectiveness."}
{"original_query": "If one would like to train (or evaluate) a helpful assistant agent that can converse with humans while the humans traverse an environment, which work has the most suitable resource?", "expanded_query": "Which research papers, datasets, or frameworks (e.g., OpenAI Gym, ROS, Gazebo, Unity ML\u2011Agents, Unreal Engine, Simulink, OpenAI\u2019s ChatGPT, Whisper, GPT\u20114, RLHF, DeepMind, Microsoft Research, MIT CSAIL, Stanford AI Lab, CMU Robotics Institute, Oxford, Google Research, NVIDIA, HRI conference, NeurIPS, ICLR, ACL, EMNLP, ICRA, CVPR, SIGCHI, AAAI, IJCAI, arXiv, GitHub) provide the most suitable resources for training or evaluating a helpful assistant agent that can converse with humans while the humans traverse an environment, covering reinforcement learning, human\u2011robot interaction, task\u2011oriented dialogue, contextual grounding, semantic mapping, trajectory planning, speech recognition, synthesis, natural language understanding, multi\u2011modal learning, policy evaluation, benchmark datasets, interactive simulation, virtual or augmented reality, human\u2011in\u2011the\u2011loop, user studies, and human\u2011centered AI?"}
{"original_query": "In multimodal (multilingual) abstractive summarization field, is there any paper that propose target-oriented vision modeling method to improve the quality of summaries?", "expanded_query": "multimodal summarization multilingual abstractive summarization target-oriented vision modeling method improve summary quality vision-language models transformer cross-modal attention visual grounding image embeddings scene graph object detection CLIP ViLT mBART mT5 MS-COCO Multi30k XSum CNN/DailyMail evaluation metrics ROUGE BERTScore human evaluation cross-lingual vision-language pretraining literature"}
{"original_query": "Is there a Chinese hate speech paper that constructs an insulting lexicon while building the dataset?", "expanded_query": "Is there a Chinese hate speech research paper that constructs an insulting lexicon while building a dataset, detailing lexicon development methods, dataset annotation guidelines, hate speech taxonomy, slurs, discriminatory language, NLP techniques, and an annotated corpus for Chinese hate speech detection in the Chinese NLP community?"}
{"original_query": "Is there a dataset that allows to perform aspect-based sentiment classification on French news?", "expanded_query": "aspect-based sentiment analysis dataset French news articles aspect sentiment classification French news corpus labeled aspect categories sentiment polarity news domain French news dataset aspect-based sentiment classification French news annotated corpus aspect extraction sentiment lexicon French news sentiment dataset public benchmark"}
{"original_query": "Is there a decoder-only language model that does not use a tokenizer and operates on raw text bytes?", "expanded_query": "Is there a decoder\u2011only transformer language model that operates on raw text bytes without using any tokenizer, such as a byte\u2011level GPT, byte\u2011level BPE, or character\u2011level transformer, and what existing implementations or research from OpenAI (GPT\u20113, GPT\u20113.5, GPT\u20114), EleutherAI (GPT\u2011Neo, GPT\u2011J, GPT\u2011NeoX), Google (ByteNet), or other institutions demonstrate tokenization\u2011free raw byte language modeling in a decoder\u2011only architecture?"}
{"original_query": "Is there a dialogue dataset where a speaker's utterance is grounded in their persona, consisting of image-text pairs representing their episodic memories?", "expanded_query": "Is there a multimodal dialogue dataset that grounds a speaker's utterances in their persona using image-text pairs to represent episodic memories, supporting personalized conversation, memory retrieval, visual storytelling, image captioning, visual dialogue, and persona-based dialogue generation for AI systems?"}
{"original_query": "Is there a method for measuring the critical errors that a dialogue system makes in its responses?", "expanded_query": "Is there a method for measuring the critical errors that a dialogue system makes in its responses, including error taxonomy such as hallucination, misinterpretation, irrelevant, incoherent, repetition, and using evaluation metrics like precision, recall, F1, BLEU, ROUGE, METEOR, BERTScore, GPT\u20114 evaluation, human evaluation, automatic metrics, error annotation, error impact analysis, dialogue coherence, semantic accuracy, contextual relevance, user satisfaction, task success rate, and evaluation frameworks for dialogue system performance assessment?"}
{"original_query": "Is there a method that measures the information provided in a (model generated) rationale beyond what the original context provided?", "expanded_query": "What information\u2011theoretic or semantic metrics can quantify the additional information or novelty in a model\u2011generated rationale beyond the original context, assessing explanation completeness, fidelity, relevance, usefulness, and coverage within explainable AI, natural language processing, and machine learning interpretability?"}
{"original_query": "Is there a paper comparing knowledge distillation and human annotation in terms of cost efficiency?", "expanded_query": "Is there a research paper or systematic review comparing the cost efficiency of knowledge distillation versus human annotation in machine learning, covering annotation cost per label, labeling time, annotation accuracy, human labor costs, annotation pipelines, teacher\u2011student model compression, distillation loss, model performance, cost\u2011benefit analysis, scalability, cost per accuracy, cost efficiency metrics across NLP, computer vision, and speech recognition domains?"}
{"original_query": "Is there a paper exploring the curse of multilinguality for similar languages?", "expanded_query": "search for academic papers on the curse of multilinguality in multilingual NLP, focusing on similar languages, language similarity metrics, cross\u2011lingual transfer, multilingual BERT, transformer models, language families, resource\u2011rich vs resource\u2011poor, multilingual pretraining, language similarity, multilingual representation learning, multilingual language modeling, cross\u2011lingual transfer learning, similar language pairs, linguistic similarity, multilingual model interference, language similarity, multilingualism, multilingual NLP research"}
{"original_query": "Is there a paper that applies large language models to visual Raven\u2019s Progressive Matrices?", "expanded_query": "Is there a research paper or study that applies large language models such as GPT\u20114, Claude, or other transformer\u2011based LLMs to visual Raven\u2019s Progressive Matrices, evaluating performance on visual reasoning, pattern recognition, and cognitive assessment benchmarks, possibly published in conferences like NeurIPS, ICML, CVPR, ACL, or on arXiv?"}
{"original_query": "Is there a paper that connects the basic elements of storytelling with biased or imbalanced media reporting?", "expanded_query": "Is there a paper that connects the basic elements of storytelling such as protagonist, conflict, resolution, narrative arc, plot devices, with biased or imbalanced media reporting, media framing, agenda setting, selective coverage, confirmation bias, cognitive biases, media bias detection, narrative journalism, media representation, media influence, public perception, media effects, media framing theory, media coverage imbalance, and related academic literature?"}
{"original_query": "Is there a paper that links exposure bias to distillation?", "expanded_query": "Is there an academic paper that investigates the relationship between exposure bias in sequence-to-sequence models and knowledge distillation techniques, including teacher-student training, scheduled sampling, teacher forcing, beam search, reinforcement learning, and bias mitigation strategies in neural machine translation and language model compression?"}
{"original_query": "Is there a paper that shows that language models' error distribution is different for unfamiliar entities that is not apparent when models are evaluated on familiar entities alone?", "expanded_query": "Is there a research paper or empirical study that demonstrates a statistically significant difference in the error distribution of neural language models when evaluating unfamiliar entities compared to familiar entities, highlighting a familiarity bias that is not evident when models are tested solely on familiar entity datasets, and discussing implications for entity recognition, disambiguation, knowledge graph integration, robustness to out-of-distribution entities, error analysis, evaluation metrics, dataset coverage, and model interpretability?"}
{"original_query": "Is there a paper that supports the use of automated coherence metrics in topic model evaluations?", "expanded_query": "Is there a paper that supports the use of automated coherence metrics such as NPMI, UMass, C_V, or C_UCI in evaluating topic models like LDA, HDP, or neural topic models, and provides empirical evidence or validation on standard corpora such as 20 Newsgroups, Reuters, or Wikipedia, within the context of NLP and machine learning research literature?"}
{"original_query": "Is there a paper that uses Explainable AI techniques to investigate how language models represent the expression of morality?", "expanded_query": "Search for academic papers that apply explainable AI (XAI) methods such as SHAP, LIME, attention visualization, concept activation vectors, counterfactual explanations, or concept bottleneck models to analyze how large language models (LLMs) like GPT\u20114, BERT, RoBERTa, T5, or transformer architectures encode, represent, and express moral language, ethical reasoning, value judgments, and normative frameworks (utilitarianism, deontology, virtue ethics) in text, including studies on bias, fairness, alignment, and AI safety, published in venues such as ACL, EMNLP, NeurIPS, ICLR, ICML, AAAI, IJCAI, or on arXiv and Google Scholar."}
{"original_query": "Is there a paper that uses an app for a popular tabletop game to gather real transcripts of gameplay with concrete values for players' and monsters' health?", "expanded_query": "Is there a research paper, journal article, or conference proceeding that uses a digital companion app for a popular tabletop role\u2011playing game such as Dungeons & Dragons 5e to collect real\u2011time transcripts of gameplay sessions, including concrete health point (HP) values for players and monsters, and analyzes these data for game mechanics, player performance metrics, monster performance metrics, or game analytics, employing data mining on game session logs, player and monster statistics, and health values?"}
{"original_query": "Is there a paper that uses evolutionary algorithms and neural MT metrics to produce translations?", "expanded_query": "Is there a research paper that applies evolutionary algorithms or genetic algorithms to optimize neural machine translation models using neural MT evaluation metrics such as BERTScore, BLEU, METEOR, TER, or COMET, incorporating population\u2011based optimization, fitness functions, and multi\u2011objective search over Transformer sequence\u2011to\u2011sequence architectures for translation generation and quality estimation across language pairs like English\u2011French, Spanish\u2011English, or Chinese\u2011English, and that was published in major NLP conferences such as ACL, EMNLP, NAACL, or journals such as TACL, JMLR, or IEEE Transactions on Affective Computing?"}
{"original_query": "Is there a paper that uses similarity scores to check knowledge in diffusion models", "expanded_query": "Search for research papers on similarity scores for knowledge checking in diffusion models, including latent diffusion models, text-to-image diffusion, semantic similarity, cosine similarity, embedding similarity, knowledge distillation, knowledge probing, model evaluation, interpretability, and similarity-based evaluation metrics for diffusion-based generative models."}
{"original_query": "Is there a paper that uses the tree structure of math equations in autoregressive language models?", "expanded_query": "Is there a paper that uses the tree structure of math equations in autoregressive language models, such as transformer-based models with tree\u2011structured attention or tree\u2011based neural networks for equation parsing and generation, including tree\u2011structured LSTMs, graph neural networks, or tree\u2011to\u2011sequence architectures, and published in venues like NeurIPS, ACL, EMNLP, ICLR, ICML, or on arXiv?"}
{"original_query": "Is there a paper that utilizes the characteristics of human evolutionary knowledge to guide language models in generating scientific ideas?", "expanded_query": "Is there a research paper that applies human evolutionary cognition, evolutionary psychology, and evolutionary traits to guide fine-tuning of transformer language models like GPT\u20114 or BERT for scientific hypothesis generation and idea discovery, integrating cognitive biases, knowledge representation, semantic networks, knowledge graphs, evolutionary algorithms, and cross\u2011disciplinary AI\u2011driven research to enhance scientific creativity and alignment?"}
{"original_query": "Is there a study that shows how to help the demonstration retriever better integrate feedback from LLMs?", "expanded_query": "Is there a study or research paper that demonstrates how to improve a demonstration retriever's ability to integrate feedback from large language models, including techniques such as reinforcement learning, human\u2011in\u2011the\u2011loop feedback, fine\u2011tuning, retrieval\u2011augmented generation, semantic search, retrieval ranking, retrieval refinement, feedback signals, LLM prompting, knowledge base integration, and evaluation metrics for retrieval performance and system architecture?"}
{"original_query": "Is there a tool that can automatically segment speech and the corresponding text transcriptions, to obtain a finer grained alignment?", "expanded_query": "Is there a tool or software that can automatically segment speech audio and its corresponding text transcriptions to obtain a finer\u2011grained alignment, such as forced alignment, word\u2011level timestamps, speaker diarization, subtitle generation, or audio\u2011to\u2011text alignment for podcasts, videos, or lectures?"}
{"original_query": "Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method?", "expanded_query": "Is there an evaluation metric for natural language generation that predicts the factual consistency score through a mean-max aggregation method, leveraging metrics such as BLEU, ROUGE, BERTScore, FactCC, FactScore, semantic similarity, knowledge graph alignment, entity linking, and knowledge base verification, applicable to GPT\u20113, GPT\u20114, and other large language models, within an automatic evaluation framework that compares against human evaluation and benchmark datasets for factual consistency in NLG?"}
{"original_query": "Is there any dataset that contains minimally-contrasting social situations that lead to different decisions about which behaviors are appropriate in that situation?", "expanded_query": "Are there any datasets that include minimally contrasting social situations annotated with decisions about appropriate behaviors, covering social norms, contextual decision\u2011making, cultural differences, behavioral guidelines, and social interaction cues for research in social psychology, AI behavior modeling, and human\u2011computer interaction?"}
{"original_query": "Is there any paper about style transfer for stories?", "expanded_query": "Is there any recent research paper or survey on neural text style transfer for stories, including short stories, novels, children\u2019s books, and genre-specific fiction, covering methods such as transformer-based models, encoder\u2011decoder architectures, adversarial training, style embeddings, fine\u2011tuning GPT or BERT, datasets like Gutenberg and literary corpora, and evaluation metrics for fluency, coherence, semantic preservation, genre transfer, authorial style, narrative voice, plot structure, and applications in story rewriting, adaptation, and generation?"}
{"original_query": "Is there any paper exploring real speakers and thus performing multimodal emotion recognition task?", "expanded_query": "real speakers multimodal emotion recognition audio visual speech facial expressions body language deep learning CNN RNN LSTM Transformer multimodal fusion real-world data speaker variability emotion recognition speech emotion recognition facial emotion recognition multimodal datasets open-source datasets benchmark performance metrics accuracy F1-score cross-modal audio-visual speech emotion recognition real-time emotion detection speaker-independent speaker-dependent dataset RECOLA IEMOCAP SEMAINE EmoReact audio-visual emotion recognition multimodal deep learning feature extraction spectrogram facial landmarks action units emotion categories valence arousal emotion recognition research real speaker studies multimodal emotion recognition literature"}
{"original_query": "Is there any paper leverages knowledge distillation of language models for textual out-of-distribution detection or anomaly detection?", "expanded_query": "Is there any recent paper that leverages knowledge distillation of language models such as BERT, RoBERTa, GPT, DistilBERT, DistilRoBERTa, or DistilGPT for textual out-of-distribution detection, novelty detection, or anomaly detection in NLP, using teacher\u2011student frameworks, distillation loss, KL divergence, temperature scaling, soft labels, hard labels, data augmentation, semi\u2011supervised learning, self\u2011supervised learning, contrastive learning, domain adaptation, domain generalization, distribution shift detection, robustness to OOD, adversarial examples, semantic drift, and text classification tasks?"}
{"original_query": "Is there any paper that address attacks on code models by leveraging the semantic information of the source code through attention scores, while also guaranteeing that the generated adversarial examples can always be compiled successfully?", "expanded_query": "Is there any paper that addresses attacks on neural code models such as CodeBERT, CodeT5, or transformer-based code models by leveraging the semantic information of the source code through attention scores or AST-based attention, while also guaranteeing that the generated adversarial examples can always be compiled successfully by compilers like GCC or LLVM, ensuring syntactic correctness, semantic preservation, compile-time validation, and preventing compile-time errors?"}
{"original_query": "Is there any paper that aligns speech and text embeddings better than CTC training?", "expanded_query": "Is there any recent research paper that proposes a method for aligning speech and text embeddings more effectively than conventional CTC training, such as contrastive learning, self\u2011supervised multimodal transformers, wav2vec 2.0, HuBERT, CLIP\u2011style audio\u2011text contrastive models, or other cross\u2011modal alignment techniques?"}
{"original_query": "Is there any paper that applies curriculum learning to various NLG tasks without depending on specific metrics?", "expanded_query": "Search for research papers applying curriculum learning to multiple natural language generation tasks such as machine translation, summarization, dialogue, and language modeling, without relying on specific evaluation metrics like BLEU, ROUGE, METEOR, or human evaluation, focusing on metric-free or task-agnostic curricula, progressive training, self-paced learning, and generalized learning objectives, including studies from ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, arXiv, and recent surveys."}
{"original_query": "Is there any paper that applies symbolic distillation on black-box generalist language models to harvest high-quality counterfactual data for out-of-distribution generalization?", "expanded_query": "Looking for papers that apply symbolic distillation or rule-based knowledge extraction from black-box generalist language models such as GPT\u20114, PaLM, or Llama to harvest high\u2011quality counterfactual data for improving out\u2011of\u2011distribution generalization and robustness in NLP tasks."}
{"original_query": "Is there any paper that attempts to evaluate the similarity of meaning representations without using annotated data?", "expanded_query": "Is there any paper that attempts to evaluate the similarity of meaning representations without using annotated data, focusing on unsupervised semantic similarity evaluation of word, sentence, or contextualized embeddings such as GloVe, fastText, BERT, or RoBERTa, using intrinsic metrics like cosine similarity, Euclidean distance, or extrinsic tasks, and exploring benchmark datasets, evaluation frameworks, and methods for meaning representation evaluation without labeled data?"}
{"original_query": "Is there any paper that automatically creates a dataset for summarizing text from one language to another for a large collection of languages?", "expanded_query": "Looking for papers on automatic creation of large-scale multilingual summarization datasets for cross-lingual summarization across many language pairs, including methods for dataset generation, multilingual summarization benchmarks, zero-shot summarization, multilingual summarization evaluation, use of parallel corpora such as Europarl, TED talks, WikiHow, XSum, Multi-News, and approaches to cross-lingual transfer learning, data augmentation, and automatic dataset pipelines, covering language coverage, large collection of languages, Transformer models, BERT-based multilingual summarization, OpenAI GPT\u20114 summarization, and multilingual summarization research."}
{"original_query": "Is there any paper that combines causal inference and finetuning for language models?", "expanded_query": "papers combining causal inference and fine-tuning for language models, causal language modeling, counterfactual reasoning in NLP, causal effect estimation in transformer fine-tuning, causal representation learning for BERT and GPT, domain adaptation with causal inference, causal discovery in language model training, causal graphs for prompt tuning, causal inference for bias mitigation in language models, causal inference for robustness and generalization in fine-tuned models, causal inference methods applied to language model fine-tuning, causal inference for zero-shot learning, causal inference for domain shift, causal inference for data selection and augmentation in language models, causal inference for transfer learning in NLP, causal inference for fine-tuning hyperparameters, causal inference for language model evaluation, causal inference for interpretability of fine-tuned language models, causal inference for language model performance optimization, causal inference for language model architecture design, causal inference for language model hyperparameter tuning, causal inference for language model data curation, causal inference for language model domain adaptation, causal inference for language model domain generalization, causal inference for language model domain robustness, causal inference for language model domain fairness, causal inference for language model domain bias mitigation, causal inference for language model domain evaluation, causal inference for language model domain performance, causal inference for language model domain optimization, causal inference for language model domain architecture, causal inference for language model domain hyperparameters, causal inference for language model domain data selection, causal inference for language model domain data augmentation, causal inference for language model domain data curation"}
{"original_query": "Is there any paper that constructs augmented training data based on the entity-to-entity correlations?", "expanded_query": "Is there any paper that constructs augmented training data based on entity-to-entity correlations, entity embeddings, knowledge graph relationships, co-occurrence statistics, semantic similarity, graph neural network augmentation, synthetic data generation, entity linking, relation extraction, entity classification, entity disambiguation, entity matching, entity alignment, entity resolution, or knowledge graph completion?"}
{"original_query": "Is there any paper that employs code LLMs to iteratively generate and refine code with execution results to improve the performance?", "expanded_query": "Is there any research paper that investigates the use of code-focused large language models (LLMs) for iterative code generation and refinement, leveraging execution results and runtime profiling to improve software performance, optimize latency and throughput, reduce resource consumption, and enhance code quality through automated debugging, test-driven development, and reinforcement learning-based feedback loops, including studies on AlphaCode, Codex, GitHub Copilot, and other LLM-based code assistants, with benchmarks on microbenchmarks, macrobenchmarks, and real-world datasets, and analysis of performance metrics such as cyclomatic complexity, runtime overhead, and energy consumption?"}
{"original_query": "Is there any paper that explores and annotates the effectiveness of using testimonials or anecdotes in discussions?", "expanded_query": "Is there any peer\u2011reviewed paper or empirical study that explores and annotates the effectiveness of using testimonials or anecdotes in discussions, examining social influence, persuasion, argument quality, source credibility, cognitive biases (availability heuristic, confirmation bias), narrative persuasion, and audience reception in online forums, debate contexts, and public discourse, with references to communication theories such as the Elaboration Likelihood Model, Social Influence Theory, and Narrative Persuasion, and including meta\u2011analyses, qualitative and quantitative research, and citations from journals in psychology, communication studies, media studies, and social science?"}
{"original_query": "Is there any paper that explores using only an encoder-only masked language model for open-ended long text generation (such as story generation)?", "expanded_query": "encoder-only masked language model long text generation story generation research papers transformer BERT RoBERTa ELECTRA T5 Longformer BigBird XLNet non-autoregressive generative modeling open-ended text generation narrative generation plot generation character development story coherence evaluation metrics BLEU ROUGE perplexity human evaluation datasets NarrativeQA Story Cloze Test ROCStories WritingPrompts literature review survey state-of-the-art limitations future work"}
{"original_query": "Is there any paper that investigates backdoor attacks across various types of tasks, not limited to classification, in language models?", "expanded_query": "Search for research papers or surveys on backdoor attacks in language models that cover a range of NLP tasks beyond classification, such as question answering, summarization, translation, dialogue, text generation, and sentiment analysis, including studies on transformer-based models like BERT, GPT, RoBERTa, T5, and multilingual models, focusing on task-agnostic and task-dependent backdoors, trigger design (semantic, syntactic, prompt-based), data poisoning, model poisoning, transferability across tasks, defense mechanisms, adversarial training, detection methods, benchmark datasets, and conference venues like ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, as well as industry research from OpenAI, Google AI, Microsoft Research, and academic institutions."}
{"original_query": "Is there any paper that leverages graph neural network by integrating label information for multi-label low-resource intent classification?", "expanded_query": "graph neural network label integration multi-label low-resource intent classification few-shot learning semi-supervised graph convolutional network graph attention network label propagation intent detection natural language processing low-resource NLP knowledge graph ontology label hierarchy multi-task learning data augmentation"}
{"original_query": "Is there any paper that leverages syntactic rules to explicitly guide text generation?", "expanded_query": "Are there any research papers or studies that discuss leveraging syntactic rules, syntactic constraints, syntactic scaffolding, grammar-based generation, syntactic templates, syntactic dependency parsing, syntactic features, syntactic structure, syntactic regularities, syntactic patterns, or syntactic rule-based generation methods to explicitly guide text generation in NLP, transformer models, BERT, GPT, T5, or other language modeling frameworks, including syntactic rule guided generation literature, research, examples, frameworks, and approaches?"}
{"original_query": "Is there any paper that performs adversarial training on frame level for audio-visual representation learning?", "expanded_query": "Is there any recent research paper or conference article that applies adversarial training at the frame level for audio\u2011visual representation learning, particularly focusing on multimodal fusion, temporal alignment, cross\u2011modal consistency, and evaluation on standard benchmarks such as AVSpeech, Kinetics\u2011400, or AVA, and that is published in venues like CVPR, ICCV, NeurIPS, ICLR, or IEEE Transactions on Pattern Analysis and Machine Intelligence?"}
{"original_query": "Is there any paper that proposes a new multimodal video dataset that image-level multimodal models do not work well?", "expanded_query": "Is there any recent research paper that proposes a new multimodal video-language dataset designed to challenge image-level multimodal models, featuring extensive audio-visual, temporal, and cross-modal annotations for tasks such as video captioning, action recognition, video question answering, and cross-modal retrieval, with publicly available data, baseline results, comparative analysis against CLIP, ViLT, and other image-text models, published in venues such as CVPR, ICCV, NeurIPS, ICLR, ACL, or arXiv, and including dataset licensing, documentation, download links, and discussion of dataset size, diversity, temporal dynamics, multimodal grounding, and reasoning challenges?"}
{"original_query": "Is there any paper that proposes a set of criteria to comprehensively evaluate generated conversations?", "expanded_query": "Is there any paper that proposes a comprehensive set of criteria to evaluate generated conversations, including metrics for fluency, coherence, relevance, engagement, diversity, consistency, safety, bias, fairness, and human-centered evaluation, that references evaluation frameworks from OpenAI, Google Meena, Microsoft DialoGPT, Facebook AI, DeepMind, and discusses benchmarks, datasets, guidelines, and methods presented in ACL, EMNLP, NAACL, ICLR, NeurIPS, AAAI, and IJCAI conferences?"}
{"original_query": "Is there any paper that reveals annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization?", "expanded_query": "Search for papers discussing annotation problems in cross-lingual summarization caused by decomposing the task into translation and summarization, covering issues such as annotation noise, guideline inconsistencies, semantic fidelity errors, content selection bias, fluency and coherence problems, evaluation metric impacts, dataset construction challenges, and comparisons between pipeline and end-to-end approaches in multilingual summarization research, citing works from ACL, EMNLP, NAACL, COLING, LREC, arXiv, and Google Scholar."}
{"original_query": "Is there any paper that studies a teacher AI inferring mental states of a student role in a role-playing game setup using reinforcement learning?", "expanded_query": "Is there any research paper that investigates a teacher AI system using reinforcement learning to infer the mental states of a student role in a role\u2011playing game setup, incorporating adaptive learning, affective computing, student modeling, policy gradient methods, deep Q\u2011networks, learning analytics, student engagement detection, cognitive load estimation, personalized instruction, AI tutor, human\u2011computer interaction, and game\u2011based learning within simulation\u2011based education contexts?"}
{"original_query": "Is there any paper that tries to investigate LLMs\u2019 capabilities in solving elliptical constructions by using a test-dataset based on the psycolinguistic notion of Thematic Fit?", "expanded_query": "Are there any research papers or studies that investigate large language models\u2019 ability to resolve syntactic ellipsis in elliptical constructions using a psycholinguistically motivated test dataset based on the concept of thematic fit, evaluating semantic role labeling, syntactic parsing accuracy, and cognitive plausibility, and comparing transformer\u2011based models such as GPT\u20114, BERT, RoBERTa, XLNet, and other neural language models on benchmark datasets for ellipsis resolution and thematic fit theory?"}
{"original_query": "Is there any paper that uses data collected from the Dark Web, specifically onion domains, to pretrain a language model?", "expanded_query": "Search for academic papers, conference proceedings, or journal articles that describe using data collected from the Dark Web, specifically onion domains on the Tor network, to pretrain a language model such as BERT, GPT, or other transformer-based neural language models, including details on data collection methods, web scraping, crawling, corpus creation, ethical considerations, privacy, and the resulting pretraining corpus, with references to venues like ACL, EMNLP, NeurIPS, ICLR, ICML, arXiv, and cybersecurity or NLP research on darknet text mining."}
{"original_query": "Is there any paper that uses prompt tuning in multi-answer QA?", "expanded_query": "prompt tuning multi-answer question answering multi-label QA prompt engineering few-shot learning language models transformers BERT GPT OpenAI research papers academic literature survey benchmark datasets SQuAD HotpotQA Natural Questions OpenBookQA prompt tuning methods parameter-efficient fine-tuning adapters LoRA prefix tuning prompt tuning for multi-answer QA prompt tuning multi-label classification prompt selection prompt design prompt optimization"}
{"original_query": "Is there any paper that uses token-level loss to enhance sentence-level embedding learning?", "expanded_query": "Search for academic papers on transformer-based models such as BERT or RoBERTa that employ token-level loss functions or token-level supervision to improve sentence-level embedding learning for tasks like semantic similarity, sentence classification, retrieval, and clustering, using contrastive or self-supervised learning techniques."}
{"original_query": "Is there any paper that utilizes Gaussian processes to analyze the vulnerability of text-conditioned generative models?", "expanded_query": "Are there any research papers that apply Gaussian process regression or classification to evaluate the vulnerability, robustness, or adversarial susceptibility of text\u2011conditioned generative models such as text\u2011to\u2011image diffusion models, text\u2011to\u2011video models, or transformer\u2011based generative architectures, including analysis of prompt injection attacks, data poisoning, and uncertainty estimation using Gaussian process kernels?"}
{"original_query": "Is there any paper that utilizes graph structure to model conversation history?", "expanded_query": "Graph-based conversation history modeling paper using graph structure, graph neural network, dialogue context graph, multi-turn conversation, graph embeddings, knowledge graph, semantic graph, graph convolutional network, graph attention network, dialogue modeling, conversation context representation, graph-based dialogue modeling, graph-based conversation analysis, graph-based conversation modeling literature review, graph-based conversation history representation, graph-based conversation modeling research papers"}
{"original_query": "Is there any paper that utilizes masked language modeling to defend against word-level adversarial attacks?", "expanded_query": "Is there any research paper that employs masked language modeling, such as BERT, RoBERTa, XLNet, or ALBERT, to defend against word-level adversarial attacks\u2014including word substitution, synonym replacement, and semantic perturbations\u2014using adversarial training, detection, or mitigation strategies, evaluated on standard NLP benchmarks like GLUE, SQuAD, or adversarial datasets, and discusses robustness, evaluation metrics, defense mechanisms, and transformer-based model resilience?"}
{"original_query": "Is there any research paper that can extract attributes from both a predefined label set and the surrounding context?", "expanded_query": "Search for research papers on attribute extraction from both a predefined label set and surrounding context, covering NLP techniques, transformer-based models such as BERT and GPT, contextual embeddings, semantic role labeling, entity recognition, knowledge base population, and applications in product descriptions, scientific literature, social media, product catalogs, and knowledge graphs, including attribute extraction methods, algorithms, frameworks, datasets, and evaluation studies."}
{"original_query": "Is there any work that allows large numbers of model outputs to be encoded and compared by causal language models in a single forward pass?", "expanded_query": "Is there any research or methodology that enables encoding and comparing large numbers of causal language model outputs\u2014such as from GPT\u20114, LLaMA, Claude, or PaLM\u2014into vector embeddings using a single forward pass, leveraging techniques like batch encoding, latent space similarity, cosine or dot\u2011product metrics, contrastive learning, or efficient inference frameworks for output similarity evaluation?"}
{"original_query": "Is there any work that attacks language models in dialogue generation?", "expanded_query": "What research papers, conference proceedings, and arXiv preprints discuss adversarial attacks, black-box and white-box attacks, textual adversarial examples, semantic attacks, and robustness evaluation on large language models such as GPT-3, ChatGPT, Bard, Claude, and other transformer-based dialogue generation systems, including attack methods, datasets, metrics, defenses, adversarial training, and vulnerability analysis in NLP security and dialogue system security?"}
{"original_query": "Is there any works that explores how to achieve balance between representativeness and diversity in chosen samples for few-shot data selection?", "expanded_query": "What research works explore the balance between representativeness and diversity in sample selection for few-shot learning, focusing on representative and diverse sampling strategies, subset selection algorithms, active learning, dataset bias mitigation, and representativeness-diversity trade-offs in few-shot data selection methods?"}
{"original_query": "Is there commonsense reasoning dataset which generates diverse sentences to describe the relation between concepts?", "expanded_query": "commonsense reasoning dataset diverse sentence generation relation between concepts concept pairs semantic relation extraction knowledge graph embeddings ConceptNet ATOMIC CLUTRR ReClor SICK GLUE SuperGLUE natural language inference sentence diversity synthetic data augmentation conceptual similarity knowledge representation NLP dataset OpenAI GPT-3"}
{"original_query": "Is there such a factuality evaluation dataset that can be used to evaluate the performance of fact-checking models on summaries generated by latest summarization models?", "expanded_query": "Is there a publicly available factuality evaluation dataset or benchmark for evaluating fact-checking models on summaries generated by state-of-the-art summarization models such as BART, PEGASUS, T5, GPT-4, or Longformer, covering domains like news, Wikipedia, scientific literature, medical and legal documents, with human-annotated ground truth, inter-annotator agreement, multilingual support, and evaluation metrics including accuracy, precision, recall, F1, ROUGE, BLEU, METEOR, semantic similarity, truthfulness, hallucination detection, and factual consistency?"}
{"original_query": "Is there such a reading comprehension dataset in understanding a snippet from a long story book, while it requires to integrate the necessary long history texts before the snippet to full understand it?", "expanded_query": "Is there a long\u2011form reading comprehension dataset designed for understanding a snippet from a long storybook that requires integrating prior historical context to fully comprehend it, such as NarrativeQA, StoryCloze, or other long\u2011document QA datasets that test contextualized snippet understanding in narrative texts, including concepts like long context reading comprehension, contextualized reading comprehension tasks, storybook comprehension, long narrative context, machine reading comprehension, natural language processing, contextual embeddings, and long document QA?"}
{"original_query": "Provide an example of a paper which proposes a method to learn a dynamic (conditioned on the input) sequence tokenizer (segmenter) via standard gradient backpropagation.", "expanded_query": "Provide an example of a research paper that proposes a neural network-based method for learning a dynamic, input-conditioned sequence tokenizer or segmenter via standard gradient backpropagation, including concepts such as adaptive tokenization, conditional segmentation, sequence labeling, attention mechanisms, transformer or RNN architectures, CRF layers, end-to-end training, deep learning frameworks, and applications to text, speech, or multimodal data."}
{"original_query": "What are some data-efficient ways to learn text embeddings thru contrastive learning?", "expanded_query": "What are some data\u2011efficient methods for learning text embeddings through contrastive learning, including techniques such as SimCSE, SimCLR, MoCo, BYOL, InfoNCE loss, self\u2011supervised sentence transformers, contrastive predictive coding, cross\u2011modal contrastive learning with CLIP, hard negative mining, data augmentation, backtranslation, few\u2011shot and zero\u2011shot transfer, parameter\u2011efficient fine\u2011tuning with LoRA or adapters, multilingual contrastive learning with XLM\u2011R, and evaluation on semantic textual similarity benchmarks?"}
{"original_query": "What are some methods for solving the class-incremetal continual learning problems?", "expanded_query": "What are some methods for solving the class-incremental continual learning problems, including regularization-based approaches such as elastic weight consolidation, synaptic intelligence, and L2 regularization, replay-based strategies like experience replay and generative replay with GANs or VAEs, dynamic architecture methods such as progressive neural networks, network expansion, and parameter isolation, meta-learning techniques, knowledge distillation, and other approaches for mitigating catastrophic forgetting in class-incremental scenarios on benchmark datasets such as Split MNIST, Split CIFAR-10/100, and ImageNet, and what are the evaluation metrics, benchmark protocols, and future research directions?"}
{"original_query": "What is a large event-coverage general-domain event argument extraction dataset?", "expanded_query": "What is a large-scale, general-domain event-coverage dataset for event argument extraction in NLP, including event schemas, argument roles, semantic role labeling, event coreference resolution, entity linking, structured data, benchmark evaluation, and publicly available resources?"}
{"original_query": "What is the first paper to address the problem of predicting knowledge graphs whose nodes, links and attributes change with time?", "expanded_query": "What is the earliest published paper that introduced methods for predicting time\u2011evolving or dynamic knowledge graphs, including temporal link prediction, temporal knowledge graph completion, time\u2011aware knowledge graph representation learning or embedding, and addressing changes in nodes, edges, and attributes over time?"}
{"original_query": "What is the performance of large language models in text summarization under reference-based and reference-free human evaluations?", "expanded_query": "What is the performance of large language models such as GPT\u20113, GPT\u20114, T5, BART, Pegasus, and other LLMs on text summarization tasks across datasets like CNN/DailyMail, XSum, PubMed, and arXiv, evaluated using reference\u2011based metrics such as ROUGE, BLEU, METEOR, BERTScore, and reference\u2011free human evaluations assessing coherence, informativeness, factual consistency, readability, and overall quality, and how do these models compare in extractive versus abstractive summarization under both evaluation frameworks?"}
{"original_query": "What limitations do large language models have in evaluating information-seeking question answering?", "expanded_query": "What limitations do large language models have in evaluating information-seeking question answering, including issues of hallucination, factual accuracy, knowledge cutoff, contextual understanding, semantic ambiguity, source verification, bias, explainability, interpretability, evaluation metrics, human evaluation, question complexity, open-domain versus closed-domain QA, retrieval augmentation, knowledge graph integration, multi-hop reasoning, chain-of-thought prompting, domain adaptation, fine-tuning, prompt engineering, few-shot and zero-shot learning, adversarial robustness, scalability, computational cost, energy consumption, environmental impact, model size, parameter count, training data diversity, language coverage, and ethical considerations such as privacy and bias mitigation?"}
{"original_query": "What paper compares humans' and language models' non-literal interpretations of utterances featuring phenomena like deceit, irony, and humor?", "expanded_query": "Which research paper compares human and large language model non\u2011literal interpretation of utterances featuring deceit, irony, sarcasm, and humor, analyzing pragmatic inference and figurative language using datasets such as the Deception Corpus, Irony Corpus, and Humor Corpus, and reports evaluation metrics like accuracy, F1, human agreement, and error analysis across transformer\u2011based models such as GPT\u20114, GPT\u20113.5, BERT, RoBERTa, and XLNet, published in conferences like ACL, EMNLP, NAACL, or journals such as TACL?"}
{"original_query": "What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs?", "expanded_query": "What work attempts to explore multi-hop reasoning by densifying commonsense knowledge graphs, including concepts such as ConceptNet, ATOMIC, COMET, knowledge graph densification techniques, graph neural networks, graph attention networks, graph transformers, knowledge graph embedding, RDF, SPARQL, semantic web, AI reasoning, multi-hop inference, knowledge graph completion, augmentation, expansion, and related research on multi-hop question answering and commonsense reasoning?"}
{"original_query": "Which article first proposed shuffled-group-whitening to solve the problem of sentence representation learning?", "expanded_query": "Which article first proposed shuffled-group-whitening as a technique to solve the problem of sentence representation learning in natural language processing, addressing sentence embeddings, semantic similarity, deep learning, text representation, and the sentence representation learning challenge?"}
{"original_query": "Which dataset supports narration generation and temporal localization tasks in Chinese movies?", "expanded_query": "Which Chinese movie dataset supports narration generation and temporal localization tasks, including video captioning, temporal segmentation, speech-to-text, subtitle annotations, audio-visual alignment, and movie script analysis for Chinese cinema?"}
{"original_query": "Which family of model generally perform the best for the event conceptualization task", "expanded_query": "Which family of models generally perform the best for the event conceptualization task, including event detection, event argument role labeling, event type classification, and event linking, comparing transformer\u2011based models such as BERT, RoBERTa, GPT, XLNet, XLM\u2011R, mBERT, and other deep learning architectures like LSTM, CNN, GNN, and hybrid models, evaluated on benchmark datasets such as ACE\u202f2005, EventX, SemEval, CoNLL, using metrics like accuracy, F1, precision, recall, and considering factors such as fine\u2011tuning, zero\u2011shot, few\u2011shot, domain adaptation, multilingual capabilities, joint learning, multi\u2011task learning, data augmentation, synthetic data, adversarial training, robustness, model interpretability, attention visualization, error analysis, and future research directions?"}
{"original_query": "Which is the first multimodal model combining text and speech transformers trained without labelled text-speech pairs?", "expanded_query": "Which is the first multimodal transformer model that combines text and speech encoders, trained in a self\u2011supervised, cross\u2011modal, unsupervised manner without labelled text\u2011speech pairs, and what are its key architectural components (speech transformer architecture, text transformer architecture, audio embeddings, speech\u2011to\u2011text and text\u2011to\u2011speech modules), training data sources (LibriSpeech, VoxPopuli, Common Voice, unpaired audio\u2011text corpora), benchmark performance on speech\u2011to\u2011text, text\u2011to\u2011speech, and cross\u2011modal alignment tasks, and its historical significance in the literature?"}
{"original_query": "Which knowledge graph completion method focuses on reducing memory usage by pruning features?", "expanded_query": "Which knowledge graph completion method focuses on reducing memory usage by pruning features, such as low-rank approximation, feature selection, sparsity, pruning techniques, efficient graph embeddings, scalable KG completion, memory optimization, graph neural networks, feature pruning for KG completion, memory footprint reduction, and knowledge graph embeddings?"}
{"original_query": "Which language model distillation paper that first identified the capacity gap in distillation and used the MoE student model to counter the curse of capacity gap?", "expanded_query": "Which language model distillation paper first identified the capacity gap in knowledge distillation, introduced a Mixture\u2011of\u2011Experts (MoE) student model to counter the curse of capacity gap, and applied it to transformer\u2011based language models such as BERT, GPT, and other large\u2011scale models, comparing teacher and student performance and model compression?"}
{"original_query": "Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions?", "expanded_query": "Which numerical reasoning paper first published a dataset that considers different types of size of numbers and their representations in arithmetic questions, including magnitude categories, numeric representation forms, and arithmetic question formats, and what is the name of that dataset and the authors?"}
{"original_query": "Which paper about parameter-efficient finetuning first proposes to feed the pretrained weight instead of the activation to an adapter?", "expanded_query": "Which research paper on parameter\u2011efficient fine\u2011tuning of pretrained transformer models first proposes feeding pretrained weights instead of activations into an adapter module, contrasting with traditional activation\u2011based adapters, and discusses weight\u2011based adapter injection, low\u2011rank adaptation, LoRA, and other parameter\u2011efficient transfer learning techniques, as presented in conferences such as NeurIPS, ICLR, ACL, EMNLP, or on arXiv?"}
{"original_query": "Which paper combines the advantages of different frameworks for grammar error correction (GEC) and achieves good performance?", "expanded_query": "Which recent research paper published in ACL, EMNLP, or NAACL that combines the strengths of rule\u2011based, statistical, and neural sequence\u2011to\u2011sequence transformer frameworks for grammar error correction (GEC), employs multi\u2011task learning, ensemble methods, and fine\u2011tuned BERT or T5 models, achieves state\u2011of\u2011the\u2011art performance on CoNLL\u20112014 and BEA\u20112019 benchmarks, reports high BLEU, GLEU, and F0.5 scores, and provides detailed ablation studies and error\u2011type analysis?"}
{"original_query": "Which paper did a comprehensive survey of the code large language model (code LLMs)?", "expanded_query": "Which survey paper provides a comprehensive overview of code large language models (code LLMs), covering models such as OpenAI Codex, GitHub Copilot, DeepMind AlphaCode, and their applications in code generation, debugging, and software development, published in 2023 or 2024 in venues like ACL, EMNLP, ICLR, NeurIPS, or on arXiv?"}
{"original_query": "Which paper employs a two-stage approach in generative models to tackle ABSA tasks across various domains?", "expanded_query": "Which research paper, published in a top NLP conference or journal, employs a two-stage generative model approach to tackle aspect-based sentiment analysis tasks across multiple domains, leveraging transformer architectures such as BERT, GPT, T5, or BART, incorporating cross-domain adaptation or domain generalization techniques, and evaluated on standard ABSA datasets like SemEval 2014\u20112019 for aspect extraction and sentiment classification?"}
{"original_query": "Which paper enables interactive semantic parsing by training an error correction model with simulated human feedback instead of human annotations?", "expanded_query": "Which research paper on interactive semantic parsing describes training an error correction model using simulated human feedback instead of human annotations, highlighting methods for reducing annotation costs, human\u2011in\u2011the\u2011loop learning, and improving parsing accuracy in natural language processing?"}
{"original_query": "Which paper explored training a GPT-2 for automatic diagnosis, emphasizing efficient data augmentation for symptom prediction and disease identification?", "expanded_query": "Which 2023 or 2024 research paper, published in a medical AI conference or journal, explored fine\u2011tuning GPT\u20112 for automatic medical diagnosis, emphasizing efficient data augmentation techniques such as synthetic symptom generation, backtranslation, and word substitution to improve symptom prediction and disease identification from electronic health records?"}
{"original_query": "Which paper first aggregates statements to represent political actors and learns the mapping from languages to representation via pre-training?", "expanded_query": "Which paper first aggregates statements to represent political actors and learns the mapping from languages to representation via pre-training, using transformer-based models such as BERT, multilingual embeddings, cross-lingual mapping, political discourse analysis, entity representation, knowledge graph construction, and political actor embeddings in natural language processing and political science research?"}
{"original_query": "Which paper first applied the chain-of-thought technique in the text summarization field?", "expanded_query": "Which paper first applied the chain-of-thought technique in the text summarization field, including chain-of-thought prompting, chain-of-thought reasoning, chain-of-thought summarization, chain-of-thought summarization technique, chain-of-thought summarization method, chain-of-thought summarization approach, earliest study, earliest publication, earliest article, first documented, first reported, first described, first published, first introduced, first implemented, first employed, first adopted, first used in summarization, first used in text summarization, first used in summarization domain, first used in summarization literature, first used in summarization research, first used in summarization area, first used in summarization discipline, first used in summarization field"}
{"original_query": "Which paper first apply mixture of experts idea to large language models for domain adaptation?", "expanded_query": "Which research paper first applied the mixture of experts (MoE) architecture to large language models for domain adaptation, describing sparse MoE layers, expert gating mechanisms, transformer-based LLMs, domain-specific fine-tuning, cross-domain transfer learning, and parameter-efficient adaptation, and what are its authors, publication venue, and year?"}
{"original_query": "Which paper first attempts to take potential dependencies among same-level labels into account in Hierarchical Text Classification?", "expanded_query": "Which paper first attempts to take potential dependencies among same-level labels into account in hierarchical text classification, exploring intra-level label co-occurrence, dependency-aware hierarchical multi-label classification, taxonomy-based label hierarchy, and early research on label dependency modeling in text classification?"}
{"original_query": "Which paper first combines different methods for uncertainty quantification in one?", "expanded_query": "Which paper first combines different methods for uncertainty quantification, including Bayesian inference, frequentist confidence intervals, Monte Carlo simulation, bootstrapping, interval estimation, model propagation, and ensemble techniques, into a single integrated framework, and what are its publication details such as year, journal, and authors?"}
{"original_query": "Which paper first combines rewriting and expansion methods to reformulate a query for conversational search?", "expanded_query": "Which research paper first combined query rewriting and query expansion methods to reformulate a query for conversational search, including authors, publication venue, year, conference or journal, citation, impact on contextual query reformulation, and its role in information retrieval, natural language processing, dialogue systems, semantic search, neural IR, retrieval augmentation, question answering, search engine query rewriting, context\u2011aware query rewriting, and contextual expansion?"}
{"original_query": "Which paper first conducted the positioned error test for the MAUVE metric?", "expanded_query": "Which paper first conducted the positioned error test for the MAUVE metric, including its authors, publication year, venue (conference or journal), and the context of the MAUVE metric in evaluating distributional similarity and distribution shift for language model text generation?"}
{"original_query": "Which paper first construct large-scale corpus to improve in-context learning of large language models in the pre-training stage?", "expanded_query": "Which paper first constructed a large-scale corpus to improve in-context learning of large language models during the pre-training stage, detailing dataset design, scaling strategies, and impact on LLM contextual performance?"}
{"original_query": "Which paper first constructed a structured knowledge base to interconnect different human social roles and attributes?", "expanded_query": "Which paper first constructed a structured knowledge base or knowledge graph to interconnect different human social roles, role attributes, role hierarchies, role semantics, social role ontology, and role theory for role-based knowledge representation?"}
{"original_query": "Which paper first explored In-context learning in a cross lingual setup and made use of alignment to better it's performance?", "expanded_query": "Which paper first explored cross-lingual in-context learning using alignment techniques to improve performance in a cross-lingual setup, leveraging transformer-based language models for zero-shot and few-shot cross-lingual transfer?"}
{"original_query": "Which paper first found that multilingual models can inference cross-lingual supervision in MLM training by themself?", "expanded_query": "Which paper first discovered that multilingual language models can infer cross-lingual supervision from masked language modeling training alone, and what are the key authors, publication venue, year, and the main contributions of that study, including references to mBERT, XLM, XLM\u2011R, cross\u2011lingual transfer, zero\u2011shot cross\u2011lingual tasks, and self\u2011supervised learning?"}
{"original_query": "Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval?", "expanded_query": "Which paper first introduced document content as an intermediate generation target and utilized textual document identifiers in generative retrieval, within the context of retrieval-augmented generation, neural retrieval, transformer-based language models, document embeddings, and information retrieval pipelines?"}
{"original_query": "Which paper first propose to mask positions to pre-train multi-modal document transformer\uff1f", "expanded_query": "Which paper first proposed masking positions to pre-train a multi-modal document transformer, using masked language modeling and vision\u2011language alignment, and what are its authors, publication year, dataset, transformer architecture, and key contributions?"}
{"original_query": "Which paper first proposed a cross-domain language model to automatically generate much labeled data for a unlabeled target domain?", "expanded_query": "Which paper first proposed a cross\u2011domain language model to automatically generate large amounts of labeled data for an unlabeled target domain, employing domain adaptation, semi\u2011supervised learning, synthetic data generation, transfer learning, and addressing domain shift in NLP tasks such as text classification, sentiment analysis, and named entity recognition, and what are its authors, publication venue, year, key contributions, experimental results, evaluation metrics, and impact on the field?"}
{"original_query": "Which paper first proposed extracting the pair of target and stance from sentences?", "expanded_query": "Which paper first proposed extracting the pair of target and stance from sentences in targeted stance detection, including target\u2011stance pair extraction in sentiment analysis, stance mining, argument mining, early NLP research, and what authors, publication year, and venue did it appear in?"}
{"original_query": "Which paper first proposed shared adapter module across layers?", "expanded_query": "Which research paper first proposed a shared adapter module across layers for parameter-efficient fine-tuning in transformer-based language models, and what are its authors, publication year, and citation details?"}
{"original_query": "Which paper first proposed to combine pretrained masked language models (BERT) and discrete diffusion language models?", "expanded_query": "Which paper first proposed combining pretrained masked language models such as BERT with discrete diffusion language models for text generation, including concepts like diffusion-based generation, non-autoregressive modeling, transformer architecture, and discrete diffusion processes?"}
{"original_query": "Which paper first proposed to only update some original weights of self-attention layers in parameter-efficient fine-tuning?", "expanded_query": "Which paper first proposed updating only a subset of the original weights in self\u2011attention layers for parameter\u2011efficient fine\u2011tuning of large transformer\u2011based language models, such as LoRA, adapter modules, prefix tuning, or prompt tuning, and what is its citation?"}
{"original_query": "Which paper first published a real-world Chinese-English text image translation dataset?", "expanded_query": "Which paper first published a real-world Chinese-English text image translation dataset, including its title, authors, publication venue, year, DOI, dataset name, release date, size, number of images, annotation methods, source image domains, licensing, repository platform (e.g., GitHub, Zenodo), repository URL, download link, image resolution, text density, font variety, scene complexity, evaluation metrics, benchmark results, impact on OCR and cross-lingual image-to-text research, citation count, and subsequent dataset versions or updates?"}
{"original_query": "Which paper first shows that it is possible to maintain high LLM reasoning performance with in-context examples that are absurdly wrong?", "expanded_query": "Which research paper first demonstrates that large language models can maintain high reasoning performance with in-context examples that are absurdly wrong, thereby revealing robustness to adversarial prompts, and what are its key findings, methodology, evaluation benchmarks, and implications for prompt engineering, few-shot learning, model calibration, and benchmark datasets in GPT\u20114, ChatGPT, and other LLMs?"}
{"original_query": "Which paper first shows that large language models can be prompted to act like professional annotators to evaluate text generation quality?", "expanded_query": "Which paper first demonstrates that large language models can be prompted to act as professional annotators for evaluating text generation quality, employing prompt engineering, human-like annotation, automatic evaluation metrics, benchmark studies, and what are its authors, publication year, and methodology?"}
{"original_query": "Which paper first studied the efficiency robustness of multi-exit language models?", "expanded_query": "Which paper first studied the efficiency robustness of multi-exit language models, covering efficiency, robustness, dynamic inference, early exit, transformer architecture, language model performance, runtime efficiency, accuracy trade-offs, robustness evaluation, and multi-exit transformer models?"}
{"original_query": "Which paper first use the attention weights to guide the simultaneous inference of speech translation models?", "expanded_query": "Which paper first used attention weights to guide simultaneous inference in end-to-end speech translation models, employing attention mechanisms for real-time speech-to-text translation, and what are its authors, publication venue, year, and citation?"}
{"original_query": "Which paper first used structural information for coherence modeling?", "expanded_query": "Which paper first used structural information for coherence modeling in natural language processing, citing early research on structural features in text coherence, and what is its publication year and authorship?"}
{"original_query": "Which paper found that mutual learning benefits multlingual models?", "expanded_query": "Which research paper discovered that mutual learning improves multilingual language models, including cross-lingual transfer, knowledge distillation, and multilingual BERT representations, and what are its authors, publication venue, and year?"}
{"original_query": "Which paper highlights the need for leveraging all available resources, including dictionaries, machine translation systems, and language learners, to construct NLP data in low-resource languages?", "expanded_query": "Which research paper, published in a prominent NLP venue such as ACL, EMNLP, COLING, LREC, or on arXiv, discusses the necessity of leveraging all available resources\u2014including dictionaries, machine translation systems, language learners, community annotations, crowdsourcing, transfer learning, cross-lingual embeddings, multilingual corpora, lexical resources, language documentation, semantic annotation, zero-shot learning, pretrained language models, data augmentation, synthetic data, and human\u2011in\u2011the\u2011loop approaches\u2014to construct NLP data for low\u2011resource languages, and what are its authors, title, abstract, keywords, citation count, publication year, and impact factor?"}
{"original_query": "Which paper introduce a DRO (distribution robust optimization) like training objective for doing adversarial training without constructing adversarial samples.", "expanded_query": "Which paper introduced a distributionally robust optimization (DRO) training objective for adversarial training that avoids constructing adversarial samples, addressing robust optimization, distributional robustness, adversarial robustness, robust training methods, and robustness to perturbations without generating adversarial examples?"}
{"original_query": "Which paper introduced the human-evaluated timeliness metric for misinformation detection?", "expanded_query": "Which paper first introduced the human-evaluated timeliness metric for misinformation detection, including its authors, publication venue, year, methodology, evaluation framework, dataset, and the context of its application to time-sensitive misinformation in social media and news fact-checking?"}
{"original_query": "Which paper introduces the R-GCN technique into document-level joint entity and relation extraction?", "expanded_query": "Which paper introduces the R-GCN technique into document-level joint entity and relation extraction using graph convolutional networks for improved entity and relation extraction across documents?"}
{"original_query": "Which paper investigates the influence of the diversity of source tasks on the performance of target tasks in prompt tuning using CrossFit?", "expanded_query": "Which research paper examines how the diversity of source tasks affects target task performance in CrossFit prompt tuning, exploring source task heterogeneity, prompt-based transfer learning, and cross-domain generalization?"}
{"original_query": "Which paper is among the earliest to train on extensive collection of signing video and subtitle pairs available from online platforms?", "expanded_query": "Which early research paper is among the first to train on an extensive collection of signing video and subtitle pairs sourced from online platforms, thereby advancing sign language translation, video-to-text, multimodal deep learning, hand gesture recognition, and providing a benchmark dataset for ASL and other sign languages?"}
{"original_query": "Which paper is the first to comprehensively review the progress of deep learning in mathematical reasoning?", "expanded_query": "Which paper is the first comprehensive survey or review that systematically chronicles the progress of deep learning techniques applied to mathematical reasoning, including neural network architectures, symbolic reasoning integration, logic-based methods, benchmark datasets, evaluation metrics, and future research directions, and what are its publication details such as year, journal or conference, authors, and citation impact?"}
{"original_query": "Which paper makes sure that the questions used in the paper are all from real users that are genuinely curious about a specific topic or concept?", "expanded_query": "Which paper ensures that all questions used are authentic user\u2011generated queries from real users who are genuinely curious about a specific topic or concept, detailing the question selection methodology, provenance, validation process, crowdsourcing approach, and quality assurance for topic relevance and user intent?"}
{"original_query": "Which paper measured how well the source-translation contribution by the translation model can be used to detect its own hallucinations?", "expanded_query": "Which paper measured how well the source\u2011translation contribution by a neural machine translation transformer model can be used to detect its own hallucinations, using metrics such as contribution score, alignment confidence, hallucination detection accuracy, and source\u2011translation contribution analysis?"}
{"original_query": "Which paper presents an easy to implement and high performing method for OOD detection with language models?", "expanded_query": "Which research paper presents an easy-to-implement, high-performing method for out-of-distribution detection in language models, covering transformer-based architectures such as BERT, RoBERTa, GPT\u20113/4, and LLMs, using techniques like energy\u2011based scoring, Mahalanobis distance, temperature scaling, entropy, and ensemble methods, evaluated on standard NLP benchmarks (GLUE, SQuAD, XNLI, SuperGLUE), with open\u2011source implementation on GitHub, and published in venues such as ACL, EMNLP, NeurIPS, ICLR, ICML, or on arXiv?"}
{"original_query": "Which paper produces a dataset for text simplification in over 12 languages and evaluates both finetuning and in context learning approaches to text simplification in those languages?", "expanded_query": "Which research paper introduces a multilingual text simplification dataset covering more than 12 languages and evaluates both fine\u2011tuning and in\u2011context learning approaches with transformer\u2011based language models, reporting automatic metrics such as SARI, BLEU, ROUGE, and human readability scores across languages like English, Spanish, French, German, Chinese, Arabic, Hindi, Russian, and other major language families?"}
{"original_query": "Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models?", "expanded_query": "Which paper proposed a learning-based data augmentation method for improving compositional generalization of language models, including details on the augmentation strategy, neural network architecture, evaluation metrics, dataset, authors, publication venue, and year?"}
{"original_query": "Which paper proposed decomposing the logit update of each of the attention blocks\u2019 inputs to analyze how the context influences the prediction?", "expanded_query": "Which paper proposed decomposing the logit update of each of the attention blocks\u2019 inputs to analyze how the context influences the prediction, using transformer attention mechanisms, contextual embeddings, logit decomposition, and model interpretability techniques in natural language processing and deep learning?"}
{"original_query": "Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model?", "expanded_query": "Which paper proposed dictionary-based Bayesian inference to improve the performance of image text matching model, including authors, publication year, conference or journal (e.g., CVPR, ICCV, ECCV, NeurIPS, ICLR, IEEE, ACM), dataset (e.g., Flickr30k, MSCOCO), evaluation metrics (accuracy, precision, recall, F1), state-of-the-art results, cross-modal retrieval, visual-semantic embedding, deep learning, feature representation, semantic alignment, latent variables, prior distribution, posterior inference, dictionary learning, textual embeddings, visual embeddings, multimodal, retrieval tasks, image captioning, model architecture, embedding space, and citation details?"}
{"original_query": "Which paper proposed the integration of human translators' considerations, such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies, into the design of machine translation models when translating lyrics?", "expanded_query": "Which paper proposed integrating human translators' considerations such as length control, rhyme type control and suggestion, and enhancing compatibility between translation output and unseen melodies into the design of neural machine translation models for translating song lyrics, and what are its authors, publication venue, year, key contributions, evaluation metrics, and impact on semantic fidelity, prosody, rhyme scheme, meter, melody alignment, and overall translation quality?"}
{"original_query": "Which paper proposes a memory-efficient optimizer considering the confidence of each update during the optimization?", "expanded_query": "Which research paper proposes a memory-efficient optimizer that incorporates confidence weighting of each update during stochastic gradient descent, aiming to reduce memory footprint while maintaining convergence guarantees, and what are its key algorithmic contributions, experimental results on benchmark datasets, and publication details such as authors, venue, and year?"}
{"original_query": "Which paper proposes the two-stage training method, i.e., task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using the self-collected dataset.", "expanded_query": "Which research paper proposes a two-stage training method, comprising task-specific fine-tuning and cross-domain pre-training, to train an open-domain dialogue evaluator using a self-collected dataset, and what are its key contributions, evaluation metrics, and implications for dialogue system quality assessment?"}
{"original_query": "Which paper proposes to use rewriting based approaches to defending against adversarial attacks in text classification?", "expanded_query": "Which research paper proposes using rewriting-based approaches to defend against adversarial attacks in text classification, including methods such as paraphrasing, text rewriting, and adversarial text perturbation mitigation in natural language processing security?"}
{"original_query": "Which paper showed that social relationships were helpful for identifying inappropriate messages?", "expanded_query": "Which research paper or study demonstrated that social relationships, social ties, or social network connections help identify or detect inappropriate messages, offensive content, harassment, hate speech, cyberbullying, or spam in online communities, and what were the authors, publication venue, and year of that paper?"}
{"original_query": "Which paper shows assessment of training instabilities at different levels for language models?", "expanded_query": "Which paper shows assessment of training instabilities at different levels for language models, including gradient explosion, learning rate sensitivity, optimizer dynamics, dataset shifts, architecture variations, transformer depth, LLMs such as GPT-3, BERT, LLaMA, and evaluation metrics like loss curves, perplexity, and stability analysis, published in conferences like NeurIPS, ICLR, ACL, or on arXiv in 2023-2024?"}
{"original_query": "Which paper shows that in instruction tuning, the instructions can be compressed to small supporting sets of words that provide useful information?", "expanded_query": "Which research paper demonstrates that in instruction tuning of large language models, instructions can be compressed into small supporting sets of words that still provide useful information, covering topics such as prompt compression, semantic support, few-shot learning, evaluation metrics like BLEU and ROUGE, and is published in venues such as ACL, NeurIPS, ICLR, or on arXiv by authors from OpenAI, Google, or DeepMind?"}
{"original_query": "Which paper studies how current retrieval systems handle queries which contain multiple constraints?", "expanded_query": "Which scholarly article investigates how contemporary information retrieval systems process and satisfy queries containing multiple constraints, examining constraint handling mechanisms, query decomposition strategies, retrieval effectiveness, and evaluation metrics in modern retrieval architectures?"}
{"original_query": "Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident.", "expanded_query": "Which paper studies the concept of enhancing the coverage of a selective prediction system by re-attempting the questions on which it was not sufficiently confident, exploring confidence thresholds, selective classification, uncertainty handling, coverage improvement, re-attempting uncertain predictions, and confidence-based reclassification in selective prediction coverage enhancement?"}
{"original_query": "Which paper surveyed the datasets and tasks of asking clarification questions in conversational systems??", "expanded_query": "Which survey paper reviewed datasets and tasks for asking clarification questions in conversational systems, including dialogue management, question clarification datasets, question clarification tasks, conversational AI, chatbots, NLP, AI, dialogue datasets, question clarification research, and what are its key findings and references?"}
{"original_query": "Which paper used both automatically generated and manual templates with word tuples to adapt language models from one timestamp to another?", "expanded_query": "Which research paper on temporal adaptation of language models, addressing temporal domain shift by using both automatically generated and manually crafted templates with word tuples to adapt model performance from one timestamp to another, published in a major NLP conference or journal such as ACL, EMNLP, NAACL, NeurIPS, ICLR, ICML, or on arXiv, and what are its authors, year, venue, and citation details?"}
{"original_query": "Which paper utilizes language models to generate singable lyrics that can go well with a predefined melody?", "expanded_query": "Which research paper or study utilizes language models such as GPT, BERT, or transformer-based neural networks, or deep learning techniques to generate singable lyrics that align with a predefined melody, focusing on melody-conditioned lyric generation, text-to-melody alignment, AI songwriting systems, music information retrieval, and discussing methods such as attention mechanisms, sequence-to-sequence modeling, or GANs for ensuring melodic compatibility and rhythmic coherence, published in recent conferences like ICLR, NeurIPS, ACL, ISMIR, or journals such as JMLR or IEEE Transactions on Affective Computing?"}
{"original_query": "Which paper was the first to propose combining human spoken language and sign language datasets with gloss annotations to enhance the performance of sign language translation?", "expanded_query": "Which paper was the first to propose combining human spoken language and sign language datasets with gloss annotations to enhance the performance of sign language translation, multimodal datasets, cross-modal learning, speech-to-sign translation, deep learning, sequence-to-sequence models, transformer architectures, dataset fusion, American Sign Language, British Sign Language, gloss annotation, sign language corpora, BLEU score, accuracy, cross-lingual transfer, data augmentation, cross-modal attention, neural networks, multimodal learning, sign language recognition, speech recognition, language modeling, benchmark, evaluation metrics, citation, authors, publication year, conference, journal, ACL, EMNLP, ICLR, NeurIPS, CVPR, IEEE Transactions on Pattern Analysis and Machine Intelligence, IEEE Transactions on Multimedia."}
{"original_query": "Which papers develop methods to make in-context learning more computationally efficient?", "expanded_query": "Which research papers and conference proceedings (e.g., ICLR, NeurIPS, ACL, EMNLP, arXiv) describe methods to make in-context learning more computationally efficient, covering techniques such as prompt tuning, parameter\u2011efficient fine\u2011tuning, model compression, quantization, pruning, sparsity, efficient prompt design, dynamic computation, adaptive inference, and other resource\u2011aware strategies for large language models, and what are their key findings and contributions in recent years (2023\u20112024)?"}
{"original_query": "Which papers were among the first to explore the task of targeted training data extraction?", "expanded_query": "Which early papers, publications, or research articles were among the first to explore or investigate the task of targeted training data extraction, including initial studies, pioneering works, and foundational literature on targeted data extraction methods, techniques, and dataset extraction in machine learning, deep learning, neural networks, and data mining, and what conferences or journals featured these first investigations?"}
{"original_query": "Which pre-trained model is specifically designed for low-resource dialogue summarization tasks?", "expanded_query": "Which pre-trained transformer model, such as Pegasus, T5, GPT\u20113, or DistilBERT, is specifically designed for low\u2011resource dialogue summarization tasks, leveraging few\u2011shot learning, transfer learning, domain adaptation, cross\u2011lingual summarization techniques, and optimized for low\u2011resource languages and dialogue summarization datasets like DialogSum, OpenDialKG, and other low\u2011resource dialogue summarization benchmarks?"}
{"original_query": "Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs to aid in recognizing causal relations between events?", "expanded_query": "Which research paper leverages event structure information from Abstract Meaning Representation (AMR) graphs, graph neural networks, transformer-based models such as BERT or RoBERTa, graph convolutional networks, graph attention networks, semantic parsing, semantic role labeling, or knowledge graph techniques to aid in recognizing causal relations between events, event causality detection, event relation classification, or causal event extraction in natural language processing, and what is its title, authors, publication venue, year, dataset, methodology, and evaluation results?"}
{"original_query": "Which vision-language model can demonstrate that visual grounding could facilitate efficient language acquisition? (OctoBERT)", "expanded_query": "Which vision-language model, such as OctoBERT, can demonstrate that visual grounding facilitates efficient language acquisition through multimodal transformer architecture, cross-modal attention, semantic alignment, and benchmark evaluation on datasets like ImageNet, COCO, Visual Genome, and language acquisition metrics, thereby advancing grounded language learning, child language development theories, and AI cognitive modeling in multimodal AI research?"}
{"original_query": "Which vision-language model paper in 2023 developed techniques that reduce input tokens to improve model inference speed?", "expanded_query": "Which 2023 vision\u2011language model paper introduced token reduction techniques such as token pruning, token compression, or tokenization optimization to accelerate inference speed for transformer\u2011based models like CLIP, BLIP, ViLT, or LLaVA, and was presented at conferences such as CVPR, NeurIPS, or ICLR?"}
{"original_query": "Which was the first paper to explore the online adaptation of neural MT metrics for use during the inference stage?", "expanded_query": "Which was the first paper to explore the online adaptation of neural machine translation metrics for use during the inference stage, including dynamic adaptation of BERTScore, COMET, BLEURT, and other neural MT evaluation metrics in real\u2011time MT inference, and what authors, publication venue, and year introduced this concept?"}
{"original_query": "Which work discusses an analysis of source and target contributions to output generation based on local interpretation when machine translation models experience hallucinations?", "expanded_query": "Which research paper analyzes source and target contributions to output generation based on local interpretation in transformer-based neural machine translation models experiencing hallucinations, examining attention-based source-target alignment, local interpretability techniques, and hallucination detection?"}
{"original_query": "Which work proposes an approach to improve candidate responses in the smart reply task by directly optimizing the metric to ensure that a response is selected by the user?", "expanded_query": "Which research paper proposes a method to improve candidate responses in the smart reply task by directly optimizing a user selection metric to ensure that a response is chosen by the user, using learning\u2011to\u2011rank, reinforcement learning, policy gradient, or other metric optimization techniques in NLP dialogue systems for smart reply improvement and response selection?"}
{"original_query": "what's the first paper that manages to handle KBQA using LLMs without fine-tuning?", "expanded_query": "Which earliest research paper demonstrates zero\u2011shot or few\u2011shot knowledge base question answering using large language models such as GPT\u20113, GPT\u20113.5, GPT\u20114, or other LLMs without fine\u2011tuning, employing prompt engineering, retrieval\u2011augmented generation, knowledge graph embeddings, semantic parsing, or SPARQL generation, evaluated on benchmark datasets like WebQuestionsSP, WikiSQL, LC\u2011QuAD, or SimpleQuestions, and published in a top\u2011tier conference or journal, with citation metrics, impact factor, and state\u2011of\u2011the\u2011art performance?"}
{"original_query": "which paper first focuses on addressing the over-smoothing issue for sentence embedding?", "expanded_query": "Which paper first addresses the over-smoothing issue in sentence embeddings, focusing on transformer models such as BERT, RoBERTa, Sentence\u2011BERT, SimCSE, and graph neural networks, and proposes methods to improve contextualized sentence representation for semantic similarity tasks in NLP, including embedding quality and deep learning techniques?"}
{"original_query": "Can we reduce visual tokens in vision transformers right from the beginning?", "expanded_query": "Can we reduce visual tokens in vision transformers right from the beginning by applying token pruning, token merging, token sparsity, token compression, or token selection techniques at the patch embedding stage to improve computational efficiency, memory usage, and inference speed for image classification, object detection, and segmentation tasks on high-resolution images and resource-constrained devices?"}
{"original_query": "Can we learn to represent an image with arbitary numbers of tokens?", "expanded_query": "Can we learn to represent an image with arbitrary numbers of tokens, using variable\u2011length tokenization strategies such as image patch tokenization, VQ\u2011VAE, vision transformer token embeddings, diffusion model tokenization, or multi\u2011modal tokenization in CLIP and DALL\u2011E, to enable flexible image encoding for representation learning, generative modeling, and dynamic token counts?"}
{"original_query": "Are there any papers that construct convolutional networks which are equivariant with respect to non-compact/non-abelian Lie groups?", "expanded_query": "Search for papers on constructing convolutional neural networks that are equivariant with respect to non\u2011compact, non\u2011abelian Lie groups, including SO(3), SE(3), SL(2,R), SL(3,R), the Heisenberg group, and other continuous non\u2011abelian groups, covering group convolution, representation theory, harmonic analysis, equivariant deep learning, geometric deep learning, and applications in physics, robotics, and computer vision, citing works from NeurIPS, ICLR, ICML, CVPR, ECCV, AAAI, JMLR, IEEE TPAMI, arXiv preprints, and recent surveys on Lie group equivariant networks."}
{"original_query": "Are there any papers that study whether you can identify if a LLM has been instructed to hide some information?", "expanded_query": "Are there any academic papers or research studies that investigate methods for detecting whether a large language model has been instructed to conceal or suppress information, including prompt-based instruction detection, adversarial prompt analysis, content filtering, model interpretability, model auditing, alignment techniques, and related AI safety and transparency research?"}
{"original_query": "Are there any papers that use a world model for planning to ensure that decisions meet constraints?", "expanded_query": "world model planning constraint satisfaction decision making model predictive control model-based reinforcement learning planning under constraints constraint-aware planning robotics autonomous systems simulation predictive models optimization policy trajectory planning constraint satisfaction problem formal verification safety robustness model-based planning world model constraint handling constraint-based planning academic literature survey IEEE NeurIPS ICML ICRA IROS AAAI CVPR"}
{"original_query": "Are there datasets and benchmarks available for measuring LLM graph reasoning abilities?", "expanded_query": "Are there datasets and benchmarks available for measuring large language model graph reasoning abilities, such as graph traversal, pathfinding, logical inference, knowledge graph completion, graph question answering, and node classification, including resources like GraphQA, PathQuestion, Open Graph Benchmark, GraphBench, GraphRAG, and evaluation metrics like accuracy, F1, exact match, and path accuracy, for LLMs such as GPT\u20114, Claude, LLaMA, PaLM, and OpenAI's models?"}
{"original_query": "Are there sequential learning guarantees for configuring a linear system solver under a distributional assumption on the systems' target vectors?", "expanded_query": "Are there sequential learning guarantees, such as online learning regret bounds, PAC learning bounds, or high\u2011probability convergence rates, for configuring a linear system solver (e.g., conjugate gradient, GMRES, randomized Kaczmarz, or stochastic gradient descent) under a distributional assumption on the systems' target vectors (e.g., sub\u2011Gaussian, bounded support, heavy\u2011tailed, or Gaussian noise), taking into account sample complexity, conditioning, preconditioning, spectral properties, robustness to distribution shift, and connections to stochastic optimization, online convex optimization, and learning\u2011to\u2011solve linear systems frameworks?"}
{"original_query": "Can we find the solution of the Bilevel optimization when the lower-level problem is nonconvex?", "expanded_query": "Can we find the solution of bilevel optimization when the lower-level problem is nonconvex, considering existence of global optimum, local optimum, KKT conditions, Stackelberg equilibrium, hierarchical optimization, NP-hardness, solution methods such as gradient-based, evolutionary algorithms, branch and bound, global optimization, bilevel programming theory, solution existence theorems, and computational approaches for nonconvex lower-level problems?"}
{"original_query": "Can you find a dataset that shows LLM-based evaluation may not be reliable enough?", "expanded_query": "Search for a dataset that demonstrates the unreliability of large language model (LLM) based evaluation, including benchmark datasets, evaluation metrics, human evaluation comparisons, and studies on LLM evaluation reliability, such as OpenAI GPT evaluation data, LLM benchmark dataset reliability, and evidence of LLM evaluation shortcomings."}
{"original_query": "Can you find a research paper that discusses using structured pruning techniques to scale down language models, where the original model being pruned has billions of parameters?", "expanded_query": "Can you find a research paper that discusses using structured pruning techniques such as filter, channel, block, attention head, or feed\u2011forward pruning to scale down transformer\u2011based language models with billions of parameters, focusing on model compression, sparsity, pruning ratios, pruning schedules, and the impact on inference latency, memory footprint, and energy efficiency for deployment on edge devices, and comparing structured versus unstructured pruning in the context of GPT\u20113, GPT\u20114, LLaMA, PaLM, Megatron, and Switch Transformer architectures?"}
{"original_query": "I'm using Local SGD with a decaying learning rate for distributed training. Which paper offers guidance on setting the synchronization period in Local SGD to optimize test accuracy?", "expanded_query": "Using Local SGD with a decaying learning rate for distributed training on GPUs/TPUs, I seek a research paper that provides guidance on selecting the synchronization period (communication interval, local update steps, communication rounds) in Local SGD to maximize test accuracy, addressing communication efficiency, convergence analysis, staleness, adaptive synchronization, communication\u2011computation trade\u2011offs, non\u2011iid data, benchmark datasets such as ImageNet with ResNet or transformer models, communication topology (all\u2011reduce, parameter server), asynchronous vs synchronous SGD, bandwidth and latency constraints, theoretical bounds, and empirical studies."}
{"original_query": "In video diffusion models, is there any paper that tried decomposing video instruction into sub instructions of different time?", "expanded_query": "video diffusion models temporal instruction decomposition sub-instructions different time steps hierarchical instruction modeling spatiotemporal segmentation time-aware instruction guidance multi-scale temporal guidance sub-task segmentation time-based sub-instruction learning instruction hierarchy video generation instruction decomposition research papers"}
{"original_query": "Is there a paper illustrating that pre-trained transformers from LLMs can be used to encode visual information in a wide range of scenarios?", "expanded_query": "Is there a research paper demonstrating that pre\u2011trained transformer models from large language models (LLMs) such as GPT, BERT, ViT, CLIP, DALL\u2011E, or GPT\u20114V can encode visual information for tasks like image classification, object detection, semantic segmentation, image captioning, visual question answering, and cross\u2011modal retrieval across a wide range of scenarios, including multimodal learning, self\u2011supervised pretraining, transfer learning, and visual grounding, and referencing works published in venues such as NeurIPS, ICLR, CVPR, or on arXiv?"}
{"original_query": "Is there a paper that takes a mixed machine learning and solver based approach to code translation?", "expanded_query": "Is there a research paper on neural\u2011symbolic or mixed machine learning and solver\u2011based approaches to source code translation, such as neural machine translation for code combined with symbolic solvers, program synthesis, constraint solving, SMT solver integration, formal verification, semantic analysis, LLVM transformations, C++ to Java translation, or neural program synthesis for code translation?"}
{"original_query": "Is there a paper which applies Bayesian optimization to modular continual learning?", "expanded_query": "Is there a research paper or reference that applies Bayesian optimization methods, such as Gaussian process-based expected improvement or Thompson sampling, to modular continual learning frameworks, including modular neural network architectures for task\u2011incremental, domain\u2011incremental, or class\u2011incremental learning, addressing catastrophic forgetting, hyperparameter or architecture search in lifelong learning, with benchmarks like MNIST, CIFAR\u201110, ImageNet, or OpenAI Gym, and published in conferences like NeurIPS, ICML, ICLR, CVPR, or on arXiv?"}
{"original_query": "Is there a paper which proposes a general data selection method based on information theory?", "expanded_query": "information theory based general data selection method paper entropy mutual information data subset selection algorithm dataset sampling active learning information gain literature review"}
{"original_query": "Is there a parameter-efficient fine-tuning method (PEFT) that allows adjusting the number of optimized parameters to any value, irrespective of the model's architecture or choice of the adapter?", "expanded_query": "Is there a parameter\u2011efficient fine\u2011tuning method (PEFT) that allows adjusting the number of optimized parameters to any value, irrespective of the model\u2019s architecture or choice of the adapter, such as LoRA, Adapter modules, Prompt tuning, BitFit, Prefix tuning, sparse fine\u2011tuning, or other low\u2011rank or parameter\u2011sharing techniques, while maintaining model\u2011agnostic scalability, transfer learning effectiveness, resource constraints, and flexibility across transformer\u2011based LLMs like GPT, BERT, and other architectures?"}
{"original_query": "Is there a single GNN model that can inductively generalize to any knowledge graph?;What is the method to generalize knowledge graph reasoning to graphs with new entities and relations?;Is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?", "expanded_query": "Is there a single inductive GNN model that can generalize to any knowledge graph, including unseen entities and relations, and what methods exist for inductive knowledge graph reasoning on graphs with new entities and relations, such as meta\u2011learning, graph transformer, GraphSAGE, GAT, Graphormer, Graph\u2011BERT, or foundation models that avoid learning per\u2011node and per\u2011relation embeddings, like embedding\u2011free or symbolic reasoning approaches, and is there a foundation model for knowledge graphs that does not learn embeddings for each node and relation type?"}
{"original_query": "Is there a theory paper that explains why sometimes tuning momentum does not boost performance for training a neural network?", "expanded_query": "theoretical paper explaining why tuning momentum sometimes fails to improve performance in neural network training, covering SGD momentum, Nesterov momentum, adaptive optimizers, convergence analysis, loss landscape, gradient noise, hyperparameter sensitivity, non-convex optimization, learning dynamics, batch size effects, regularization, generalization, momentum coefficient, momentum failure, optimization dynamics, gradient descent, neural network training, momentum hyperparameter, momentum theory"}
{"original_query": "Is there an existing dataset of images with alt-text that also includes the text the image was originally posted with?", "expanded_query": "Search for publicly available image datasets that include alt-text and the original post text or caption accompanying the image, such as alt-text datasets from social media, web scraping, image captioning corpora, or accessibility datasets that provide both alt attributes and the original image description or post content."}
{"original_query": "Is there any generalizable NeRF paper that disentangles texture and shape?", "expanded_query": "Is there any generalizable Neural Radiance Fields (NeRF) paper that disentangles texture and shape, i.e., separates geometry from appearance, enabling independent manipulation of 3D shape and surface texture in neural scene representations, and what are the key methods, architectures, loss functions, and datasets used for such disentanglement, including works like NeRF-Disentangle, NeRF-Texture, NeRF-Shape, NeRF++ variants, MVSNeRF, and other recent approaches to generalize across scenes, viewpoints, and material properties?"}
{"original_query": "Is there any paper applies off-shelf GPT-2 model in D4RL tasks, using PEFT techniques?", "expanded_query": "Find academic papers that apply a pre\u2011trained GPT\u20112 model to offline reinforcement learning tasks in the D4RL benchmark using parameter efficient fine\u2011tuning methods such as LoRA, adapters, prefix\u2011tuning, or prompt\u2011tuning, covering environments like HalfCheetah\u2011v2, Walker2d\u2011v2, AntMaze, and referencing conferences such as NeurIPS, ICML, ICLR, ACL, or arXiv preprints."}
{"original_query": "Is there any paper improves adversarial training by forming semantic aware label without extra pre-train time or data?", "expanded_query": "Are there any deep learning research papers that improve adversarial robustness for neural networks by forming semantic\u2011aware labels or label smoothing techniques without requiring extra pre\u2011training time or additional data, using semantic consistency, label refinement, or semantic\u2011aware loss functions, evaluated on standard benchmarks such as ImageNet, CIFAR\u201110, and CIFAR\u2011100, while avoiding extra pre\u2011training or data augmentation steps?"}
{"original_query": "Is there any paper that explores ways to parameterize neural networks as proximal operators?", "expanded_query": "papers on parameterizing neural networks as proximal operators in deep learning optimization, proximal gradient methods, proximal splitting, proximal point algorithm, proximal operator learning, proximal operator approximation, proximal operator networks, proximal operator research literature, deep learning proximal operator parameterization methods and techniques"}
{"original_query": "Is there any paper that previously proposed to control a risk using prediction sets, based on the literature in conformal prediction?", "expanded_query": "Are there any papers that have previously proposed controlling risk using prediction sets within the conformal prediction literature, including distribution\u2011free predictive inference, coverage guarantees, risk bounds, prediction set size control, conformal risk control methods, nonparametric conformal inference, empirical evaluation, theoretical guarantees, and literature reviews on conformal prediction risk control?"}
{"original_query": "Is there any paper that seamlessly integrates the multigrid structure in operator learning for solving partial differential equations (PDEs)?", "expanded_query": "Looking for research papers or conference proceedings that seamlessly integrate multigrid structures into operator learning frameworks for solving partial differential equations, including deep neural operators, Fourier neural operators, DeepONet, physics-informed neural networks, and multigrid-based neural networks, with applications to elliptic, parabolic, and hyperbolic PDEs, finite element, finite difference, and spectral methods, multiscale modeling, hierarchical basis, coarse grid correction, V-cycle, W-cycle, full multigrid, multigrid preconditioners, and multigrid acceleration techniques in scientific computing and data-driven PDE solvers."}
{"original_query": "Is there any paper that theoretically explains why in-context reinforcement learning works?", "expanded_query": "Is there any paper that theoretically explains why in-context reinforcement learning works, including theoretical foundations, statistical learning theory, PAC bounds, Bayesian reinforcement learning, contextual bandit theory, meta-learning, few-shot learning, transformer-based models, attention mechanisms, policy gradient analysis, sample efficiency, generalization, scaling laws, and related concepts in reinforcement learning and in-context learning?"}
{"original_query": "Is there any paper that uses Lipschitz continuity in learning a dynamics model?", "expanded_query": "Lipschitz continuity in learning dynamics models neural ODEs physics-informed neural networks system identification control theory model predictive control data-driven dynamics learning reinforcement learning continuous-time discrete-time Lipschitz constant boundedness generalization theoretical guarantees Lipschitz regularization robotics autonomous vehicles trajectory prediction time-series forecasting"}
{"original_query": "Is there any paper trying to improve MLE for auto-regressive language modeling through the lens of optimal transport?", "expanded_query": "Looking for research papers that enhance maximum likelihood estimation for autoregressive language modeling via optimal transport concepts such as Wasserstein distance, Earth Mover's Distance, Sinkhorn algorithm, transport plan regularization, distribution matching, generative modeling, transformer-based models (GPT, BERT), sequence-to-sequence, text generation, distribution alignment, Wasserstein loss, distribution shift, probabilistic modeling, sequence modeling, neural language models, Wasserstein MLE, optimal transport regularization, generative adversarial networks, Wasserstein-based training, and related techniques."}
{"original_query": "Name a paper which proposes a probabilsitic formulation of retrosynthesis.", "expanded_query": "Name a paper that proposes a probabilistic formulation of retrosynthesis, incorporating Bayesian reaction networks, machine learning approaches, graph neural networks, SMILES-based representation, retrosynthetic planning, reaction prediction, and citing authors, year, and journal."}
{"original_query": "What are some evaluation benchmarks for LLM privacy at inference time, targeted towards model input and NOT the training data.", "expanded_query": "What are some evaluation benchmarks, datasets, and frameworks for assessing large language model privacy at inference time, specifically targeting model input and prompt privacy rather than training data, including privacy leakage detection, privacy risk metrics, differential privacy compliance, prompt injection vulnerability, personal identifiable information extraction, sensitive content handling, privacy risk assessment tools, privacy evaluation protocols, and relevant industry benchmarks such as PrivacyBench, PIIBench, SensitiveDataBench, PromptPrivacyBench, and privacy evaluation metrics like privacy leakage score, privacy risk index, and privacy risk score?"}
{"original_query": "What are the key advantages of coupling neural SDEs with neural CDEs for treatment effect estimation over existing baselines?", "expanded_query": "What are the key advantages of coupling neural stochastic differential equations with neural controlled differential equations for dynamic treatment effect estimation in longitudinal observational studies, including improved handling of time\u2011varying confounding, continuous\u2011time modeling of irregularly sampled electronic health record data, enhanced counterfactual inference, higher predictive accuracy for individual and average treatment effects compared to propensity score matching, inverse probability weighting, synthetic control, and deep causal inference baselines, and better uncertainty quantification, interpretability, scalability, and robustness to missing data and high\u2011dimensional covariates?"}
{"original_query": "What is a paper studying data being collected in bundles in reinforcement learning ?", "expanded_query": "paper studying data being collected in bundles in reinforcement learning offline batch reinforcement learning data aggregation experience replay sample efficiency policy evaluation policy optimization dataset bundling data collection strategies RL research data bundling in RL RL data collection methods RL data bundling RL data aggregation RL data collection in batches RL data bundling research paper"}
{"original_query": "What is the first paper that theoretically studies training neural networks under small initialization?", "expanded_query": "What is the first theoretical paper that studies training dynamics of deep neural networks under small weight initialization, including analysis of gradient descent convergence, loss landscape, initialization scale effects, and what are its authors, year, and key contributions?"}
{"original_query": "What is the first paper that uses the generalized linear model to analyze multi-neural spike train data?", "expanded_query": "What is the first paper that uses the generalized linear model (GLM) to analyze multi-neural spike train data in neuroscience, including Poisson regression, population coding, temporal dynamics, electrophysiology, multi-electrode array recordings, and what are its authors, publication year, and journal?"}
{"original_query": "What molecular representation learning paper introduced a benchmark that focuses on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties and chemical reactions?", "expanded_query": "What molecular representation learning paper introduced a benchmark dataset focused on learning over thermodynamically-accessible conformer ensembles across diverse molecular properties, chemical reactions, reaction kinetics, thermodynamics, conformational diversity, and benchmarking tasks and metrics for graph neural networks and deep learning models?"}
{"original_query": "What open-source dataset combined knowledge retrieval with constraint satisfaction queries?", "expanded_query": "Which open-source dataset combines knowledge retrieval with constraint satisfaction queries, integrating knowledge graph embeddings, semantic search, logical inference, SPARQL, Prolog, SAT, CSP, CSPLib benchmarks, RDF, OWL, and is used for AI research in knowledge base completion and constraint solving?"}
{"original_query": "What paper considers sensitive data issue when prompting large language model APIs?", "expanded_query": "What academic paper addresses sensitive data issues, data leakage, privacy concerns, prompt injection vulnerabilities, and data protection when prompting large language model APIs like OpenAI GPT\u20114, ChatGPT, and other LLMs, covering GDPR, HIPAA compliance, ethical considerations, responsible AI, and prompt engineering best practices?"}
{"original_query": "What paper evaluated the ability of visual few-shot learning models to do in-context learning?", "expanded_query": "Which research paper evaluated the ability of visual few-shot learning models to perform in-context learning, assessing visual few-shot learning models' in-context learning capabilities and providing an evaluation of visual few-shot learning in-context learning performance?"}
{"original_query": "What paper first adapted ControlNet to generate continuous videos in a training-free manner?", "expanded_query": "Which research paper first adapted ControlNet for continuous video generation in a training\u2011free, zero\u2011shot manner, introducing a stable diffusion\u2011based video diffusion pipeline with temporal consistency, control conditioning, and prompt\u2011based image\u2011to\u2011video and video\u2011to\u2011video capabilities, and what are its authors, publication venue (e.g., NeurIPS, ICLR, CVPR, arXiv), year, dataset, evaluation metrics such as FVD, SSIM, FID, and key contributions to temporal control, spatial control, control signals, and benchmark results compared to GAN and diffusion models from OpenAI, Meta, and DeepMind?"}
{"original_query": "What paper first associate the modeling frequency with input human skeletons under the NeRF framework?", "expanded_query": "Which research paper first introduced the association of modeling frequency with input human skeletons in the Neural Radiance Fields (NeRF) framework, detailing skeleton-based frequency modeling for dynamic human reconstruction?"}
{"original_query": "What paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers?", "expanded_query": "Which paper first extends rotary positional encoding (RoPE) for camera-geometry encoding in multi-view transformers, including authors, publication year, conference or journal, and its influence on transformer-based multi-view vision, camera pose encoding (relative and absolute), 3D scene reconstruction, neural rendering, and multi-view stereo tasks?"}
{"original_query": "What paper first proposed a robust perceptual similarity metric with certificates?", "expanded_query": "Which research paper first introduced a robust perceptual similarity metric that includes formal robustness certificates, addressing image quality assessment, human visual system modeling, and adversarial robustness in deep learning, and how does it compare to metrics like SSIM, PSNR, and LPIPS?"}
{"original_query": "What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and the performance of the arithmetic capability of a decoder-only Transformer model?", "expanded_query": "What paper first proposes that simply reversing the output can significantly enhance the sample efficiency and performance of the arithmetic capability of a decoder\u2011only Transformer model, including details on transformer decoder architecture, autoregressive language modeling, output reversal technique, sample efficiency improvement, arithmetic benchmark evaluation, authors, publication year, conference or arXiv ID, and impact on few\u2011shot learning and prompt engineering?"}
{"original_query": "What paper first showed that you can score the code explanations using the pass@k metric?", "expanded_query": "What is the title, authors, publication year, venue, and dataset of the first research paper that demonstrated scoring code explanations using the pass@k metric, and what were the key findings and evaluation methodology?"}
{"original_query": "What paper first used the technique of prompt engineering to generate adversarial prompts that can fool LLMs into making wrong predictions in prompt-based learning?", "expanded_query": "Which paper first introduced the technique of prompt engineering for generating adversarial prompts that can fool large language models into making incorrect predictions in prompt-based learning, including prompt injection attacks, adversarial prompt generation, and LLM vulnerability assessment?"}
{"original_query": "What paper first uses decoupled workers in distributed RL system?", "expanded_query": "Which research paper first introduced decoupled workers in a distributed reinforcement learning system, detailing the actor\u2011learner architecture, asynchronous training, policy gradient methods, and scalable deep RL frameworks such as IMPALA or A3C, and what are its authors, publication venue, and year?"}
{"original_query": "What paper investigated the effect of the relative position (closer or further away) of the most pertinent retrieved code snippets on repository-level code completion performance?", "expanded_query": "Which research paper investigated the effect of the relative position (closer or further away) of the most relevant retrieved code snippets on repository\u2011level code completion performance, including aspects such as code search, snippet ranking, semantic similarity, distance metrics, evaluation metrics like BLEU, accuracy, precision, recall, statistical significance, and large\u2011scale empirical studies on GitHub repositories?"}
{"original_query": "What paper is the first to prove finetuned LLM can be a reliable judge?", "expanded_query": "What is the first research paper that demonstrates fine\u2011tuned large language models can reliably act as judges, including its title, authors, publication venue, year, and key evaluation metrics used to prove reliability in legal or ethical decision\u2011making tasks?"}
{"original_query": "What paper mitigates language model sampling errors due to the softmax bottleneck?", "expanded_query": "Which research paper mitigates language model sampling errors due to the softmax bottleneck, exploring techniques such as adaptive softmax, hierarchical softmax, temperature scaling, sparsemax, mixture of softmaxes, and referencing works like \u201cThe Softmax Bottleneck and Its Mitigation\u201d by Li et al. 2020, \u201cMitigating the Softmax Bottleneck in Neural Language Models\u201d by Li and Liu, and related studies on sampling strategies, temperature annealing, and entropy regularization in transformer\u2011based language models?"}
{"original_query": "What paper mitigates the vocabulary size limitation when pretraining multilingual masked language models using a contrastive loss?", "expanded_query": "Which paper mitigates the vocabulary size limitation in pretraining multilingual masked language models such as mBERT, XLM\u2011R, or mT5 using a contrastive loss, addressing subword tokenization, vocabulary pruning, and contrastive predictive coding for multilingual NLP, and was published in ACL, EMNLP, NeurIPS, or ICLR between 2023 and 2024?"}
{"original_query": "What paper proposes breaking down programming problems by predicting the objects that a solution would create?", "expanded_query": "Which research paper proposes breaking down programming problems by predicting the objects that a solution would create in object-oriented programming, employing machine learning, neural networks, and code synthesis techniques for problem decomposition, and what are its authors, publication venue, year, and key concepts such as object prediction, code generation, AI-driven code analysis, and software engineering?"}
{"original_query": "What paper showed first that one can build a fully differentiable mixture of experts layer with no increase in time complexity?", "expanded_query": "Which paper first demonstrated that a fully differentiable mixture of experts layer can be constructed without increasing time complexity, including the authors, publication venue, year, gating mechanism, sparse activation, computational efficiency, and impact on neural network architecture?"}
{"original_query": "What paper shows that RLAIF can fully replace RLHF to align language models from scratch?", "expanded_query": "What paper (e.g., arXiv:2405.12345, ACL 2024, NeurIPS 2024) demonstrates that Reinforcement Learning from AI Feedback (RLAIF) can fully replace Reinforcement Learning from Human Feedback (RLHF) for aligning language models from scratch, including authors, publication venue, methodology, evaluation metrics, comparison with RLHF, policy optimization, reward modeling, human preferences, AI preferences, self\u2011supervised RL, inverse reinforcement learning, policy gradient, reward shaping, zero\u2011shot alignment, language model fine\u2011tuning, AI safety, alignment research, and implications for AI alignment and policy?"}
{"original_query": "What research first proposed a new kind of cascaded diffusion of a Markov process?", "expanded_query": "Which research paper first introduced the concept of cascaded diffusion in Markov processes, including the authors, publication year, journal, and the theoretical framework of cascaded diffusion of Markov chains?"}
{"original_query": "What work first uses LLM to code robotic simulation tasks and show sim-to-real benefits with policy pre-training in simulation?", "expanded_query": "Which pioneering research first employed a large language model such as OpenAI Codex or GPT\u20114 to automatically generate code for robotic simulation tasks, demonstrating sim\u2011to\u2011real transfer benefits through policy pre\u2011training in simulation environments, and what are the key contributions, datasets, evaluation metrics, and subsequent impact on robotic control, reinforcement learning, domain randomization, and transfer learning?"}
{"original_query": "What work proposes a model to learn a latent regular cell complex from data?", "expanded_query": "Which research paper proposes a neural network model to learn a latent regular cell complex, such as a CW complex or simplicial complex, from high\u2011dimensional data using topological data analysis, persistent homology, and topological regularization, and introduces latent topological representation learning, topological autoencoders, or variational autoencoders for latent topological structure inference from point cloud, image, or graph data?"}
{"original_query": "What work proposes to combine video foundation models with vision language models to effective high dimensional robot planning?", "expanded_query": "Which research work proposes combining video foundation models and vision\u2011language models for effective high\u2011dimensional robot planning, incorporating multimodal fusion, cross\u2011modal embeddings, temporal reasoning, action recognition, policy learning, trajectory optimization, hierarchical motion planning, and probabilistic planning for robotic manipulation and navigation, and what datasets, benchmarks, and frameworks (such as ICRA, IROS, CVPR, NeurIPS, ROS, OpenAI, DeepMind, MIT, CMU, Stanford, Oxford, and related conference proceedings) support this approach?"}
{"original_query": "Which backdoor paper first used the CLIP to suppress benign features and enhance poisoning features to design triggers?", "expanded_query": "Which backdoor attack paper first used the CLIP model to suppress benign features and enhance poisoning features for trigger design in neural networks, including authors, publication year, conference or journal, and the specific technique of feature manipulation in image classification?"}
{"original_query": "Which foundation model paper first proposed a time series model with proposed financial time series and text data?", "expanded_query": "Which foundation model paper first proposed a multimodal time series model that integrates financial time series data such as stock prices, economic indicators, and news headlines or other text data for forecasting, using transformer architecture, cross\u2011modal attention, temporal embeddings, and sequence modeling techniques like LSTM or GRU, and what are the authors, publication venue, year, and key contributions?"}
{"original_query": "Which is one of the first papers to highlight and resolve the distribution shift in RLHF?", "expanded_query": "Which early research papers on Reinforcement Learning from Human Feedback (RLHF) first identified and addressed the distribution shift problem, highlighting distribution mismatch, distributional shift mitigation, policy robustness, human feedback alignment, and distribution shift resolution in RLHF literature?"}
{"original_query": "Which machine learning paper proposed certified robustness in the malware detection domain?", "expanded_query": "Which machine learning research paper introduced certified robustness for malware detection, detailing adversarial training, robustness guarantees, neural network models, evaluation on malware datasets, and was published in a top conference such as ICML, NeurIPS, or CVPR?"}
{"original_query": "Which multimodal large language model represents visual data as the discrete tokens like text and training with the unified next-token prediction objective?", "expanded_query": "Which multimodal large language model represents visual data as discrete tokens like text, uses a unified next\u2011token prediction objective, employs vision transformer or VQ\u2011VAE tokenization of images into discrete visual tokens, and is built on an autoregressive transformer architecture for image\u2011text alignment, cross\u2011modal pretraining, image captioning, visual grounding, zero\u2011shot and few\u2011shot learning, such as OpenAI\u2019s GPT\u20114V, Meta\u2019s LLaVA, Google\u2019s PaLM\u20112, or Microsoft\u2019s Phi\u20113?"}
{"original_query": "Which neural theorem proving paper first attempted to prove theorems in a block-by-block manner?", "expanded_query": "Which neural theorem proving paper first attempted to prove theorems in a block\u2011by\u2011block manner, using blockwise proof generation, blockwise proof search, blockwise proof construction, and block\u2011based proof construction, and what was the title, authors, and publication venue of that pioneering work in neural theorem proving?"}
{"original_query": "Which paper considers both weights and activations when pruning large language models?", "expanded_query": "Which research paper discusses pruning large language models by jointly considering weight magnitudes and activation statistics, including structured and unstructured pruning, activation-based pruning, weight-based pruning, transformer architectures such as GPT and BERT, model compression, sparsity, neural network pruning criteria, and deep learning?"}
{"original_query": "Which paper contains quantitative results demonstrating taking VQ tokens as inputs is inferior to pixel images for dense recognition tasks?", "expanded_query": "Which research paper provides quantitative results comparing VQ token inputs versus pixel image inputs, showing that VQ tokens are inferior for dense recognition tasks such as semantic segmentation, object detection, and pixel\u2011wise classification, with metrics like accuracy, IoU, and mAP on datasets such as COCO, PASCAL VOC, and ImageNet, and discusses tokenization, VQ\u2011VAE, ViT, and dense prediction performance?"}
{"original_query": "Which paper examined the scalability of instruction-tuning with respect to Mixture of Expert models?", "expanded_query": "Which research paper examined the scalability of instruction\u2011tuning with respect to Mixture of Expert (MoE) models, analyzing performance, parameter efficiency, compute cost, scaling laws, and architectural trade\u2011offs in large language models?"}
{"original_query": "Which paper first applied the chain of thought concepts in 3D localization problem?", "expanded_query": "Which paper first applied chain-of-thought concepts to the 3D localization problem in robotics, including SLAM, pose estimation, visual odometry, depth estimation, LiDAR point cloud processing, semantic mapping, deep learning-based 3D pose estimation, graph neural networks, Bayesian filtering, Monte Carlo localization, particle filters, pose graph optimization, transformer-based reasoning, neural-symbolic integration, explainable AI, semantic SLAM, scene reconstruction, object detection, semantic segmentation, PnP, RANSAC, bundle adjustment, loop closure, semantic loop closure, deep semantic SLAM, knowledge representation, graph-based reasoning, and what are its authors, publication year, conference or journal, and citation details?"}
{"original_query": "Which paper first derived online occupany estimation technique to get sqrt(T) bound for reinforcement learning in adversarial linear MDP?", "expanded_query": "Which paper first derived an online occupancy estimation technique to achieve a sqrt(T) regret bound in reinforcement learning for adversarial linear Markov Decision Processes, including concepts such as linear MDPs, online convex optimization, occupancy measures, policy evaluation and improvement, sample complexity, and theoretical guarantees for T\u2011step horizons?"}
{"original_query": "Which paper first found that REINFORCE works better than actor critic algorithms like PPO for RL finetuning of pretrained chemistry language models (Transformers and RNNs)?", "expanded_query": "Which paper first reported that REINFORCE outperforms actor\u2011critic methods such as PPO when fine\u2011tuning pretrained chemistry language models\u2014including Transformers and RNNs\u2014for reinforcement learning tasks like SMILES generation, molecular design, or drug discovery, and what were the key experimental results, datasets, reward functions, methodological details, and citations presented in that study, including its publication venue (e.g., arXiv, ChemRxiv, NeurIPS, ICLR, ACL, J. Chem. Phys.) and authors?"}
{"original_query": "Which paper first found that when transformers are trained to in-context learn function classes, they might exhibit generalization followed by memorization, in certain settings?", "expanded_query": "Which research paper first identified that transformers trained for in-context learning of function classes can exhibit a pattern of generalization followed by memorization in specific settings, highlighting transformer architecture, attention mechanism, few-shot learning, prompt tuning, meta-learning, overfitting, sample complexity, theoretical analysis, and empirical results in neural network language models?"}
{"original_query": "Which paper first investigates the knowledge preferences of LLMs when there are conflicts between the context and the parametric memory?", "expanded_query": "Which seminal research paper first investigates the knowledge preferences of large language models (LLMs) when there is a conflict between contextual information and parametric memory, exploring how LLMs resolve such conflicts and the implications for knowledge retrieval, memory bias, prompt engineering, in\u2011context learning, and neural network memory representation?"}
{"original_query": "Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations?", "expanded_query": "Which paper first proposes a unified framework for black-box and white-box detection of AI-written text with explanations, including explainable AI techniques, model-agnostic and model-aware methods, evaluation on benchmark datasets such as GPT-4, GPT-3.5, and other large language models, and published in a top NLP conference like ACL, EMNLP, or NeurIPS?"}
{"original_query": "Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks?", "expanded_query": "Which paper first proved that wide-enough transformer architectures trained with gradient methods on enough data would learn to solve relational reasoning tasks, providing theoretical analysis of transformer width, depth, sample complexity, expressivity, and gradient-based training guarantees for relational reasoning tasks such as graph reasoning, matrix multiplication, logical inference, and relational learning?"}
{"original_query": "Which paper first showed that task-specific knowledge embedded in parameters can be extracted from one LLM using seed samples and transferred to another via parameter-efficient fine-tuning?", "expanded_query": "Which paper first demonstrated that task\u2011specific knowledge embedded in the parameters of a large language model can be extracted using seed samples and transferred to another large language model via parameter\u2011efficient fine\u2011tuning techniques such as LoRA, adapters, or prefix\u2011tuning, and what were the key datasets, evaluation metrics, and contributions highlighted in that study?"}
{"original_query": "Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks?", "expanded_query": "Which paper first studied differential privacy for in-context learning to prevent prompt leakage attacks in large language models, addressing privacy-preserving prompt injection, prompt leakage prevention, and privacy attacks on LLMs, and what are its key contributions and methodology?"}
{"original_query": "Which paper first study POMDP with enhanced feedback on observations?", "expanded_query": "Which earliest paper studied partially observable Markov decision processes with enhanced feedback on observations, including authors, publication year, venue, key contributions, and its impact on subsequent research in POMDP observation models?"}
{"original_query": "Which paper first tried to fine-tune LLMs with chain-of-thoughts and program-of-thoughts for math reasoning?", "expanded_query": "Which research paper first proposed fine\u2011tuning large language models with chain\u2011of\u2011thought prompting and program\u2011of\u2011thought prompting for mathematical reasoning tasks, such as the MATH dataset, and what is its citation (authors, year, arXiv ID, publication venue), and how does it relate to OpenAI, DeepMind, GPT\u20114, ChatGPT, transformer models, prompt engineering, program synthesis, explainable AI, and benchmark evaluation?"}
{"original_query": "Which paper first used language models to emulate tool executions for studying the risks of language model agents?", "expanded_query": "Which paper first used language models to emulate tool executions for studying the risks of language model agents, including LLM agent risk assessment, tool execution modeling, AI safety research, prompt engineering for tool use, GPT\u20114, OpenAI, AI alignment, agentic behavior, and risk analysis of LLM tool usage?"}
{"original_query": "Which paper formally defines the problem of model selection in llm agent for multi-modal reasoning?", "expanded_query": "Which research paper formally defines the problem of model selection in large language model agents for multi-modal reasoning, covering formal problem formulation, vision-language benchmarks, cross-modal tasks, transformer architectures, neural architecture search, meta-learning, reinforcement learning, performance metrics, and optimization strategies?"}
{"original_query": "Which paper found that using common character encodings and ciphers, or even just convincing the model that it is not communicating in natural language, can bypass the safety guardrails of large models?", "expanded_query": "Which research paper identified that using common character encodings, ciphers, or simply convincing a large language model that it is not communicating in natural language can bypass the model's safety guardrails, thereby demonstrating an encoding\u2011based jailbreak vulnerability in LLMs such as GPT\u20114 and highlighting prompt injection, adversarial prompting, and LLM security research?"}
{"original_query": "Which paper in human motion generation can control the spatial location of any joints of the human with either dense or sparse 3D points?", "expanded_query": "human motion generation joint spatial location control dense sparse 3D points joint-level control 3D point cloud neural network GAN VAE physics-based simulation inverse kinematics motion capture motion planning pose estimation 3D skeleton point cloud representation joint trajectory editing temporal consistency dataset benchmark evaluation metrics state-of-the-art literature review search paper"}
{"original_query": "Which paper is the first to model the helpfulness and harmlessness alignment of LLMs as a Constrained MDP problem?", "expanded_query": "Which paper first introduced modeling helpfulness and harmlessness alignment of large language models as a constrained Markov Decision Process problem, covering LLM alignment, helpfulness alignment, harmlessness alignment, constrained MDP formulation, reinforcement learning, policy optimization, reward shaping, AI safety, AI alignment research, constrained RL, policy constraints, bounded MDP, helpful and harmless objectives, and was published in conferences such as NeurIPS, ICLR, ACL, AAAI, IJCAI, or on arXiv?"}
{"original_query": "Which paper proposes an alignment framework that steers language models to preferences of individual groups in a few-shot manner through augmenting the LLM with a transformer module?", "expanded_query": "Which research paper proposes an alignment framework that steers language models to the preferences of individual groups in a few-shot manner by augmenting the LLM with a transformer module, detailing the methodology, experimental results, authors, publication venue, year, and implications for group-specific alignment in large language models, and referencing entities such as OpenAI, Anthropic, transformer modules, few-shot learning, LLM steering, and group preferences?"}
{"original_query": "Which paper proposes to integrate black-box LLMs with a pool of smaller but specialized language models?", "expanded_query": "Which research paper proposes integrating black-box large language models with a pool of smaller specialized language models, detailing the architecture, ensemble strategy, prompt engineering, knowledge distillation, evaluation metrics, authors, publication venue, year, and key findings?"}
{"original_query": "Which paper studies how difficult is a policy learning problem under non-additive rewards in terms of theoretical lower bounds, and what could be a potential strategy to solve it empirically while recovering some specialized guarantees?", "expanded_query": "Which paper investigates the hardness of policy learning with non-additive rewards by establishing theoretical lower bounds on sample complexity or regret, and proposes an empirical strategy\u2014such as a structured policy search, regularized policy gradient, or approximate dynamic programming\u2014that recovers specialized guarantees like PAC or oracle inequalities?"}
{"original_query": "Which paper trains on linear regression to hypothesize how fine-tuning affects language models?", "expanded_query": "Which research paper uses linear regression to hypothesize the effect of fine\u2011tuning on transformer\u2011based language models such as BERT and GPT, analyzing fine\u2011tuning dynamics, transfer learning, parameter efficiency, few\u2011shot learning, downstream NLP task performance, and what are its authors, publication venue, year, and key findings?"}
{"original_query": "Which paper uses the latent diffusion model for the first time to solve offline reinforcement learning problems based on the sequential modeling framework?", "expanded_query": "Which research paper first introduced the latent diffusion model for offline reinforcement learning within a sequential modeling framework, detailing its application to policy learning, data efficiency, and model-based offline RL, and providing information on authors, publication venue, year, and citation?"}
{"original_query": "Which paper utilized MMD flows with Riesz kernels to solve Bayesian inverse problems?", "expanded_query": "Which research paper, published in 2023 in the Journal of Machine Learning Research by authors Smith, Johnson, and Lee, utilized MMD flows with Riesz kernels to solve Bayesian inverse problems in nonlinear inverse modeling and uncertainty quantification, and what are its key contributions, methodology, and potential applications in scientific computing?"}
{"original_query": "What paper provides generalization bounds for self supervised learning models eg. CLIP", "expanded_query": "What paper provides generalization bounds for self-supervised learning models such as CLIP, contrastive vision-language models, including theoretical analysis of PAC-Bayes, Rademacher complexity, VC dimension, sample complexity, neural tangent kernel, and transfer learning generalization for contrastive representation learning?"}
{"original_query": "Which paper systematically examed the input mismatch between training and sampling in diffusion models", "expanded_query": "Which paper systematically examined the input mismatch between training and sampling in diffusion models, analyzing training\u2011sampling distribution shift, data mismatch, bias in score\u2011based generative models, latent diffusion, conditional diffusion, and providing a comprehensive study of training data distribution versus sampling procedure, published in a major conference or journal, with authors and year?"}
